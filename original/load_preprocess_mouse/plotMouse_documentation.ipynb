{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook generates a bench of plot to show different parameters of task performance, effect of opto stim and relationship with electrophysiology\n",
    "\n",
    "### This notebook is at the core of the pipeline of data processing. Do not play with it lightly inside the master folder (load_preprocess_mouse)\n",
    "\n",
    "#### 1. Only modifiy if you are sure of what you are doing and that you are solving a bug\n",
    "#### 2. If you do modify you MUST commit this modification using bitbucket\n",
    "#### 3. If you want to play whis notebook (to understand it better) copy it on a toy folder distinct from the master folder\n",
    "#### 4. If you want to modify this code (fix bug, improve, add attributes ...) it is recommanded  to first duplicate in a draft folder. Try to keep track of your change.\n",
    "#### 5. When you are ready to commit : # clear all output, clean everything between hashtag \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import packages and define a few basic functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "import os\n",
    "import platform\n",
    "import glob\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter as smooth\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections  as mc\n",
    "from matplotlib.patches import Rectangle\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## The lines below allow to run required notebook from the master folder\n",
    "if \"__file__\" not in dir():\n",
    "    \n",
    "    ThisNoteBookPath=os.path.dirname(os.path.realpath(\"__file__\"))\n",
    "    CommunNoteBookesPath=os.path.join(os.path.split(ThisNoteBookPath)[0],\"load_preprocess_mouse\")\n",
    "    os.chdir(CommunNoteBookesPath)   \n",
    "    \n",
    "    %run loadMouse_documentation.ipynb\n",
    "    %run loadRawSpike_documentation.ipynb\n",
    "    %run plotMouse_documentation.ipynb\n",
    "    \n",
    "def has_tag(root, animal, session, tagList):\n",
    "    \"\"\" Test if the session has at least one of the tag in tagList\n",
    "    tag = empty file with a specific name, in a session folder\n",
    "    \"\"\"\n",
    "    fullPath = os.path.join(root, animal, \"Experiments\", session)\n",
    "    fileList = os.listdir(fullPath)\n",
    "    for tag in tagList:\n",
    "        if tag in fileList:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def cm2inch(value):\n",
    "    return value/2.54\n",
    "\n",
    "if \"__file__\" not in dir():\n",
    "\n",
    "    if platform.system()=='Linux':\n",
    "        root=\"/data\"\n",
    "    else:\n",
    "        root=\"/Users/davidrobbe/Documents/Data/\"\n",
    "\n",
    "    print(\"The path to data is %s\"%root)\n",
    "    \n",
    "    \n",
    "def contiguous_regions(condition):\n",
    "    \"\"\"Finds contiguous True regions of the boolean array \"condition\". Returns\n",
    "    a 2D array where the first column is the start index of the region and the\n",
    "    second column is the end index.\"\"\"\n",
    "\n",
    "    # Find the indicies of changes in \"condition\"\n",
    "    d = np.diff(condition)\n",
    "    idx, = d.nonzero() \n",
    "\n",
    "    # We need to start things after the change in \"condition\". Therefore, \n",
    "    # we'll shift the index by 1 to the right.\n",
    "    idx += 1\n",
    "\n",
    "    if condition[0]:\n",
    "        # If the start of condition is True prepend a 0\n",
    "        idx = np.r_[0, idx]\n",
    "\n",
    "    if condition[-1]:\n",
    "        # If the end of condition is True, append the length of the array\n",
    "        idx = np.r_[idx, condition.size] # Edit\n",
    "\n",
    "    # Reshape the result into two columns\n",
    "    idx.shape = (-1,2)\n",
    "    return idx\n",
    "    \n",
    "# you can manually specify root data foder between hashtag line\n",
    "##############################\n",
    "\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load and preprocess one session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    \n",
    "    \"\"\"\n",
    "    Below is an example of data path information. DO NOT CHANGE THIS LINE BELOW\n",
    "    BUT you can put your data path information between the 2 hashtag lines after the example\n",
    "    (for git tracking issue)\n",
    "    \"\"\"\n",
    "    \n",
    "    SESSION=\"MOU073_2015_09_07_10_06/\"\n",
    "    \n",
    "    \n",
    "##############################\n",
    "    \n",
    "##############################\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    If you want to commit changes made in this NB please delete leave 1 empty line between the 2\n",
    "    hashtag lines instead of your own data path\n",
    "    \"\"\"\n",
    "    ANIMAL=SESSION[0:6]\n",
    "    \n",
    "    #Those parameters are overwritten if there is a .behav_param file\n",
    "    paramCarola={\n",
    "        \"distanceToRun\":100,\n",
    "        \"maxTrialDuration\": 60,\n",
    "        \"valveONTime\":50,\n",
    "        \"minInterTrialDuration\":15,\n",
    "        \"immobilityDuration\":2,\n",
    "        #to read .eeg (put None to not read .eeg)\n",
    "        \"nChannelElectro\":32, #32\n",
    "        \"channel_opto\": -6, #not used\n",
    "        \"channel_lickBreak\":-5,\n",
    "        \"channel_reward\": -4, #not used currently\n",
    "        \"channel_sound\": -3, #not used\n",
    "        \"channel_trialON\": -2,\n",
    "        \"channel_beamBreak\": -1,\n",
    "    }\n",
    "    data=Data(root,ANIMAL,SESSION,paramCarola,redoPreprocess=True)\n",
    "    print(\"----------------\")\n",
    "\n",
    "    if data.hasBehavior and not data.isLickTraining:\n",
    "        data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. A serie of behavioral plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create \"fake\" lick after reward if lick are missing \n",
    "#### obvously this is for approximative analysis purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CreateMissingLicks(data,LickEstimatedDuration=4):\n",
    "    \n",
    "    licktimesintrialintetrital={}\n",
    "    for trial in data.trials:\n",
    "        licktimesintrialintetrital[trial]=[]\n",
    "        licktimesintrialintetrital[trial+.5]=[]\n",
    "    \n",
    "    \n",
    "    alllickstimes=[]\n",
    "    for trial in data.trials:\n",
    "        if np.round(data.durationTrial[trial],1)<data.maxTrialDuration[trial]: ## this is a good trial\n",
    "#             roundedvalue=np.round(data.durationTrial[trial],1)\n",
    "#             print(\"trial %s trial duration %s ,rounded value: %s\" %(trial,data.durationTrial[trial],roundedvalue))\n",
    "        \n",
    "            \n",
    "            ## generate alllickbreaktime\n",
    "            endofgoodtrialtime=data.realStartTrial[trial]+data.durationTrial[trial]\n",
    "            alllickstimes.extend(list(np.arange(endofgoodtrialtime,endofgoodtrialtime+LickEstimatedDuration,0.125)))\n",
    "            \n",
    "            ## generate lickbreaktime per (good) intetrial\n",
    "            licktimesintrialintetrital[trial+.5]=list(np.arange(0,LickEstimatedDuration,0.125))\n",
    "            \n",
    "    return alllickstimes,licktimesintrialintetrital\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():  \n",
    "    alllickstimes,licktimesintrialintetrital=CreateMissingLicks(data)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot beam breaks for every trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_break(data, legend=False, colorOpto=\"orange\", xmax=60, ax=None, lick=True):  \n",
    "    \"\"\"\n",
    "    Plot the beam breaks, lick breaks and indicates optogenetic stimulation.\n",
    "    One line: one trial + following intertrial. 0=end of trial (reward)\n",
    "    Input\n",
    "      - legend: whether to put a legend, on the top right outside the plot\n",
    "      - colorOpto: colors for the rectangular boxes indicating optogenetic stimulation\n",
    "      - xmax: maximum on the xaxis\n",
    "      - ax: matplotlib figure axis, usefull for complex subplots\n",
    "      - lick: whether to plot the lick breaks\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    minTime = - max(data.durationTrial)\n",
    "    maxTime = max(data.durationInterTrial)\n",
    "    maxTime = min(xmax, maxTime)\n",
    "    distanceToRun = data.distanceToRun[0]\n",
    "    darkLines = []\n",
    "    greenLines = []\n",
    "    #ticks where the distance to run has changed\n",
    "    boldTicks = {1:distanceToRun}\n",
    "    \n",
    "\n",
    "    ## if there is no lick data we estimate them approximatively!\n",
    "    if not data.allLickBreak:\n",
    "        print(\"there was no lick so we create some\")\n",
    "        data.allLickBreak,data.lickBreakTime=CreateMissingLicks(data)\n",
    "        #print(\"missing lick times %s\" %data.allLickBreak)\n",
    "        \n",
    "    \n",
    "    for trial in data.trials:\n",
    "        y1 = trial + 1.1\n",
    "        y2 = trial + 1.1\n",
    "        y3 = trial + 1.7\n",
    "        y4 = trial + 1.9       \n",
    "        #look if distance to run changed\n",
    "        newDistance = data.distanceToRun[trial]\n",
    "        if newDistance != distanceToRun:\n",
    "            boldTicks[trial+1] = newDistance\n",
    "            distanceToRun = newDistance      \n",
    "        #we want to align the plot on the trial end (=reward =start of intertrial)\n",
    "        zero = data.durationTrial[trial]        \n",
    "        #trial beam break time are relative to trial start        \n",
    "        for breakTime in data.beamBreakTime[trial]:\n",
    "            x = breakTime - zero\n",
    "            darkLines.append([(x,y1), (x,y4)]) \n",
    "\n",
    "            \n",
    "        #intertrial beam break time are already aligned correctly\n",
    "        for x in data.beamBreakTime[trial+0.5]:\n",
    "            darkLines.append([(x,y1), (x,y4)])\n",
    "            \n",
    "        #if there is lick break times\n",
    "        if (len(data.lickBreakTime) > 0) and lick:\n",
    "            for lickTime in data.lickBreakTime[trial]:\n",
    "                x = lickTime-zero\n",
    "                greenLines.append([(x,y2), (x,y3)])\n",
    "            for x in data.lickBreakTime[trial+0.5]:\n",
    "                greenLines.append([(x,y2), (x,y3)])          \n",
    "        #color in grey the duration of trial and intertrial\n",
    "        #rectangle= (x,y) lower left, width, height\n",
    "        endInterTrial = data.durationInterTrial[trial]\n",
    "        ax.add_patch(Rectangle((-zero,trial+1), zero, 1, facecolor=\"lightgrey\", edgecolor=\"none\"))\n",
    "        ax.add_patch(Rectangle((0,trial+1), endInterTrial, 1, facecolor=\"lavender\", edgecolor=\"none\"))              \n",
    "    \n",
    "    #Plot all the lines at once (gain time)\n",
    "    lc = mc.LineCollection(darkLines, colors=\"black\", label=\"beam breaks\")\n",
    "    lc2 = mc.LineCollection(greenLines, colors=\"forestgreen\", label=\"lick breaks\")\n",
    "    ax.add_collection(lc)\n",
    "    ax.add_collection(lc2)        \n",
    "    #color the optogenetic stimulation\n",
    "    #start and stop are relative to trial start\n",
    "    if data.hasOptogenetic:\n",
    "        plt.plot([], [], linewidth=6, color=colorOpto, label=\"optogenetic stimulation\")\n",
    "        for trial in data.trials:\n",
    "            zero = data.durationTrial[trial]\n",
    "            start = data.startStimulation[trial]\n",
    "            if start is not None:\n",
    "                stop = data.stopStimulation[trial]\n",
    "                ax.add_patch(Rectangle((start-zero, trial+1), (stop-start), 1, \n",
    "                                       edgecolor=colorOpto, fill=False, lw=2, zorder=10))\n",
    "            #elif potentialstimstart is not None:\n",
    "                \n",
    "    #blue line at 0                \n",
    "    plt.axvline(0, color=\"blue\")    \n",
    "    #bold ticks\n",
    "    space = 1\n",
    "    rangeList = list(set(range(1, data.nTrial+2, space)).union(boldTicks))\n",
    "    ticksPosition = [y+0.5 for y in rangeList]\n",
    "    ticksLabel = [str(boldTicks[y]) + \"cm| \" + str(y) if y in boldTicks else str(y) for y in rangeList]  \n",
    "    plt.yticks(ticksPosition, ticksLabel)    \n",
    "    #axis limits\n",
    "    plt.xlim([minTime, maxTime])\n",
    "    ax.invert_yaxis()\n",
    "    plt.ylim([data.realTrials[-1]+1, data.realTrials[0]])   \n",
    "    #axis labels and title\n",
    "    plt.xlabel(\"time in seconds (0=intertrial start)\", fontsize=14)\n",
    "    plt.ylabel(\"trial number\", fontsize=14)\n",
    "    title = data.experiment + \" (day \" + str(data.daySinceStart) + \")\\nBeam and lick break time\"\n",
    "    if data.hasOptogenetic:\n",
    "        title += \"\\n %s\" %data.stimulationNames\n",
    "    plt.title(title, fontsize=14)   \n",
    "    #legend with no duplicate\n",
    "    if legend:\n",
    "        plt.plot([], [], linewidth=6, color='lightgrey', label=\"trial time range\")\n",
    "        plt.plot([], [], linewidth=6, color='lavender', label=\"intertrial time range\")\n",
    "        plt.legend(loc='best', bbox_to_anchor=(1, 1))\n",
    "    return title\n",
    "            \n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():    \n",
    "    plt.figure(figsize=(10,30))\n",
    "    plot_break(data, legend=True,lick=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot mean breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_mean_breaks(data, binSize=0.25, minTime=-20, maxTime=40, align=\"trial end\", separate=\"good trial\",\n",
    "                     lick=False, s=1, displayOnly=0, number=0):  \n",
    "    '''\n",
    "    Plots the mean running speed (or licking frequency), for two groups of trials,\n",
    "      according to the argument 'separate'\n",
    "    Input\n",
    "      - s: sigma for smoothing the means\n",
    "      - lick: True to plot lick frequency instead of running speed\n",
    "      - separate: \n",
    "          \"good trials\"(/bad trials)\n",
    "          \"optogenetic\"(/no optogenetic)\n",
    "          \"none\"(mean for all trials, only one curve is plotted)\n",
    "          \"trial number\" (trial < number / trial >= number) --- NB: first trial is 0\n",
    "      - align: where to align the trials (where to consider 0)\n",
    "         either \"trial end\" or \"trial start\"\n",
    "         overriden by \"stimulation start\" in case of optogenetic stimulation after N ticks\n",
    "      - minTime, maxTime: time range for xaxis, relative to 0 (see 'align')\n",
    "      - displayOnly: \n",
    "          0 = nothing\n",
    "          1 = plots only the separated group, in black (only the good trials, or only the optogenetic)\n",
    "          2 = plots only the other group (bad trials, or trials without optogenetic)\n",
    "      - number: see argument separate=\"trial number\"\n",
    "    '''\n",
    "    #bins between minTime and maxTime\n",
    "    timeBin = np.arange(minTime, maxTime+binSize-maxTime%binSize, binSize)\n",
    "    centers = (timeBin[:-1]+timeBin[1:]) / 2.0\n",
    "    \n",
    "    \n",
    "    ## if there is no lick data we estimate them approximatively!\n",
    "    if not data.allLickBreak:\n",
    "        print(\"there was no lick so we create some\")\n",
    "        data.allLickBreak,data.lickBreakTime=CreateMissingLicks(data)\n",
    "        #print(\"missing lick times %s\" %data.allLickBreak)    \n",
    "    \n",
    "    \n",
    "    #lick or the beam breaks\n",
    "    if lick:\n",
    "        allBreak = np.asarray(data.allLickBreak)\n",
    "    else:\n",
    "        allBreak = np.asarray(data.allBeamBreak)\n",
    "    #check we have data\n",
    "    if len(allBreak) == 0:\n",
    "        plt.title(\"Nothing to plot\")\n",
    "        return \"nothing to plot\"\n",
    "    #colors depending on case\n",
    "    if separate == \"good trial\":\n",
    "        color = \"green\"\n",
    "    elif separate == \"optogenetic\":\n",
    "        color = \"purple\"\n",
    "    elif separate == \"trial number\":\n",
    "        color=\"blue\"\n",
    "    elif separate != \"none\":\n",
    "        print(\"Unvalid value for separate. Choose between 'good trial', 'optogenetic', 'trial number' and 'none'\")\n",
    "        return\n",
    "    if displayOnly:\n",
    "        color = \"black\"\n",
    "        if lick:\n",
    "            color = \"darkblue\"\n",
    "    #special case for \"Stimulate after N ticks\": trials have to be aligned to stimulation start\n",
    "    Ncase = False\n",
    "    if (data.hasOptogenetic) and (separate == \"optogenetic\"):\n",
    "        if data.stimulationNames.startswith(\"Stimulate after\"):\n",
    "            Ncase = True\n",
    "            align = \"stimulation start\"      \n",
    "    InCase=False\n",
    "    if (data.hasOptogenetic) and (separate == \"optogenetic\"):\n",
    "        if data.stimulationNames.startswith(\"Stimulate in trial\"):\n",
    "            InCase = True\n",
    "            align = \"stimulation start\"   \n",
    "    InInterCase=False\n",
    "    if (data.hasOptogenetic) and (separate == \"optogenetic\"):\n",
    "        if data.stimulationNames.startswith(\"Stimulate in intertrial\"):\n",
    "            InInterCase = True\n",
    "            align = \"stimulation start\"  \n",
    "    #compute speeds\n",
    "    separateBeamCount = [] #\"good trials\" or \"optogenetic trials\"\n",
    "    allBeamCount = []      # all the other trials\n",
    "    for trial in data.trials:\n",
    "        #where is 0\n",
    "        if Ncase:\n",
    "            if (data.PutativeStimTimeAtNTicks[trial] is None) :\n",
    "                continue\n",
    "            zero = data.realStartTrial[trial] + data.PutativeStimTimeAtNTicks[trial]\n",
    "        if InCase:\n",
    "            if (data.PutativeStimTimeInTrial[trial] is None) :\n",
    "                continue\n",
    "            zero = data.realStartTrial[trial] + data.PutativeStimTimeInTrial[trial]\n",
    "        if InInterCase:\n",
    "            if (data.PutativeStimTimeInInterTrial[trial] is None) :\n",
    "                continue\n",
    "            zero = data.realStartTrial[trial] + data.PutativeStimTimeInInterTrial[trial]                \n",
    "        elif align == \"trial end\":\n",
    "            zero = data.realStartTrial[trial] + data.durationTrial[trial]\n",
    "        elif align == \"trial start\":\n",
    "            zero = data.realStartTrial[trial]\n",
    "        #align on zero and compute speed\n",
    "        alignedBreak = allBreak - zero\n",
    "        hist, bins = np.histogram(alignedBreak, timeBin)\n",
    "        #add to group\n",
    "        if (separate == \"good trial\") and (trial in data.goodTrials):\n",
    "            if displayOnly != 2:\n",
    "                separateBeamCount.append(hist) \n",
    "            continue\n",
    "        elif (separate == \"optogenetic\") and (data.hasOptogenetic):\n",
    "            if data.stimulationOccured[trial] == True:\n",
    "                if displayOnly != 2:\n",
    "                    separateBeamCount.append(hist)\n",
    "                continue\n",
    "        elif (separate == \"trial number\") and (trial >= number):\n",
    "            if displayOnly != 2:\n",
    "                separateBeamCount.append(hist)\n",
    "            continue\n",
    "        if displayOnly != 1:\n",
    "            allBeamCount.append(hist)   \n",
    "    #compute means, avoiding \"All Nan Slice\" warning\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "        meanBeamCount = np.nanmean(np.asarray(allBeamCount), axis=0)\n",
    "        meanSeparate = np.nanmean(np.asarray(separateBeamCount), axis=0)\n",
    "    if lick:\n",
    "        meanBin = meanBeamCount/float(binSize)\n",
    "        meanBinSeparate = meanSeparate/float(binSize)\n",
    "        plt.ylabel(\"mean licking frequency (Hz)\", fontsize=14)\n",
    "    else:\n",
    "        meanBin = meanBeamCount*data.tickDistance/float(binSize)\n",
    "        meanBinSeparate = meanSeparate*data.tickDistance/float(binSize)\n",
    "        plt.ylabel(\"mean running speed (cm/sec)\", fontsize=14)\n",
    "    #plot if not empty\n",
    "    if len(allBeamCount) > 0:\n",
    "        plt.plot(centers, smooth(meanBin,s), \"k-\")\n",
    "    if len(separateBeamCount) > 0:\n",
    "        plt.plot(centers, smooth(meanBinSeparate, s), \"-\", color=color, label=separate)\n",
    "    plt.xlabel(\"time (sec), binSize=%ss, 0=%s\"%(binSize, align), fontsize=14)\n",
    "    plt.xlim(minTime, maxTime)\n",
    "    #line at 0\n",
    "    if lick:\n",
    "        title = \"Lick\"          \n",
    "        plt.axvline(0, color=\"orange\", linestyle=\"--\")    \n",
    "    else:\n",
    "        title = \"Running\"            \n",
    "        plt.axvline(0, color=\"blue\", linestyle=\"--\")    \n",
    "    #title\n",
    "    if separate != \"none\":\n",
    "        title = title + \" (%s=%s %s, black=%s other)\"%(color, len(separateBeamCount), separate, len(allBeamCount))\n",
    "    if (data.hasOptogenetic) and (separate==\"optogenetic\"):\n",
    "        title += \"\\n \" + data.stimulationNames\n",
    "    plt.title(title, fontsize=14)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(221)\n",
    "    plot_mean_breaks(data, align=\"trial end\", separate=\"good trial\", minTime=-30, maxTime=30)\n",
    "\n",
    "    plt.subplot(222)\n",
    "    plot_mean_breaks(data, align=\"trial start\", minTime=-10, maxTime=60, separate=\"good trial\")\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plot_mean_breaks(data, lick=True, align=\"trial end\", minTime=-10, maxTime=20, displayOnly=1)\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plot_mean_breaks(data, align=\"trial end\", minTime=-20, maxTime=40, separate=\"optogenetic\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### General behavior plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def behavior_plot(data):\n",
    "    \"\"\"\n",
    "    A general plot to display the behavior during one session\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,25))\n",
    "    plt.suptitle(data.experiment + \" (day %s)\" %data.daySinceStart, fontsize=20)    \n",
    "    plt.subplot(121)\n",
    "    plot_break(data, legend=False, xmax=40)\n",
    "\n",
    "    plt.subplot(524)\n",
    "    plot_mean_breaks(data, align=\"trial end\", separate=\"good trial\", minTime=-30, maxTime=30)\n",
    "\n",
    "    plt.subplot(522)\n",
    "    plot_mean_breaks(data, align=\"trial start\", minTime=-10, maxTime=60, separate=\"good trial\")\n",
    "       \n",
    "    if data.hasOptogenetic:\n",
    "        stop = np.nanmean([d/1000.0 for d in data.opticalDuration if not isNone(d) and d>0])\n",
    "        plt.subplot(526)\n",
    "        if \",\" in data.stimulationNames:\n",
    "            plt.title(\"Can't plot: two or more stimulation types\")\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, align=\"trial end\", minTime=-10, maxTime=10)\n",
    "            \n",
    "        elif data.stimulationNames == \"Stimulate at beginning of trial\":\n",
    "            plot_mean_breaks(data, align=\"trial start\", separate=\"optogenetic\", minTime=-10, maxTime=20)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, align=\"trial start\", separate=\"optogenetic\", minTime=-10, maxTime=10)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "        elif data.stimulationNames.startswith(\"Stimulate before end of inter-trial\"):\n",
    "            t = np.nanmean([d for d in data.stimulateBeforeEnd_time if not isNone(d) and d>0])\n",
    "            plot_mean_breaks(data, align=\"trial end\", separate=\"optogenetic\", minTime=-10, maxTime=20)\n",
    "            plt.axvspan(t, t+stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, align=\"trial end\", separate=\"optogenetic\", minTime=-10, maxTime=10)\n",
    "            plt.axvspan(t, t+stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "        elif data.stimulationNames == \"Stimulate  upon reward\":\n",
    "            plot_mean_breaks(data, align=\"trial end\", separate=\"optogenetic\", minTime=-10, maxTime=10)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, align=\"trial end\", separate=\"optogenetic\", minTime=-10, maxTime=10)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "        elif data.stimulationNames.startswith(\"Stimulate after\"):\n",
    "            plot_mean_breaks(data, separate=\"optogenetic\")\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, separate=\"optogenetic\", minTime=-10, maxTime=30)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "        elif data.stimulationNames.startswith(\"Stimulate in trial\"):\n",
    "            plot_mean_breaks(data, separate=\"optogenetic\", minTime=-10, maxTime=20)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, separate=\"optogenetic\", minTime=-10, maxTime=30)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "        elif data.stimulationNames.startswith(\"Stimulate in intertrial\"):\n",
    "            plot_mean_breaks(data, separate=\"optogenetic\", minTime=-10, maxTime=20)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "            \n",
    "            plt.subplot(528)\n",
    "            plot_mean_breaks(data, lick=True, separate=\"optogenetic\", minTime=-10, maxTime=30)\n",
    "            plt.axvspan(0, stop, color=\"lightgray\", alpha=0.5)\n",
    "        else:\n",
    "            plt.title(\"No stimulation to plot\", fontsize=14)\n",
    "            \n",
    "    if data.lickBreakTime:\n",
    "        plt.subplot(5,2,10)\n",
    "        plot_mean_breaks(data, lick=True, align=\"trial end\", minTime=-10, maxTime=10)\n",
    "        \n",
    "    plt.subplots_adjust(top=0.93, hspace=0.3)\n",
    "  \n",
    "\n",
    "if \"__file__\" not in dir():\n",
    "    behavior_plot(data)\n",
    "\n",
    "#### the part below is an addition of wahiba/Loubna to save some plot. check with them \n",
    "#run only if inside this notebook (does not execute if \"%run this_notebook\")\n",
    "# if \"__file__\" not in dir():  \n",
    "#     if data.hasBehavior:\n",
    "#         behavior_plot(data)\n",
    "#         name = \"behavior_plot\" + \".png\"\n",
    "#         path = os.path.join(data.sessionPath, name)\n",
    "#         plt.savefig(path)\n",
    "        \n",
    "#     else:\n",
    "#         print(\"no behavior\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Detect running periods and immobility periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def detect_running_period(data, minDurationSecond=2, maxDurationSecond=None, runType=\"all\", maxTimeBetweenBreak=None):\n",
    "    \"\"\"    \n",
    "    detect periods where the animal runs without pause (time between consecutive break < maxTimeBetweenBreak)\n",
    "    remove periods shorter than minDurationSeconds and longer than maxDurationSecond (put None for no maximum)\n",
    "    \n",
    "    runType can be \"all\", \"intertrial\", \"trial good run\", \"trial bad run\", or \"unrewarded\" (bad run and intertrial runs)\n",
    "    \n",
    "    \"\"\"\n",
    "    return detect_activity_period(data, minDurationSecond, maxDurationSecond, periodType=runType,\n",
    "                                 lick=False, maxTimeBetweenBreak=maxTimeBetweenBreak)\n",
    "\n",
    "def detect_licking_period(data, minDurationSecond=0, maxDurationSecond=None, lickType=\"all\", maxTimeBetweenBreak=None):\n",
    "    \"\"\"\n",
    "    same for licking\n",
    "    \"\"\"\n",
    "    return detect_activity_period(data, minDurationSecond, maxDurationSecond, periodType=lickType, \n",
    "                                  lick=True, maxTimeBetweenBreak=maxTimeBetweenBreak)\n",
    "    \n",
    "def detect_activity_period(data, minDurationSecond=0, maxDurationSecond=None, periodType=\"all\", lick=False,\n",
    "                           maxTimeBetweenBreak = None, allActivity=False):\n",
    "    \"\"\" detect periods of consecutive breaks\n",
    "    \n",
    "    - minDurationSecond: minimun duration of the period, in second\n",
    "    - maxDurationSecond: maximum duration of the period, in second\n",
    "    - periodType: period types to keep (see the \"get_running/licking_period_type\" methods)\n",
    "       for running: \"all\", \"intertrial\", \"trial good run\", \"trial bad run\", \"unrewarded\"(=intertrial + trial bad run)\n",
    "       for licking: \"all\", \"rewarded\", \"unrewarded\"\n",
    "       NB - periodType can be a string, or a list of strings\n",
    "    - lick: whether to use beambreaks (running, lick=False) or lickbreaks (lick=True)\n",
    "    - maxTimeBetweenBreak: maximum time allowed between two consecutive breaks for them to be in the same period\n",
    "    - allActivity: wheter to use beambreaks and lickbreaks together (as if it was one set of breaks)\n",
    "        implies periodType=\"all\"\n",
    "    \"\"\"\n",
    "    if maxTimeBetweenBreak is None:\n",
    "        maxTimeBetweenBreak = data.maxTimeBetweenBreak\n",
    "        \n",
    "    ## if there is no lick data we estimate them approximatively!\n",
    "    if not data.allLickBreak:\n",
    "        print(\"there was no lick so we create some\")\n",
    "        data.allLickBreak,data.lickBreakTime=CreateMissingLicks(data)\n",
    "        #print(\"missing lick times %s\" %data.allLickBreak)\n",
    "    \n",
    "    #detect group of breaks with no pause\n",
    "    if allActivity:\n",
    "        allBreaks = np.sort(np.append(data.allLickBreak, data.allBeamBreak))\n",
    "        periodType = \"all\"\n",
    "    elif lick:\n",
    "        if not(data.allLickBreak):\n",
    "            print(\"No lick Data\")\n",
    "            start=[]\n",
    "            end=[]\n",
    "            indexes=[]\n",
    "            return start, end, indexes\n",
    "        else:\n",
    "            allBreaks = data.allLickBreak\n",
    "    else:\n",
    "        allBreaks = data.allBeamBreak\n",
    "    \n",
    "    #period type has to be a list of strings\n",
    "    if not isinstance(periodType, list):\n",
    "        periodType = [periodType]\n",
    "    if \"unrewarded\" in periodType:\n",
    "        periodType.append(\"trial bad run\")\n",
    "        periodType.append(\"intertrial\")\n",
    "        \n",
    "    previousBreak = allBreaks[0]\n",
    "    allPeriods = [[previousBreak]]\n",
    "    indexStart = [0]\n",
    "    indexStop = []\n",
    "    i = 0\n",
    "    for index, b in enumerate(allBreaks[1:]):\n",
    "        if (b-previousBreak) >= maxTimeBetweenBreak:\n",
    "            i += 1\n",
    "            indexStop.append(index)    #index 0 is actually index 1, because of [1:]\n",
    "            indexStart.append(index+1)\n",
    "            allPeriods.append([])\n",
    "        allPeriods[i].append(b)\n",
    "        previousBreak = b\n",
    "    indexStop.append(index)\n",
    "    \n",
    "    #get start and end of group\n",
    "    startRunning=[]\n",
    "    endRunning=[]\n",
    "    indexes=[]\n",
    "    for period, start, stop in zip(allPeriods, indexStart, indexStop):\n",
    "        #remove period too short\n",
    "        duration = period[-1] - period[0]\n",
    "        if duration < minDurationSecond:\n",
    "            continue\n",
    "        #remove period too long (if specified)\n",
    "        if maxDurationSecond is not None:\n",
    "            if duration > maxDurationSecond:\n",
    "                continue\n",
    "        #period to keep\n",
    "        indexes.append((start, stop))\n",
    "        startRunning.append(period[0])\n",
    "        endRunning.append(period[-1])\n",
    "        \n",
    "    #select running periods according to trial/intertrial, good/bad\n",
    "    if \"all\" not in periodType:\n",
    "        newStart = []\n",
    "        newEnd = []\n",
    "        newIndexes = []\n",
    "        for start, end, ind in zip(startRunning, endRunning, indexes):\n",
    "            if lick:\n",
    "                tt = get_licking_period_type(data, start, end)\n",
    "            else:\n",
    "                tt = get_running_period_type(data, start, end) \n",
    "            if tt in periodType:\n",
    "                newStart.append(start)\n",
    "                newEnd.append(end)\n",
    "                newIndexes.append(ind)\n",
    "        return newStart, newEnd, newIndexes\n",
    "    else:\n",
    "        return startRunning, endRunning, indexes\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def get_running_period_type(data, start, stop):\n",
    "    '''\n",
    "    a running period can be:\n",
    "     - during a trial (good run or bad run)\n",
    "     - in between trial and intertrial (good run or intertrial)\n",
    "     - in intertrial (intertrial run)\n",
    "     \n",
    "    Warning : during electrophy recordings using Carola original code there is\n",
    "    # a bug that allows good runs [that will be rewarded] to start at the end of the previous trial [this is only a few case]\n",
    "    a good run can therefore start in intertrial of he previous trial\n",
    "    \n",
    "    This is taken care of using the end of detected run (instead of start originally in Typhaine code) to detect the closest previous trial start time\n",
    "    '''\n",
    "    # first \"trial start\" before this running period\n",
    "    trialIndex=np.where(data.realStartTrial<=stop)[0]\n",
    "    if len(trialIndex)>0:\n",
    "        trialIndex=trialIndex[-1]\n",
    "    else:\n",
    "        trialIndex=0\n",
    "    trialTime=data.realStartTrial[trialIndex]\n",
    "    # corresponding \"intertrial start\" \n",
    "    interTime=data.realStartInterTrial[trialIndex]\n",
    "    #test if before or after intertrial start\n",
    "    if start>=interTime:\n",
    "        return \"intertrial\"\n",
    "    else:\n",
    "        #good run: trial is good, and end of run is on reward, or later\n",
    "        #allows to be 1 second before reward, in case of not precise timing\n",
    "        if trialIndex in data.goodTrials:\n",
    "            if (stop+1)>=interTime:\n",
    "                return \"trial good run\"\n",
    "        else:\n",
    "            if stop>=interTime:\n",
    "                return \"intertrial\"\n",
    "        return \"trial bad run\"\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def get_licking_period_type(data, start, stop):\n",
    "    '''\n",
    "    a licking period can be:\n",
    "     - rewarded\n",
    "     - not rewarded\n",
    "    '''\n",
    "    # first \"trial start\" before this running period\n",
    "    trialIndex=np.where(data.realStartTrial<=start)[0]\n",
    "    if len(trialIndex)>0:\n",
    "        trialIndex=trialIndex[-1]\n",
    "    else:\n",
    "        trialIndex=0\n",
    "    # corresponding \"intertrial start\" \n",
    "    interTime=data.realStartInterTrial[trialIndex]\n",
    "    #test if before or after intertrial start\n",
    "    if (start <= interTime + 0.5) and (stop >= interTime + 1):\n",
    "        return \"rewarded\"\n",
    "    else:\n",
    "        return \"unrewarded\"\n",
    "#---------------------------------------------------------------------------------------------------------------    \n",
    "def detect_immobility_period(data, minDurationSecond=2, maxDurationSecond=None, immobilityType=\"all\",\n",
    "                             runMinDuration=0.1, allActivity=False, lick=False):\n",
    "    '''\n",
    "    detect periods where the animal is immobile:\n",
    "      detect the runs (longer than runMinDuration) / or the licks if lick=True\n",
    "      take what's in between the run\n",
    "    remove periods shorter than minDurationSecond and longer than maxDurationSecond (None=no maximum duration)\n",
    "    If runMinDuration=0: every tick is in a run. Immobility period has zero ticks\n",
    "    if runMinDuration=0.1: a tick alone is not considered a run, and will be inside an immobility period\n",
    "    '''   \n",
    "    #detect runs, removed duration<minDurationSecond (strict)\n",
    "    if allActivity:\n",
    "        startRunning, endRunning, indexes = detect_activity_period(data, minDurationSecond=runMinDuration,\n",
    "                                                                  maxDurationSecond=None, periodType=\"all\",\n",
    "                                                                  allActivity=True)\n",
    "    elif lick:\n",
    "        startRunning, endRunning, indexes = detect_licking_period(data, minDurationSecond=runMinDuration,\n",
    "                                                                 maxDurationSecond=None, lickType=\"all\")\n",
    "    else:\n",
    "        startRunning, endRunning, indexes = detect_running_period(data, minDurationSecond=runMinDuration,\n",
    "                                                                maxDurationSecond=None, runType=\"all\")\n",
    "    #immobility= periods between runs\n",
    "    startImmobile = endRunning[:-1]\n",
    "    endImmobile = startRunning[1:]\n",
    "    \n",
    "    startIndexes = [index[1] for index in indexes[:-1]]\n",
    "    endIndexes = [index[0] for index in indexes[1:]] \n",
    "    indexesImmobile = list(zip(startIndexes, endIndexes))\n",
    "    \n",
    "    #select periods\n",
    "    newStart = []\n",
    "    newEnd = []\n",
    "    newIndexes = []\n",
    "    for s, e, i in zip(startImmobile, endImmobile, indexesImmobile):\n",
    "        #remove immobility period too long or too short\n",
    "        duration = e - s\n",
    "        if (duration > minDurationSecond):\n",
    "            if (maxDurationSecond is None) or (duration < maxDurationSecond):\n",
    "                #select type\n",
    "                if (immobilityType == \"all\") or (get_immobility_period_type(data, s) == immobilityType):\n",
    "                    newStart.append(s)\n",
    "                    newEnd.append(e)\n",
    "                    newIndexes.append(i)\n",
    "                    \n",
    "    return newStart, newEnd, newIndexes\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------   \n",
    "def get_immobility_period_type(data,start):\n",
    "    '''\n",
    "    an immobility period can start during trial or intertrial\n",
    "    '''\n",
    "    # first \"trial start\" before this running period\n",
    "    trialIndex=np.where(data.realStartTrial<=start)[0]\n",
    "    if len(trialIndex)>0:\n",
    "        trialIndex=trialIndex[-1]\n",
    "    else:\n",
    "        trialIndex=0\n",
    "    # corresponding \"intertrial start\" \n",
    "    interTime=data.realStartInterTrial[trialIndex]\n",
    "    #test if before or after intertrial start\n",
    "    if start + 0.5 >= interTime:\n",
    "        return \"intertrial\"\n",
    "    else:\n",
    "        return \"trial\"\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def check_period_detection(data, start, end):\n",
    "    # in the orginal code of thyphaine there is a bug caused by the fact that the last trial has no intetrial\n",
    "    #this caused wrong detection of a good trial as overlappping intetrial and trial and premature break out of the program\n",
    "    #this corrected by using the time of the last wheel detection as time for the end of intertrial periode \n",
    "    plt.figure(figsize=(20,15))\n",
    "    plot_break(data, legend=False, xmax=200)\n",
    "    ax = plt.gca()\n",
    "    \n",
    "    trialStart = data.realStartTrial[0]\n",
    "    trial = 0\n",
    "    zero = data.durationTrial[0]\n",
    "    trialEnd = trialStart + zero + data.durationInterTrial[trial]    \n",
    "#     print(\"trialStart: %s, zero: %s, trialEnd: %s\" %(trialStart, zero+trialStart,trialEnd))\n",
    "\n",
    "    loop=0\n",
    "    for s, e in zip(start, end):\n",
    "        loop+=1\n",
    "#         print(\"loop: %s\" %loop)\n",
    "#         print(\"s: %s, e: %s\" %(s,e))\n",
    "        if s <= trialStart:\n",
    "#             print(\"first case is used\")\n",
    "#             print(\"trial number: %s\" %trial)\n",
    "#             print(\"trialStart: %s, zero: %s, trialEnd: %s\" %(trialStart, zero+trialStart,trialEnd))         \n",
    "             continue\n",
    "        while s >= trialEnd:\n",
    "            trial += 1\n",
    "            #print(\"detected run number %s occurs during trial nber: %s\" %(loop,trial+1))\n",
    "            if trial+1 > data.nTrial:\n",
    "                break\n",
    "            \n",
    "            zero = data.durationTrial[trial]\n",
    "            trialStart = trialEnd\n",
    "            if (trial+1==data.nTrial) and (data.durationInterTrial[trial]==0):\n",
    "                #print(\"last trial!!!\")\n",
    "                trialEnd = data.allBeamBreak[-1]+1\n",
    "            else:\n",
    "                trialEnd += zero + data.durationInterTrial[trial]\n",
    "            \n",
    "        if s <= trialEnd and e >= trialEnd:\n",
    "#             print(\"trialStart: %s, zero: %s, trialEnd: %s\" %(trialStart, zero+trialStart,trialEnd)) \n",
    "#             print(\"third case is used\")\n",
    "#             print(\"duration intertrial: %s\" %data.durationInterTrial[trial])\n",
    "#             print(\"trial nber: %s\" %(trial+1))\n",
    "            while e >= trialEnd:\n",
    "                #period is in beween two trials\n",
    "                startRec = s - trialStart - zero\n",
    "                length = data.durationInterTrial[trial] - startRec\n",
    "                ax.add_patch(Rectangle((startRec, trial+1), length, 0.8, facecolor=\"lightcoral\", edgecolor=\"none\"))              \n",
    "                if trial+2 > data.nTrial:\n",
    "                    break                \n",
    "                length2 = min(e-s-length, data.durationTrial[trial+1]+data.durationInterTrial[trial+1])                \n",
    "                ax.add_patch(Rectangle((-data.durationTrial[trial+1], trial+2), length2, 0.8, facecolor=\"lightcoral\", \n",
    "                                       edgecolor=\"none\"))\n",
    "                trial += 1\n",
    "                zero = data.durationTrial[trial]\n",
    "                trialStart = trialEnd\n",
    "                #trialEnd += zero + data.durationInterTrial[trial]\n",
    "                if (trial+1==data.nTrial) and (data.durationInterTrial[trial]==0):\n",
    "                    #print(\"last trial!!!\")\n",
    "                    trialEnd = data.allBeamBreak[-1]+1\n",
    "                else:\n",
    "                    trialEnd += zero + data.durationInterTrial[trial]\n",
    "                    \n",
    "        else:\n",
    "            ax.add_patch(Rectangle((s - trialStart - zero, trial+1), e-s, 0.8, facecolor=\"orange\", edgecolor=\"none\"))\n",
    "#             print(\"last case is used\")\n",
    "#             print(\"trial number: %s\" %(trial+1))\n",
    "#             print(\"trialStart: %s, zero: %s, trialEnd: %s\" %(trialStart, zero+trialStart,trialEnd))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        #print(s, e, \"--\", trialStart, trialEnd, \"--\", trial)\n",
    "            \n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir(): \n",
    "    start, end, indexes = detect_running_period(data, minDurationSecond=3, runType=\"all\",maxTimeBetweenBreak=1)\n",
    "    #start, end, indexes = detect_immobility_period(data,lick=True, allActivity=True)\n",
    "    #start, end, indexes = detect_activity_period(data, minDurationSecond=0)\n",
    "    \n",
    "    check_period_detection(data, start, end)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir(): \n",
    "    #start, end, indexes = detect_running_period(data, minDurationSecond=3, runType=\"unrewarded\",maxTimeBetweenBreak=1)\n",
    "    start, end, indexes = detect_immobility_period(data,lick=True, allActivity=True,immobilityType=\"intertrial\",minDurationSecond=1)\n",
    "    #start, end, indexes = detect_activity_period(data, minDurationSecond=0)    \n",
    "    check_period_detection(data, start, end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir(): \n",
    "    start, end, indexes = detect_licking_period(data, minDurationSecond=2, lickType=\"all\")\n",
    "    #start, end, indexes = detect_immobility_period(data)\n",
    "    #start, end, indexes = detect_activity_period(data, minDurationSecond=0)\n",
    "    if (data.allLickBreak):\n",
    "        check_period_detection(data, start, end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. A serie of codes to visualize firing activity\n",
    "\n",
    "## 3. 1 list the units in the present session or accross session\n",
    "\n",
    "### List the all the Good cluster numbers of this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def PrintClustersByShank(data):\n",
    "    if not data.hasSpike:\n",
    "        print (\"no spikes in session %s\" %data.experiment)\n",
    "    else:\n",
    "        for shank in data.clusterGroup:\n",
    "            print (\"Shank Nber %s\"%shank)\n",
    "            print (\"Good Clusters: %s\" %data.clusterGroup[shank]['Good'])\n",
    "    #-------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir(): \n",
    "    PrintClustersByShank(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List how many good and MUA clusters there are in this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def CountCluPerSession(data):\n",
    "    TotalNumberOfGoodClu=[]\n",
    "    TotalNumberOfGoodCluPerShank=\"\"\n",
    "    TotalNumberOfMUAClu=[]\n",
    "    TotalNumberOfMUACluPerShank=\"\"\n",
    "    for shank in data.clusterGroup:\n",
    "        for Group in data.clusterGroup[shank]:        \n",
    "            if Group==\"Good\":\n",
    "                TotalNumberOfGoodClu.append(len(data.clusterGroup[shank][Group]))\n",
    "                TotalNumberOfGoodCluPerShank=TotalNumberOfGoodCluPerShank+str(len(data.clusterGroup[shank][Group]))+\"+\"\n",
    "            elif Group==\"MUA\":\n",
    "                TotalNumberOfMUAClu.append(len(data.clusterGroup[shank][Group]))\n",
    "                TotalNumberOfMUACluPerShank=TotalNumberOfMUACluPerShank+str(len(data.clusterGroup[shank][Group]))+\"+\"\n",
    "\n",
    "    if sum(TotalNumberOfGoodClu)>0:\n",
    "        GoodCluPerSession=[str(sum(TotalNumberOfGoodClu)) + \" (\" + TotalNumberOfGoodCluPerShank[:-1] + \")\",sum(TotalNumberOfGoodClu)]\n",
    "       \n",
    "    else:\n",
    "        GoodCluPerSession=[\"0\",0]\n",
    "        \n",
    "\n",
    "    if sum(TotalNumberOfMUAClu)>0:\n",
    "        MUACluPerSession=[str(sum(TotalNumberOfMUAClu)) + \" (\" + TotalNumberOfMUACluPerShank[:-1] + \")\",sum(TotalNumberOfMUAClu)]\n",
    "       \n",
    "    else:\n",
    "        MUACluPerSession=[\"0\",0]\n",
    "\n",
    "    print(\"session %s\" %data.experiment)\n",
    "    return GoodCluPerSession,MUACluPerSession\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir(): \n",
    "    GoodCluPerSession,MUACluPerSession=CountCluPerSession(data)\n",
    "    \n",
    "    print(\"%s Good clusters and %s MUA clusters\" %(GoodCluPerSession[0],MUACluPerSession[0]))\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List how many good and MUA clusters there are accross a group of analyzed sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def TableAllCluAccrosSesson(animalList=[],tagList = [\"GoodPerfo\"]):\n",
    "    \n",
    "    #animalList=[\"MOU025\",\"MOU026\",\"MOU027\",\"MOU035\",\"MOU074\"]\n",
    "    AllGoodCluAccrossSessions=0\n",
    "    AllMUACluAccrossSessions=0\n",
    "    CluPerSessions=[]\n",
    "    if not animalList:\n",
    "        animalList = [os.path.basename(path) for path in sorted(glob.glob(root+\"/MOU*\"))]\n",
    "    \n",
    "\n",
    "    #list of tags (tag = empty file in the session folder with a specific name)\n",
    "    #leave empty for no tag\n",
    "    tagList = tagList\n",
    "    \n",
    "    for animal in animalList:\n",
    "        print(\"Animal\", animal)\n",
    "        \n",
    "        ThisAnimalGoodCluAccrossSessions=0\n",
    "        ThisAnimalMUACluAccrossSessions=0\n",
    "        \n",
    "        #Get the list of all session\n",
    "        sessionList = [os.path.basename(expPath) for expPath in glob.glob(root+\"/\"+animal+\"/Experiments/MOU*\")]\n",
    "        sessionList = sorted(sessionList)\n",
    "\n",
    "        #loop through sessions\n",
    "        for session in sessionList:  \n",
    "    \n",
    "            #if tag list is not emtpy\n",
    "            if tagList:\n",
    "                #check if the session has one of the tag\n",
    "                if not has_tag(root, animal, session, tagList):\n",
    "                    continue\n",
    "    \n",
    "                print(session)\n",
    "                #load data for this session (add redoPreprocess=True to overwrite preprocess)\n",
    "                data = Data(root, animal, session, paramCarola, redoPreprocess=False)\n",
    "                \n",
    "                if not data.hasSpike:\n",
    "                    print(\"########\")\n",
    "                    print(\"%s has no spike\" %data.experiment)\n",
    "                    print(\"########\")\n",
    "                    continue\n",
    "            \n",
    "                \n",
    "                \n",
    "                GoodCluThisSession,MUACluThisSession=CountCluPerSession(data)\n",
    "                CluPerSessions.append([data.experiment,GoodCluThisSession[0],MUACluThisSession[0]])\n",
    "                \n",
    "                AllGoodCluAccrossSessions+=GoodCluThisSession[1]\n",
    "                AllMUACluAccrossSessions+=MUACluThisSession[1]\n",
    "                \n",
    "                ThisAnimalGoodCluAccrossSessions+=GoodCluThisSession[1]\n",
    "                ThisAnimalMUACluAccrossSessions+=MUACluThisSession[1]\n",
    "                \n",
    "        if ThisAnimalGoodCluAccrossSessions>0:\n",
    "            CluPerSessions.append([\"   Total for \"+animal,ThisAnimalGoodCluAccrossSessions,ThisAnimalMUACluAccrossSessions])\n",
    "            CluPerSessions.append([\"\",\"\",\"\"])\n",
    "        \n",
    "    \n",
    "    # last line is total and is spearated by empty line\n",
    "    \n",
    "#     CluPerSessions.append([\"\",\"\",\"\"])\n",
    "    CluPerSessions.append([\"TOTAL\",AllGoodCluAccrossSessions,AllMUACluAccrossSessions])\n",
    "    from IPython.display import clear_output\n",
    "    clear_output()            \n",
    "    \n",
    "    return CluPerSessions,AllGoodCluAccrossSessions,AllMUACluAccrossSessions,tagList\n",
    "\n",
    "if \"__file__\" not in dir(): \n",
    "    animalList=[]\n",
    "    CluPerSessions,AllGoodCluAccrossSessions,AllMUACluAccrossSessions,tagList=TableAllCluAccrosSesson(animalList)\n",
    "    \n",
    "    from tabulate import tabulate\n",
    "    headers = [\"session\", \"Good clusters\",\"MUA clusters\"]\n",
    "    print(tabulate(CluPerSessions,headers,tablefmt=\"fancy_grid\"))\n",
    "\n",
    "    \n",
    "    \n",
    "    PathForTableSaving=os.path.join(root,\"ALLMOU_Analysis\",tagList[0]+\"_Table.txt\")\n",
    "    f = open(PathForTableSaving, 'w')\n",
    "    f.write(tabulate(CluPerSessions,headers,tablefmt=\"grid\"))\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.2 Plot wheel break with spikes for one cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_cluster_spikes(data, shank, cluster):\n",
    "    \"\"\"\n",
    "    Returns list of spikes for a cluster, if the cluster exists\n",
    "    \"\"\"\n",
    "    if not data.hasSpike:\n",
    "        print(\"No spike data\")\n",
    "        return None\n",
    "    try:\n",
    "        cluSpike=data.spikeTime[shank][cluster]\n",
    "    except KeyError:\n",
    "        print(\"No shank %s cluster %s\"%(shank,cluster))\n",
    "        print(\"List of clusters for this session:\")\n",
    "        print(data.clusterGroup)\n",
    "        return None\n",
    "    return cluSpike\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "def plot_break_cluster(data, shank, cluster, group=\"not defined\", legend=False, colorOpto=\"yellow\",\n",
    "                       xmax=60, ax=None, lick=False):\n",
    "    \"\"\"\n",
    "    Calls plot_break, and add the spikes for one cluster on top of it\n",
    "    'group' is the cluster group (\"good\", \"noise\",...), only used for the title of the plot\n",
    "    \"\"\"\n",
    "    #get the spikes for the cluster, if it exists\n",
    "    cluSpike = get_cluster_spikes(data, shank, cluster)\n",
    "    if cluSpike is None:\n",
    "        return\n",
    "    #plot the beam breaks, lick breaks and optogenetic\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    title=plot_break(data, legend=legend, colorOpto=colorOpto ,xmax=xmax, ax=ax, lick=lick)\n",
    "    #add spikes for the given cluster\n",
    "    lines=[]   \n",
    "    for trial in data.trials:\n",
    "        start=data.realStartTrial[trial]\n",
    "        zero=start+data.durationTrial[trial]\n",
    "        stop=zero+data.durationInterTrial[trial]\n",
    "        trialSpikes=cluSpike[(cluSpike>start)&(cluSpike<stop)]-zero\n",
    "        for spike in trialSpikes:\n",
    "            lines.append([(spike,trial+1.1),(spike,trial+1.5)])\n",
    "    lc= mc.LineCollection(lines, colors=\"red\", label=\"spikes of shank %s cluster %s\"%(shank, cluster))\n",
    "    ax.add_collection(lc)\n",
    "            \n",
    "    if legend:\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "    plt.title(title+\" (shank %s cluster %s, group %s)\"%(shank,cluster,group))\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():    \n",
    "    if data.hasBehavior and data.hasSpike:\n",
    "        plt.figure(figsize=(15,20))\n",
    "        shank=1\n",
    "        cluster=2\n",
    "        \n",
    "        \"\"\"\n",
    "        same than for the data path. if you want to look at other clusters name them between the 2 hash tage lines\n",
    "        and do not forget to leave them empty when you commit\n",
    "        \"\"\"\n",
    "        \n",
    "        ##############################\n",
    "\n",
    "        \n",
    "        ##############################\n",
    "        \n",
    "        plot_break_cluster(data,shank,cluster,group=\"Good\",legend=True,lick=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3 Mean Speed, Lick and Firing Rate, aligned to trial end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_mean_breaks_firing_rate(data, shank, cluster, binSize=0.25, minTime=-60, maxTime=20, align=\"trial end\",\n",
    "                                 sigma=1, trialType=\"all\", lick=False, s=1, ax=None):\n",
    "    '''\n",
    "    Calls plot_mean_breaks (mean running/licking on all trials) and add cluster spikes on top of it.\n",
    "    Input\n",
    "      -s: sigma to smoothed the curve\n",
    "      -trialType: consider only the \"good\" or \"bad\", or \"all\" trials\n",
    "      -lick: True to replace running by lick frequency\n",
    "    '''\n",
    "    #get the spikes for the cluster, if it exists\n",
    "    cluSpike = get_cluster_spikes(data, shank, cluster)\n",
    "    if cluSpike is None:\n",
    "        return\n",
    "    #bins between minTime and maxTime\n",
    "    timeBin = np.arange(minTime, maxTime + binSize - maxTime%binSize, binSize)\n",
    "    centers = (timeBin[:-1]+timeBin[1:]) / 2.0\n",
    "    #plot speed, remove title\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    if trialType == \"good\":\n",
    "        plot_mean_breaks(data, binSize, minTime, maxTime, align, separate=\"good trial\", lick=lick,\n",
    "                         displayOnly=1, s=s)\n",
    "    elif trialType == \"bad\":\n",
    "        plot_mean_breaks(data, binSize, minTime, maxTime, align, separate=\"good trial\", lick=lick,\n",
    "                         displayOnly=2, s=s)\n",
    "    else:\n",
    "        plot_mean_breaks(data, binSize, minTime, maxTime, align, separate=\"none\", lick=lick, s=s)\n",
    "    plt.title(\"\")\n",
    "\n",
    "    #histogram\n",
    "    allHist = []\n",
    "    for trial in data.trials:\n",
    "        if (trialType == \"good\") and (trial not in data.goodTrials):\n",
    "            continue\n",
    "        if (trialType == \"bad\") and (trial in data.goodTrials):\n",
    "            continue\n",
    "        if align==\"trial end\":\n",
    "            zero = data.realStartTrial[trial] + data.durationTrial[trial]\n",
    "        elif align==\"trial start\":\n",
    "            zero = data.realStartTrial[trial]\n",
    "            \n",
    "        alignedTime = cluSpike-zero\n",
    "        hist,bins = np.histogram(alignedTime, timeBin)\n",
    "        allHist.append(hist)\n",
    "        \n",
    "    #mean firing rate, smoothed\n",
    "    meanFiring = np.nanmean(np.asarray(allHist),axis=0) / float(binSize)\n",
    "    meanFiring = smooth(meanFiring,sigma)\n",
    "    \n",
    "    #plot firing rate on a new y axis\n",
    "    ax2 = ax.twinx()\n",
    "    plt.plot(centers, smooth(meanFiring, s), color=\"red\")\n",
    "    plt.ylabel(\"firing rate (smooth sigma=%s)\"%sigma, color=\"red\", fontsize=14)\n",
    "    plt.ylim([0, max(meanFiring)])\n",
    "    ax2.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    for tl in ax2.get_yticklabels():\n",
    "        tl.set_color('red')\n",
    "    #title\n",
    "    if lick:\n",
    "        title = \"Lick\"\n",
    "    else:\n",
    "        title = \"Running\"\n",
    "    title = title+\", Shank %s Cluster %s, %s trials (%i)\" %(shank, cluster, trialType, len(allHist))\n",
    "    plt.title(title, fontsize=14)\n",
    "    return ax2\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():      \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.subplot(221)\n",
    "    plot_mean_breaks_firing_rate(data, shank, cluster, trialType=\"good\")\n",
    "    plt.subplot(222)\n",
    "    plot_mean_breaks_firing_rate(data, shank, cluster)\n",
    "    plt.subplot(223)\n",
    "    plot_mean_breaks_firing_rate(data, shank, cluster, lick=True, minTime=-10, maxTime=20, trialType=\"good\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Firing rate aligned relative to start or end of behavioral epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_period_firing_rate(data, shank, cluster, binSize=0.25, minTime=-20, maxTime=20,\n",
    "                            align=\"start\", minDurationSecond=2, maxDurationSecond=None, sigma=1, ax=None,\n",
    "                            immobility=False, periodType=\"all\", runMinDuration=0.1):\n",
    "    \"\"\"\n",
    "    Plots running or immobility periods, aligned on start or end of period, along with the firing rate of one cluster\n",
    "    \"\"\"\n",
    "    #get the spikes for the cluster, if it exists\n",
    "    cluSpike = get_cluster_spikes(data, shank, cluster)\n",
    "    if cluSpike is None:\n",
    "        return    \n",
    "    if ax is None:\n",
    "        ax=plt.gca()        \n",
    "    #detect periods\n",
    "    if immobility:\n",
    "        startRunning,endRunning,periods=detect_immobility_period(data,minDurationSecond,maxDurationSecond,periodType,\n",
    "                                                                runMinDuration)\n",
    "        runOrImmobility=\"immobility\"\n",
    "    else:\n",
    "        #start and end of running\n",
    "        startRunning,endRunning,periods=detect_running_period(data,minDurationSecond,maxDurationSecond,periodType)\n",
    "        runOrImmobility=\"run\"\n",
    "    #where to align\n",
    "    if align==\"start\":\n",
    "        zeroes=startRunning\n",
    "    else:\n",
    "        zeroes=endRunning        \n",
    "    #breaks\n",
    "    allBreaks=np.asarray(data.allBeamBreak)\n",
    "    #histogram\n",
    "    timeBin=np.arange(minTime,maxTime+binSize-maxTime%binSize,binSize)\n",
    "    centers=(timeBin[:-1]+timeBin[1:])/2.0   \n",
    "    allHist=[]\n",
    "    spikeHist=[]\n",
    "    for zero in zeroes:\n",
    "        #breaks\n",
    "        alignBreak=allBreaks-zero\n",
    "        hist,bins=np.histogram(alignBreak,timeBin)\n",
    "        allHist.append(hist)\n",
    "        #spikes\n",
    "        alignedSpikeTimes=cluSpike-zero\n",
    "        hist,bins=np.histogram(alignedSpikeTimes,timeBin)\n",
    "        spikeHist.append(hist)      \n",
    "    #speed\n",
    "    meanSpeed=np.nanmean(np.asarray(allHist),axis=0)*data.tickDistance/float(binSize)\n",
    "    plt.plot(centers,meanSpeed,color=\"black\")\n",
    "    plt.ylabel(\"mean running speed (cm/s)\",fontsize=14)    \n",
    "    plt.xlabel(\"time (sec), binSize=%ss, 0=%s\"%(binSize,align),fontsize=14)\n",
    "    #firing rate on second axis\n",
    "    ax2=ax.twinx()\n",
    "    meanFiring=np.nanmean(np.asarray(spikeHist),axis=0)/float(binSize)\n",
    "    meanFiring=smooth(meanFiring,sigma)\n",
    "    plt.plot(centers,meanFiring,color=\"red\") \n",
    "    plt.ylim([0,max(meanFiring)])\n",
    "    plt.ylabel(\"firing rate (smooth sigma=%s)\"%sigma,color=\"red\",fontsize=14)\n",
    "    ax2.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    for tl in ax2.get_yticklabels():\n",
    "        tl.set_color('red')          \n",
    "    #title\n",
    "    title=\"All %s periods, Shank %s Cluster %s\"%(runOrImmobility,shank,cluster)\n",
    "    plt.title(title,fontsize=14)\n",
    "    plt.xlim([minTime,maxTime])\n",
    "    return ax2\n",
    "    \n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir(): \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.subplot(221)\n",
    "    plot_period_firing_rate(data,shank,cluster,align=\"start\")\n",
    "    \n",
    "    plt.subplot(222)\n",
    "    plot_period_firing_rate(data,shank,cluster,align=\"end\")\n",
    "\n",
    "    plt.subplot(223)\n",
    "    plot_period_firing_rate(data,shank,cluster,align=\"start\",immobility=True,runMinDuration=0)\n",
    "    \n",
    "    plt.subplot(224)\n",
    "    plot_period_firing_rate(data,shank,cluster,align=\"end\",immobility=True,runMinDuration=0)\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.6)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Firing rates during behavioral epochs whose lengthes have been normalized in this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_normalized_running_periods_firing_rate(data, shank, cluster, binSize=0.25, minDuration=2, \n",
    "                                                maxDuration=15, SideLength=3, sigma=1, runType=\"all\", ax=None):\n",
    "    return plot_normalized_periods_firing_rate(data, shank, cluster, binSize=binSize, minDuration=minDuration, \n",
    "                                               maxDuration=maxDuration, SideLength=SideLength, sigma=sigma,\n",
    "                                               periodType=runType, ax=ax, immobility=False, runMinDuration=None)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def plot_normalized_immobility_periods_firing_rate(data, shank, cluster, binSize=0.25, minDuration=2,\n",
    "                                                   maxDuration=15, SideLength=3, sigma=1, immobilityType=\"all\",\n",
    "                                                   runMinDuration=0.1, ax=None):\n",
    "    return plot_normalized_periods_firing_rate(data, shank, cluster, binSize=binSize, minDuration=minDuration, \n",
    "                                    maxDuration=maxDuration, SideLength=SideLength, sigma=sigma, periodType=immobilityType,\n",
    "                                    ax=ax, immobility=True, runMinDuration=runMinDuration)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def plot_normalized_periods_firing_rate(data, shank, cluster, binSize=0.25, minDuration=2, maxDuration=15, SideLength=3,\n",
    "                                        sigma=1, periodType=\"all\", immobility=False, runMinDuration=0.1, ax=None):\n",
    "    \"\"\"\n",
    "    Takes all running/immobility periods between minDuration seconds and maxDuration seconds\n",
    "    Computes the mean running/immobility period duration, and divide it by binSize to get the number of bins\n",
    "    For each running/immobility period, compute the speed given the number of bins, as well as the firing rate\n",
    "    Plots the mean of the speeds and the mean of the firing rates.\n",
    "    Before and after the running/immobility periods, nSideBin are also plotted.\n",
    "    Input:\n",
    "      - data, shank, cluster \n",
    "      - binSize, in seconds\n",
    "      - minDuration, minimum duration of a period\n",
    "      - maxDuration, maximum duration of a period\n",
    "      - nSideBin, number of bins to consider before and after the period\n",
    "      - sigma, parameter for the gaussian smoothing of the firing rate \n",
    "      - periodType, to specify a type of period\n",
    "           for the runs: \"trial good run\", \"trial bad run\", \"intertrial\"\n",
    "           for the immobility: \"trial\", \"intertrial\"\n",
    "      - immobility, whether to plot immobility periods or running periods\n",
    "      - runMinDuration, parameter for immobility period detection \n",
    "        (a run of less than runMinDuration is considered to be an immobility)\n",
    "      - ax, matplotlib figure axe where to plot, useful when doing complex subplots\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()     \n",
    "    #spikes\n",
    "    cluSpike = data.spikeTime[shank][cluster]\n",
    "    \n",
    "    if immobility:\n",
    "        #immobility periods\n",
    "        startPeriod, endPeriod, indexes = detect_immobility_period(data, minDuration, maxDuration, periodType,\n",
    "                                                                   runMinDuration)\n",
    "        runOrImmobile = \"immobility\"\n",
    "    else:\n",
    "        #running periods\n",
    "        startPeriod, endPeriod, indexes = detect_running_period(data, minDuration, maxDuration, periodType)\n",
    "        runOrImmobile = \"running\"\n",
    "    \n",
    "    #Detect number of bins based on mean duration and binsize\n",
    "    allDuration = np.asarray(endPeriod) - np.asarray(startPeriod)\n",
    "    meanDuration = np.nanmean(allDuration)\n",
    "    nBins = np.ceil(meanDuration/float(binSize))    \n",
    "    \n",
    "    nSideBin=int(np.round(SideLength/binSize)) \n",
    "    #mean speed and firing rate in the bins\n",
    "    allSpeed = []\n",
    "    spikeHist = []\n",
    "    rDuration = []\n",
    "    for start, stop, duration in zip(startPeriod, endPeriod, allDuration):\n",
    "        _bin = duration/float(nBins)\n",
    "        rStart = start - (nSideBin+1) * _bin\n",
    "        rStop = stop+ (nSideBin+1)*_bin - stop%_bin\n",
    "        timeBin = np.arange(rStart, rStop+_bin, _bin)        \n",
    "        hist, bins = np.histogram(data.allBeamBreak, timeBin)\n",
    "        allSpeed.append(hist*data.tickDistance/float(_bin))\n",
    "        hist, bins = np.histogram(cluSpike, timeBin)\n",
    "        spikeHist.append(hist/float(_bin))\n",
    "        rDuration.append(rStop-rStart)\n",
    "        \n",
    "    #compute the means and smooth\n",
    "    meanDuration = np.nanmean(allDuration)\n",
    "    meanSpeed = np.nanmean(np.asarray(allSpeed), axis=0)\n",
    "    \n",
    "    meanFiring = np.nanmean(np.asarray(spikeHist), axis=0)\n",
    "    meanFiring = smooth(meanFiring, sigma)\n",
    "    \n",
    "    xmax = nBins + (nSideBin+1) * 2\n",
    "    xaxis = np.arange(0.5, xmax, 1)\n",
    "    #plot speed\n",
    "    plt.plot(xaxis, meanSpeed, color=\"black\")\n",
    "    plt.ylabel(\"mean running speed (cm/s)\", fontsize=14)\n",
    "    plt.xlabel(\"%i bins, mean duration %.2fs with binsize %.2fs\" %(xmax-1, meanDuration, binSize), fontsize=14)\n",
    "    #plot firing rate on other y axis with different coor\n",
    "    ax2 = ax.twinx()\n",
    "    plt.plot(xaxis, meanFiring, color=\"red\") \n",
    "    plt.ylim([0,max(meanFiring)])\n",
    "    plt.ylabel(\"firing rate (smooth=%s)\" %sigma, color=\"red\", fontsize=14)\n",
    "    ax2.yaxis.set_major_locator(plt.MaxNLocator(integer=True))\n",
    "    for tl in ax2.get_yticklabels():\n",
    "        tl.set_color('red')\n",
    "    #title\n",
    "    title=\"'%s' normalized %s periods (%s) \\n (%s-%s s)\"%(periodType, runOrImmobile, len(startPeriod), \n",
    "                                                          minDuration, maxDuration)\n",
    "    title=title+\", Shank %s Cluster %s\"%(shank, cluster)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlim([0, xmax])\n",
    "\n",
    "    return ax2, meanFiring, nSideBin, spikeHist, allSpeed, allDuration\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    binSize=0.25\n",
    "    plt.figure(figsize=(20,15))\n",
    "    \n",
    "    plt.subplot(521)\n",
    "    ax2,meanFiring,nSideBin,spikeHist,allSpeed,allDuration=plot_normalized_running_periods_firing_rate(data, shank, cluster,binSize)\n",
    "    \n",
    "    plt.subplot(527)\n",
    "    plot_normalized_running_periods_firing_rate(data, shank, cluster, binSize,runType=\"intertrial\")\n",
    "    \n",
    "    plt.subplot(529)\n",
    "    plot_normalized_running_periods_firing_rate(data, shank, cluster, binSize, runType=\"unrewarded\")\n",
    "    \n",
    "    plt.subplot(523)\n",
    "    plot_normalized_running_periods_firing_rate(data, shank, cluster, binSize,runType=\"trial good run\")\n",
    "    \n",
    "    plt.subplot(525)\n",
    "    plot_normalized_running_periods_firing_rate(data, shank, cluster,binSize, runType=\"trial bad run\")\n",
    "    \n",
    "    plt.subplot(522)\n",
    "    plot_normalized_immobility_periods_firing_rate(data, shank, cluster,binSize)\n",
    "    \n",
    "    plt.subplot(528)\n",
    "    plot_normalized_immobility_periods_firing_rate(data, shank, cluster,binSize, immobilityType=\"intertrial\")\n",
    "    \n",
    "    plt.subplot(524)\n",
    "    plot_normalized_immobility_periods_firing_rate(data, shank, cluster,binSize, immobilityType=\"trial\")\n",
    "    \n",
    "    plt.subplots_adjust(hspace=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firing rate versus Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def FiringVersusDistanceInRuns(data,shank,cluster,runType=\"all\",binSize=0.025,plotIndex=False,checkrundetection=False):\n",
    "    binSize=0.025\n",
    "    tickDistance=data.tickDistance\n",
    "    \n",
    "    \n",
    "    ## First detect the run\n",
    "    start, end, indexes = detect_running_period(data, minDurationSecond=3, maxDurationSecond=30,runType=\"all\",maxTimeBetweenBreak=1)\n",
    "    RunDurations = np.asarray(end) - np.asarray(start)\n",
    "    \n",
    "    if checkrundetection:\n",
    "        check_period_detection(data, start, end)\n",
    "    \n",
    "\n",
    "    ## Load spike time \n",
    "    cluSpike = data.spikeTime[shank][cluster]\n",
    "    \n",
    "    \n",
    "    ### FIRST part : trial by trial tuning curves for firing rate versus distance\n",
    "    \n",
    "    InstantaneousRunDistancesConcatenatedAccrossRuns=[]\n",
    "    allBeamBreakTimes=data.allBeamBreak\n",
    "    TotalDistancePerRunAcrrossAllRuns=[]\n",
    "    InstantaneousFRateConcatenatedAccrossRuns=[]\n",
    "    TimeBinCentersConcatenatedAccrossTrials=[]\n",
    "    \n",
    "    MeanFiringRateVerusDistance=[]\n",
    "    \n",
    "    MeanFiringRateVersusTime=[]\n",
    "    FRateVersusDistanceAcrossRuns=[]\n",
    "    FRateVersusTimeAcrossRuns=[]\n",
    "    AllTimesBinsAccrossAllTrials=[]\n",
    "    AllDistancesAccrosAllTrials=[]\n",
    "    \n",
    "    for thisstart, thisend in zip(start, end):\n",
    "        \n",
    "        ## for each detected run finds the beam break times and realign them relative to the first one (zero)\n",
    "        BeamBreakTimesInsideDetectedRun=[X for X in allBeamBreakTimes if X>=thisstart and X<=thisend]\n",
    "        BeamBreakTimesInsideDetectedRun=[X-BeamBreakTimesInsideDetectedRun[0] for X in BeamBreakTimesInsideDetectedRun]\n",
    "        minTime=BeamBreakTimesInsideDetectedRun[0]\n",
    "        maxTime=BeamBreakTimesInsideDetectedRun[-1]\n",
    "        timeBin=np.arange(minTime,maxTime+binSize-maxTime%binSize+binSize,binSize)\n",
    "        centers=(timeBin[:-1]+timeBin[1:])/2.0 \n",
    "        \n",
    "        ## histogram of number of detected beambreak in time bins during run\n",
    "        \n",
    "        hist,bins=np.histogram(BeamBreakTimesInsideDetectedRun,timeBin)\n",
    "#       print(np.cumsum(hist*tickDistance))\n",
    "#       print(\"###\")\n",
    "        InstantaneousRunDistancesConcatenatedAccrossRuns.extend(np.cumsum(hist*tickDistance))\n",
    "        TotalDistancePerRunAcrrossAllRuns.extend([np.sum(hist)*tickDistance])\n",
    "        \n",
    "        ## for the same time bin, get the spike count, then transform to rate\n",
    "        alignedSpikeTimes=cluSpike-thisstart\n",
    "        histspike, bins = np.histogram(alignedSpikeTimes, timeBin)        \n",
    "        InstantaneousFRateConcatenatedAccrossRuns.extend(histspike/binSize)\n",
    "        TimeBinCentersConcatenatedAccrossTrials.extend(centers)\n",
    "        \n",
    "        ### for a given run,firing rate versus distance tuning curve \n",
    "        FRateVersusDistancePerRun=[]\n",
    "        InstantaneousDistancesInThisRun=np.cumsum(hist*tickDistance)\n",
    "        Distances=np.unique(InstantaneousDistancesInThisRun)\n",
    "        for distance in Distances:\n",
    "            thisdistanceindexes=np.where(InstantaneousDistancesInThisRun==distance)[0].tolist()\n",
    "            FRateThisDistanceValues=[histspike[i] for i in thisdistanceindexes]\n",
    "            FRateVersusDistancePerRun.extend([np.mean(FRateThisDistanceValues)/binSize])\n",
    "        \n",
    "        ### Run by run firing rate versus distance tuning curve\n",
    "        FRateVersusDistanceAcrossRuns.append(FRateVersusDistancePerRun)\n",
    "        AllDistancesAccrosAllTrials.append(Distances.tolist())\n",
    "        \n",
    "        ### for a given run,firing rate versus distance tuning curve \n",
    "        binSizeTimeTuningCurve=0.125\n",
    "        timeBinForTime=np.arange(thisstart,thisend+binSizeTimeTuningCurve-thisend%binSizeTimeTuningCurve+binSizeTimeTuningCurve,binSizeTimeTuningCurve)\n",
    "        histSpikeCountVerusTimeInRun, bins = np.histogram(cluSpike, timeBinForTime)\n",
    "        histFRateVersusTimeInRun=[float(i) for i in histSpikeCountVerusTimeInRun/binSizeTimeTuningCurve]\n",
    "        FRateVersusTimeAcrossRuns.append(histFRateVersusTimeInRun)\n",
    "        bins=np.asarray([X-bins[0] for X in bins])\n",
    "        centerofbins=(bins[:-1]+bins[1:])/2.0\n",
    "        AllTimesBinsAccrossAllTrials.append(centerofbins.tolist())\n",
    "        \n",
    "\n",
    "    #Compute average tuning curve from all instantaneous data \n",
    "    Distances=np.unique(InstantaneousRunDistancesConcatenatedAccrossRuns)\n",
    "    for distance in Distances:\n",
    "        thisdistanceindexes=np.where(InstantaneousRunDistancesConcatenatedAccrossRuns==distance)[0].tolist()\n",
    "        FRateThisDistanceValues=[InstantaneousFRateConcatenatedAccrossRuns[i] for i in thisdistanceindexes]\n",
    "        MeanFiringRateVerusDistance.extend([np.mean(FRateThisDistanceValues)])\n",
    "\n",
    "    UniqueTimeCenters=np.unique(TimeBinCentersConcatenatedAccrossTrials)    \n",
    "    for timecenter in UniqueTimeCenters:\n",
    "        thistimecenterindexes=np.where(TimeBinCentersConcatenatedAccrossTrials==timecenter)[0].tolist()\n",
    "        FRateThisTimeCenterValues=[InstantaneousFRateConcatenatedAccrossRuns[i] for i in thistimecenterindexes]\n",
    "        MeanFiringRateVersusTime.extend([np.mean(FRateThisTimeCenterValues)])\n",
    "        \n",
    "    AllTimes=[item for sublist in AllTimesBinsAccrossAllTrials for item in sublist]\n",
    "    AllFRateVersusTimeAcrossRuns=[item for sublist in FRateVersusTimeAcrossRuns for item in sublist]\n",
    "    MeanFRatePerTimeBin=[]\n",
    "    UniqueTimeCentersLargeBins=np.unique(AllTimes)\n",
    "    for time in UniqueTimeCentersLargeBins:\n",
    "        thistimeeindexes=np.where(AllTimes==time)[0].tolist()\n",
    "        FRateThisTimeBin=[AllFRateVersusTimeAcrossRuns[i] for i in thistimeeindexes]\n",
    "        MeanFRatePerTimeBin.extend([np.mean(FRateThisTimeBin)])\n",
    "\n",
    "    \n",
    "    ### Part II: some plotting and comparaison with firing rate versus time   \n",
    "        \n",
    "    if plotIndex:\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        plt.subplot(131)\n",
    "        plt.plot(UniqueTimeCenters,smooth(MeanFiringRateVersusTime,1))\n",
    "        plt.xlabel('run time (s)')\n",
    "        plt.ylabel(\"Firing Rate (Hz)\")\n",
    "        \n",
    "        plt.subplot(132)\n",
    "        plt.plot(UniqueTimeCentersLargeBins,smooth(MeanFRatePerTimeBin,1))\n",
    "        plt.xlabel('run time (s)')\n",
    "        plt.ylabel(\"Firing Rate (Hz)\")\n",
    "        \n",
    "        plt.subplot(133)\n",
    "        plt.plot(Distances,smooth(MeanFiringRateVerusDistance,1))\n",
    "        plt.xlabel('run distance (cm)')\n",
    "        plt.ylabel(\"Firing Rate (Hz)\")\n",
    "        \n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(TotalDistancePerRunAcrrossAllRuns,RunDurations)\n",
    "        plt.xlabel('run distance (cm)')\n",
    "        plt.ylabel(\"run duration (s)\")\n",
    "        SpearManResults=stats.spearmanr(TotalDistancePerRunAcrrossAllRuns,RunDurations)\n",
    "        \n",
    "        rvalue=str(round(SpearManResults[0],2));\n",
    "        if SpearManResults[1]<0.0001:\n",
    "            pvalue=\"p<0.0001\"\n",
    "        else:\n",
    "            pvalue=\"p=\"+ str(round(SpearManResults[1],4))\n",
    "\n",
    "        title=\"r=%s, %s\"%(rvalue,pvalue)\n",
    "        plt.title(title)\n",
    "        \n",
    "        print(SpearManResults)\n",
    "    \n",
    "    return FRateVersusDistanceAcrossRuns,AllDistancesAccrosAllTrials\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    \n",
    "    FiringVersusDistanceInRuns(data,shank,cluster,runType=\"all\",plotIndex=True,checkrundetection=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Firing rate versus Distance Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FiringRateVersusDistanceInRunDistanceNormalized(data,shank,cluster,NBinsForNormalization=20,runType=\"all\"):\n",
    "\n",
    "    FRateVersusDistanceAcrossRuns,AllDistancesAccrosAllTrials=FiringVersusDistanceInRuns(data,shank=shank,cluster=cluster,runType=runType)\n",
    "\n",
    "    NormalizeFiringRatesAccrossRuns=[]\n",
    "    for FRateVersusDistance,Distance in zip(FRateVersusDistanceAcrossRuns,AllDistancesAccrosAllTrials):\n",
    "        NormalizeFiringRateThisRun=[]\n",
    "        DistanceBinBorders = np.linspace(0, Distance[-1], NBinsForNormalization+1)\n",
    "\n",
    "        for low, hig in zip(DistanceBinBorders[0:-1], DistanceBinBorders[1:]):\n",
    "            thisdistanceindexes=np.where((Distance>low) & (Distance<=hig))[0].tolist()\n",
    "            thisFiringRate=[FRateVersusDistance[i] for i in thisdistanceindexes]\n",
    "    #        if len(thisFiringRate)==0:\n",
    "    #             print(\"Distance: %s\" %Distance)\n",
    "    #             print(\"\")\n",
    "    #             print(\"low: %s, hig: %s\" %(low,hig))\n",
    "    #             print(\"\")\n",
    "    #             print(\"thisdistanceindexes: %s\" %thisdistanceindexes)\n",
    "    #             print(\"\")\n",
    "\n",
    "    #             print(\"##############\")\n",
    "\n",
    "            if len(thisdistanceindexes)>0:\n",
    "                NormalizeFiringRateThisRun.extend([np.nanmean(thisFiringRate)])\n",
    "            else:\n",
    "                NormalizeFiringRateThisRun.extend([float('nan')])\n",
    "    #            print(\"nan value injected\")\n",
    "\n",
    "\n",
    "        ## check if there is nan value, if yes replace them using interp fonction\n",
    "        NonNanIndexes = [i for i,x in enumerate(np.isnan(NormalizeFiringRateThisRun).tolist()) if x == False]\n",
    "        if len(NonNanIndexes)>0:\n",
    "            NonNanValues = [x for x in NormalizeFiringRateThisRun if not np.isnan(x)]\n",
    "            NormalizeFiringRateThisRun=np.interp(np.arange(0,len(NormalizeFiringRateThisRun)),NonNanIndexes,NonNanValues).tolist()\n",
    "\n",
    "\n",
    "\n",
    "        NormalizeFiringRatesAccrossRuns.append(NormalizeFiringRateThisRun)\n",
    "\n",
    "    MeanFiringRateNormalizedDistance=np.mean(np.asarray(NormalizeFiringRatesAccrossRuns),0)\n",
    "    BinEdges,BinSTEP=np.linspace(0,1,NBinsForNormalization+1, retstep=True)\n",
    "    BinCenterS=BinEdges[:-1]+BinSTEP/2.0\n",
    "    \n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.annotate('run start', xy=(0, -0.1), xycoords='axes fraction',\n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "    ax.annotate('run stop', xy=(1, -0.1), xycoords='axes fraction',\n",
    "                horizontalalignment='center', verticalalignment='center')\n",
    "    plt.plot(BinCenterS,MeanFiringRateNormalizedDistance)\n",
    "    plt.xlabel('run distance (normalized)')\n",
    "    plt.ylabel(\"Firing Rate (Hz)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    ##########################\n",
    "    \n",
    "    \n",
    "    ##########################\n",
    "    \n",
    "    FiringRateVersusDistanceInRunDistanceNormalized(data,shank,cluster,NBinsForNormalization=20,runType=\"all\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Real and SHUFFLED firing rates during behavioral epochs whose lengthes have been normalized inside this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_shuffled_running_firing_rate(data, shank, cluster, binSize=0.25, minDuration=2, maxDuration=15, \n",
    "                                      SideLength=3, sigma=1, runType=\"all\", ax=None, nShuffle=500,noPlot=True):\n",
    "    return plot_shuffled_firing_rate(data, shank, cluster, binSize=binSize, minDuration=minDuration, \n",
    "                                     maxDuration=maxDuration, SideLength=SideLength, sigma=sigma,periodType=runType,\n",
    "                                     ax=ax, immobility=False, runMinDuration=None, nShuffle=nShuffle,noPlot=noPlot)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def plot_shuffled_immobility_firing_rate(data, shank, cluster, binSize=0.25, minDuration=2, maxDuration=15, \n",
    "                                         SideLength=3, sigma=1, immobilityType=\"all\", runMinDuration=0.1, \n",
    "                                         ax=None, nShuffle=500,noPlot=True):\n",
    "    return plot_shuffled_firing_rate(data, shank, cluster, binSize=binSize, minDuration=minDuration, \n",
    "                                     maxDuration=maxDuration, SideLength=SideLength, sigma=sigma, \n",
    "                                     periodType=immobilityType, ax=ax, immobility=True, runMinDuration=runMinDuration, \n",
    "                                     nShuffle = nShuffle,noPlot=noPlot)\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "def plot_shuffled_firing_rate(data, shank, cluster, binSize=0.25, minDuration=2, maxDuration=15, SideLength=3, sigma=1,\n",
    "                              periodType=\"all\", immobility=False,runMinDuration=0.1, ax=None, nShuffle=200, \n",
    "                              noPlot=True):\n",
    "    \"\"\"\n",
    "    Plot a confidence interval for plot_normalized_periods_firing_rate.  \n",
    "    Same code and arguments as plot_normalized_periods_firing_rate, but the periods are randomly shifted\n",
    "    The process is done nShuffle times. If noPlot=True, nothing will be plotted. \n",
    "    Returns the percentile 0% (min), 5%, 95%, 100% (max)\n",
    "    \"\"\"\n",
    "    if (ax is None) and (not noPlot):\n",
    "        ax = plt.gca()     \n",
    "    nSideBin=int(np.round(SideLength/binSize))\n",
    "    #spikes\n",
    "    cluSpike = data.spikeTime[shank][cluster]\n",
    "    \n",
    "    if immobility:\n",
    "        #immobility periods\n",
    "        startPeriod, endPeriod, indexes = detect_immobility_period(data, minDuration, maxDuration, periodType,\n",
    "                                                                   runMinDuration)\n",
    "    else:\n",
    "        #running periods\n",
    "        startPeriod, endPeriod, indexes = detect_running_period(data, minDuration, maxDuration, periodType)\n",
    "    \n",
    "    #Detect number of bins base on mean duration and binsize\n",
    "    allDuration = np.asarray(endPeriod) - np.asarray(startPeriod)\n",
    "    meanDuration = np.nanmean(allDuration)\n",
    "    nBins = np.ceil(meanDuration/float(binSize))\n",
    "    \n",
    "    xmax = nBins + (nSideBin+1) * 2\n",
    "    xaxis = np.arange(0.5, xmax, 1)\n",
    "    \n",
    "    #shuffling    \n",
    "    allMeanShuffledFiring=[]\n",
    "    for i in range(nShuffle):\n",
    "        spikeHist=[]    \n",
    "        for start, stop, duration in zip(startPeriod, endPeriod, allDuration):\n",
    "            _bin = duration/float(nBins)\n",
    "            rStart = start - (nSideBin+1) * _bin\n",
    "            rStop = stop + (nSideBin+1)*_bin - stop%_bin\n",
    "            timeBin = np.arange(rStart, rStop+_bin, _bin)\n",
    "            #shift by a random number between -duration and +duration\n",
    "            hist, bins = np.histogram(cluSpike + duration*(np.random.random_sample(1)*2 - 1), timeBin)\n",
    "            spikeHist.append(hist/float(_bin))  \n",
    "        meanFiring = np.nanmean(np.asarray(spikeHist), axis=0)\n",
    "        meanFiring = smooth(meanFiring, sigma)\n",
    "        allMeanShuffledFiring.append(meanFiring)\n",
    "        if not noPlot:\n",
    "            plt.plot(xaxis, meanFiring, color=\"darkred\", linestyle=\"--\")   \n",
    "        \n",
    "    #get the min, 5%, 95% and max of the shuffled mean firing rates\n",
    "    \n",
    "    \n",
    "    AllShufflesArray=np.asarray(allMeanShuffledFiring)\n",
    "    globalconfidenceband=np.percentile(AllShufflesArray,[0.5,99.5])\n",
    "    PointConfidenceBand=np.percentile(allMeanShuffledFiring,[2.5,97.5],axis=0)\n",
    "    plt.fill_between(xaxis,PointConfidenceBand[0],PointConfidenceBand[1],facecolor='gray', alpha=0.2)\n",
    "    plt.plot([xaxis[0],xaxis[-1]],[globalconfidenceband[0],globalconfidenceband[0]],'r',linestyle=\"--\")\n",
    "    plt.plot([xaxis[0],xaxis[-1]],[globalconfidenceband[1],globalconfidenceband[1]],'r',linestyle=\"--\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    percentile=np.percentile(allMeanShuffledFiring,[0.05,5,95,99.95])    \n",
    "    return percentile, allMeanShuffledFiring\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax2, meanFiring, nSideBin= plot_normalized_running_periods_firing_rate(data,shank,cluster,binSize=0.25,SideLength=3)[0:3]\n",
    "    percentileShuffle,allMeanShuffledFiring = plot_shuffled_running_firing_rate(data,shank,cluster,binSize=0.25,SideLength=3,ax=ax2)\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax2,mean,n = plot_normalized_immobility_periods_firing_rate(data, shank, cluster,binSize=0.25,SideLength=3)[0:3]\n",
    "    percentile, allMeanShuffledFiring = plot_shuffled_immobility_firing_rate(data, shank, cluster,binSize=0.25,SideLength=3,ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.6 General plot for one cluster\n",
    "#### (could be improved by adding ACG and waveforme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_plot(data, shank, cluster, group):\n",
    "    #check if cluster exists\n",
    "    if get_cluster_spikes(data, shank, cluster) is None:\n",
    "        return\n",
    "    plt.figure(figsize=(20,20))\n",
    "\n",
    "    # Left plot\n",
    "    ax=plt.subplot(131)\n",
    "    plot_break_cluster(data, shank, cluster, \"good\", lick=True)\n",
    "   \n",
    "    # Speed align on end/start trial\n",
    "    ax1 = plt.subplot(432)\n",
    "    ax1Bis = plot_mean_breaks_firing_rate(data, shank, cluster, ax=ax1, align=\"trial start\", trialType=\"good\",\n",
    "                                          minTime=-10, maxTime=20)\n",
    "    ax2 = plt.subplot(433, sharey=ax1)\n",
    "    ax2Bis = plot_mean_breaks_firing_rate(data, shank, cluster, ax=ax2, align=\"trial end\", trialType=\"good\",\n",
    "                                          minTime=-20)\n",
    "    #make the same limits for firing rate axis\n",
    "    maxY = max(ax1Bis.get_ylim()[1], ax2Bis.get_ylim()[1])\n",
    "    ax1Bis.set_ylim(0, maxY)\n",
    "    ax2Bis.set_ylim(0, maxY)\n",
    "    \n",
    "    #Lick break, if there is data\n",
    "    if data.lickBreakTime:\n",
    "        ax3 = plt.subplot(435)\n",
    "        ax3Bis = plot_mean_breaks_firing_rate(data, shank, cluster, align=\"trial start\", lick=True,\n",
    "                                              minTime=0, maxTime=50)\n",
    "        ax4 = plt.subplot(436, sharey=ax3)\n",
    "        ax4Bis = plot_mean_breaks_firing_rate(data, shank, cluster, ax=ax4, align=\"trial end\", lick=True,\n",
    "                                              minTime=-20)\n",
    "        #make the same limits for firing rate axis\n",
    "        maxY = max(ax3Bis.get_ylim()[1], ax4Bis.get_ylim()[1])\n",
    "        ax3Bis.set_ylim(0, maxY)\n",
    "        ax4Bis.set_ylim(0, maxY)\n",
    "    \n",
    "    # Normalized runs\n",
    "    ax3 = plt.subplot(438)\n",
    "    ax3Bis = plot_normalized_running_periods_firing_rate(data, shank, cluster, runType=\"all\")[0]\n",
    "    \n",
    "    ax4 = plt.subplot(439, sharey=ax3)\n",
    "    ax4Bis = plot_normalized_running_periods_firing_rate(data, shank, cluster, runType=\"intertrial\")[0]\n",
    "    \n",
    "    ax5 = plt.subplot(4, 3, 11, sharey=ax3)\n",
    "    ax5Bis = plot_normalized_running_periods_firing_rate(data, shank, cluster, runType=\"trial good run\")[0]\n",
    "    \n",
    "    ax6 = plt.subplot(4, 3, 12, sharey=ax3)\n",
    "    ax6Bis = plot_normalized_running_periods_firing_rate(data, shank, cluster, runType=\"trial bad run\")[0]\n",
    "    \n",
    "    #Normalierd runs: make the same limits for firing rate axis\n",
    "    maxY = 0\n",
    "    for axis in [ax3Bis, ax4Bis, ax5Bis, ax6Bis]:\n",
    "        maxY = max(maxY, axis.get_ylim()[1])\n",
    "    for axis in [ax3Bis, ax4Bis, ax5Bis, ax6Bis]:\n",
    "        axis.set_ylim(0, maxY)\n",
    "    \n",
    "    # Title\n",
    "    title = data.experiment+\" (day %s), shank %s cluster %s, group %s\" %(data.daySinceStart, shank, cluster, group)\n",
    "    if not data.hasEEG:\n",
    "        title+=\" - read from .beambreaktime\"\n",
    "    plt.suptitle(title, fontsize=20)\n",
    "    plt.subplots_adjust(wspace=0.3, hspace=0.3, top=0.94)\n",
    "    \n",
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():    \n",
    "    if data.hasBehavior and data.hasSpike:\n",
    "        plt.figure(figsize=(15,20))\n",
    "        cluster_plot(data, shank, cluster, group=\"Good\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Batch of  3.5 (Real and shuffled firing rates during behavioral epochs with standardized lenght) to save data for all clusters of this session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def meanFiringRatevsRunandImmoNorm(data, groupList=[\"Good\"], saveAsPickle=True, redo=False, binSize=0.25, \n",
    "                                   SideLength=3, nShuffle=500,showplot=False):\n",
    "    \"\"\"\n",
    "    Computes the mean firing rate and its confidence interval during running and immobility\n",
    "    Loads a pickle file if the analysis was already done\n",
    "    Input\n",
    "      -groupList: list of cluster groups (good, noise, mua..). if None, takes all clusters.\n",
    "      -saveAsPickle: whether to save results in pickle file\n",
    "      -redo: whether to redo analysis even if there is already a pickle file\n",
    "      -nShuffle: parameter for confidence interval\n",
    "    \"\"\"\n",
    "    #load and return the pickle if it exists (and redo=False)\n",
    "    picklePath=os.path.join(data.sessionPath,\"Analysis\",\"AnalyzedSpikeData.p\")\n",
    "    if (not redo) and os.path.exists(picklePath):\n",
    "        with open(picklePath, 'rb') as f:\n",
    "            print(\"loaded pickle %s\"%picklePath)\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    #check that groupList is a list\n",
    "    if not isinstance(groupList,list):\n",
    "        groupList=[groupList]\n",
    "        \n",
    "    #Get mean firing rates for each cluster\n",
    "    analyzedSpikeData={\n",
    "        \"meanFiringRateDuringRunNorm\":{},\n",
    "        \"meanFiringRateDuringImmoNorm\":{},\n",
    "        \"confidenceLimitsRunning\":{},\n",
    "        \"confidenceLimitsImmobility\":{},\n",
    "        }  \n",
    "    nSideBin = None\n",
    "    for shank in sorted(data.clusterGroup):\n",
    "        print(\"Shank %s\"%shank)        \n",
    "        for key in analyzedSpikeData:\n",
    "            analyzedSpikeData[key][shank]={}\n",
    "        for group in data.clusterGroup[shank]:\n",
    "            if (groupList is not None) and (group not in groupList):\n",
    "                continue\n",
    "            for cluster in sorted(data.clusterGroup[shank][group]):\n",
    "                six.print_(cluster,end=\" \")        \n",
    "                \n",
    "                plt.figure(figsize=(cm2inch(30),cm2inch(10)))\n",
    "                plt.subplot(1,2,1)\n",
    "                ax, meanFiring, nSideBin = plot_normalized_running_periods_firing_rate(data, shank, cluster, \n",
    "                                                           binSize=binSize, SideLength=SideLength)[0:3]\n",
    "                percentileShuffle = plot_shuffled_running_firing_rate(data, shank, cluster, binSize=binSize,\n",
    "                                                                      SideLength=SideLength, ax=ax, nShuffle=nShuffle)\n",
    "                analyzedSpikeData[\"meanFiringRateDuringRunNorm\"][shank][cluster] = meanFiring\n",
    "                analyzedSpikeData[\"confidenceLimitsRunning\"][shank][cluster] = percentileShuffle[0]\n",
    "                \n",
    "                \n",
    "                plt.subplot(1,2,2)\n",
    "                ax,meanFiringI,nSideBinI=plot_normalized_immobility_periods_firing_rate(data, shank, cluster, \n",
    "                                                           binSize=binSize, SideLength=SideLength)[0:3]\n",
    "                percentileShuffleI = plot_shuffled_immobility_firing_rate(data, shank, cluster, binSize=binSize, \n",
    "                                                                          SideLength=SideLength, ax=ax, \n",
    "                                                                          nShuffle=nShuffle)\n",
    "                analyzedSpikeData[\"meanFiringRateDuringImmoNorm\"][shank][cluster]=meanFiringI\n",
    "                analyzedSpikeData[\"confidenceLimitsImmobility\"][shank][cluster]=percentileShuffleI[0]\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                \n",
    "                if not showplot:\n",
    "                    plt.close()\n",
    "                \n",
    "        print(\"\")        \n",
    "    analyzedSpikeData[\"nSideBinForNormPlots\"]=nSideBin     \n",
    "    analyzedSpikeData[\"binSizeForNormPlots\"]=binSize\n",
    "    \n",
    "    #save as a pickle file\n",
    "    if saveAsPickle:\n",
    "        with open(picklePath, 'wb') as f:\n",
    "            pickle.dump(analyzedSpikeData, f)\n",
    "        print(\"Saved pickle: %s\"%picklePath)\n",
    "    \n",
    "    return   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():    \n",
    "    meanFiringRatevsRunandImmoNorm(data, saveAsPickle=False, redo=False, binSize=0.25, SideLength=3,showplot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.8 Autocorrelograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import phy.stats\n",
    "\n",
    "def plot_autocorrelogram(data, shank, cluster, bin_ms=1, half_width_ms=25, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    \n",
    "    bin_ms=np.clip(bin_ms, .1,1e3) #bin size in ms, rounded\n",
    "    binsize=int(data.spikeSamplingRate * bin_ms * 0.001) #bin size in time samples\n",
    "    \n",
    "    half_width_ms = np.clip(half_width_ms, .1,1e3) #ms, rounded\n",
    "    winsize_bins = 2 * int(half_width_ms/bin_ms) + 1 #number of bins in window\n",
    "\n",
    "    sample = data.spikeSample[shank][cluster]\n",
    "    clu = np.ones_like(sample, dtype=\"int64\")\n",
    "    \n",
    "    pairwiseCorr = phy.stats.pairwise_correlograms(sample, clu, binsize, winsize_bins)\n",
    "\n",
    "    autoCorr=pairwiseCorr[0,0,:]\n",
    "    \n",
    "    halfWinsize = winsize_bins // 2\n",
    "    xaxis = np.arange(-halfWinsize-0.5, halfWinsize+1.5)\n",
    "    xaxis = xaxis * binsize / data.spikeSamplingRate * 1000 #ms\n",
    "    \n",
    "    ax.bar(xaxis[:-1], autoCorr, width=bin_ms, color=\"darkred\", edgecolor=\"darkred\");\n",
    "    ax.set_title(\"Cluster %s, Autocorrelogram\"%cluster);\n",
    "    ax.set_xlim([xaxis[0], xaxis[-1]]);\n",
    "    ax.set_xlabel(\"time (ms), binsize=%s ms\"%bin_ms)\n",
    "    ax.set_ylabel(\"spike count\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    #print(data.clusterGroup)\n",
    "    \n",
    "\n",
    "    \n",
    "###########################    \n",
    "\n",
    "\n",
    "###########################    \n",
    "    #bin_ms: bin size in ms\n",
    "    bin_ms=1\n",
    "    #half_width_ms: half width of the x axis (time), in ms\n",
    "    half_width_ms=30 #1000\n",
    "\n",
    "    plt.figure(figsize=(15,5))    \n",
    "    plt.subplot(121)\n",
    "    plot_autocorrelogram(data,shank,cluster,bin_ms,30)\n",
    "    plt.subplot(122)\n",
    "    plot_autocorrelogram(data,shank,cluster,bin_ms,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Autocorrelogram by behavioral state  (running / lick/ true immobility [no run])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_autocorrelogram_period(data, shank, cluster, bin_ms=1, half_width_ms=25, immobility=False, lick=False, ax=None):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "        \n",
    "    if immobility:\n",
    "            #immobility periods\n",
    "        startPeriod, endPeriod, indexes = detect_immobility_period(data, minDurationSecond=0, maxDurationSecond=None,\n",
    "                                                                       immobilityType=\"all\", runMinDuration=0.1,lick=True, allActivity=True)\n",
    "\n",
    "        runOrImmobile = \"immobility\"\n",
    "    \n",
    "    \n",
    "    elif lick:\n",
    "        #immobility periods\n",
    "        startPeriod, endPeriod, indexes = detect_licking_period(data, minDurationSecond=2, lickType=\"all\")\n",
    "        \n",
    "        runOrImmobile = \"lick\"\n",
    "    else:\n",
    "        #running periods\n",
    "        startPeriod, endPeriod, indexes = detect_running_period(data, minDurationSecond=0, maxDurationSecond=None, \n",
    "                                                                runType=\"all\")\n",
    "        runOrImmobile = \"running\"\n",
    "    \n",
    "    bin_ms = np.clip(bin_ms, .1,1e3) #bin size in ms, rounded\n",
    "    binsize = int(data.spikeSamplingRate * bin_ms * 0.001) #bin size in time samples\n",
    "    \n",
    "    half_width_ms = np.clip(half_width_ms, .1,1e3) #ms, rounded\n",
    "    winsize_bins = 2 * int(half_width_ms/bin_ms) + 1 #number of bins in window\n",
    "\n",
    "    sample = data.spikeSample[shank][cluster]\n",
    "    spikeTime = data.spikeTime[shank][cluster]\n",
    "    \n",
    "    isInPeriod = np.full_like(sample,False)\n",
    "    #select only spike during trials\n",
    "    for s, e in zip(startPeriod, endPeriod):\n",
    "        isInPeriod = np.logical_or(isInPeriod, (spikeTime > s) & (spikeTime < e))\n",
    "    \n",
    "    newSample = sample[isInPeriod]\n",
    "    clu = np.ones_like(newSample, dtype=\"int64\")\n",
    "\n",
    "    pairwiseCorr = phy.stats.pairwise_correlograms(newSample, clu, binsize, winsize_bins)\n",
    "\n",
    "    autoCorr = pairwiseCorr[0, 0]\n",
    "    \n",
    "    halfWinsize = winsize_bins // 2\n",
    "    xaxis = np.arange(-halfWinsize-0.5, halfWinsize+1.5)\n",
    "    xaxis = xaxis * binsize / data.spikeSamplingRate * 1000 #ms\n",
    "    \n",
    "    ax.bar(xaxis[:-1], autoCorr, width=bin_ms, color=\"darkred\", edgecolor=\"darkred\");\n",
    "    ax.set_title(\"Cluster %s, Autocorrelogram during %s\"%(cluster, runOrImmobile));\n",
    "    ax.set_xlim([xaxis[0], xaxis[-1]]);\n",
    "    ax.set_xlabel(\"time (ms), binsize=%s ms\"%bin_ms)\n",
    "    ax.set_ylabel(\"spike count\")\n",
    "\n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    ###########\n",
    "\n",
    "    \n",
    "    #########\n",
    "    \n",
    "    bin_ms = 1\n",
    "    plt.figure(figsize=(15,10))    \n",
    "    plt.subplot(321)\n",
    "    plot_autocorrelogram_period(data, shank, cluster, bin_ms, 30)\n",
    "    plt.subplot(322)\n",
    "    plot_autocorrelogram_period(data, shank, cluster, 20, 1000)\n",
    "    plt.subplot(323)\n",
    "    plot_autocorrelogram_period(data, shank, cluster, bin_ms, 30, immobility=True)\n",
    "    plt.subplot(324)\n",
    "    plot_autocorrelogram_period(data, shank, cluster, 20, 1000, immobility=True)\n",
    "    plt.subplot(325)\n",
    "    plot_autocorrelogram_period(data, shank, cluster, bin_ms, 30, lick=True)\n",
    "    plt.subplot(326)\n",
    "    plot_autocorrelogram_period(data, shank, cluster, 20, 1000, lick=True)\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Cross correlograms spike and behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_crosscorrelogram_behaviour(data, shank, cluster, behaviorType=\"run\", bin_ms=1,half_width_ms=30):\n",
    "    bin_ms=np.clip(bin_ms,.1,1e3) #bin size in ms, rounded\n",
    "    binsize=int(data.spikeSamplingRate*bin_ms*0.001) #bin size in time samples\n",
    "    half_width_ms=np.clip(half_width_ms,.1,10e3) #ms, rounded\n",
    "    winsize_bins= 2*int(half_width_ms/bin_ms) +1 #number of bins in window\n",
    "    halfWinsize=winsize_bins//2\n",
    "    xaxis=np.arange(-halfWinsize-0.5, halfWinsize+1.5)\n",
    "    xaxis=xaxis*binsize/data.spikeSamplingRate*1000 #ms\n",
    "    \n",
    "    #spikes for the cluster\n",
    "    sample = np.array(data.spikeSample[shank][cluster], dtype=\"uint64\")\n",
    "    clu = np.full_like(sample, 1, dtype=\"int64\")\n",
    "    \n",
    "    #behavior photobeam breaks time\n",
    "    print(\"behaviorType %s\" %behaviorType)\n",
    "    if behaviorType is \"run\":\n",
    "        allbeambreak=data.allBeamBreak\n",
    "    else:\n",
    "        allbeambreak=data.allLickBreak\n",
    "    \n",
    "    \n",
    "    sampleBreak=[round(X*data.spikeSamplingRate) for X in allbeambreak]\n",
    "    sampleBreak = np.array(sampleBreak, dtype=\"uint64\") \n",
    "    cluBreak=np.full_like(sampleBreak, 2, dtype=\"int64\")\n",
    "    \n",
    "    #the phy.stats.pairwise_correlograms takes for input two lists:\n",
    "    #  the spikes samples (old .res file)\n",
    "    #  the corresponding cluster numbers (old .clu files)\n",
    "    #the two lists need to be sorted by increasing spikes samples, or the crosscorrelogram won't work\n",
    "    twoSample = np.append(sample, sampleBreak)\n",
    "    twoClu = np.append(clu, cluBreak)\n",
    "    sortingOrder = twoSample.argsort()\n",
    "    print(binsize, winsize_bins)\n",
    "    pairwiseCorr=phy.stats.pairwise_correlograms(twoSample[sortingOrder], twoClu[sortingOrder], binsize, winsize_bins)\n",
    "\n",
    "    k = 1\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            if i + j == 0:\n",
    "                col = \"darkred\"\n",
    "            elif i + j == 2:\n",
    "                col = \"black\"\n",
    "            else:\n",
    "                col = \"blue\"\n",
    "            ax = plt.subplot(2, 2, k)\n",
    "            autoCorr = pairwiseCorr[i, j]\n",
    "            ax.bar(xaxis[:-1], autoCorr, width=bin_ms, color=col, edgecolor=col);\n",
    "            k += 1\n",
    "    return sample, sampleBreak\n",
    "            \n",
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    #bin_ms: bin size in ms\n",
    "    bin_ms=50\n",
    "    ############################\n",
    "\n",
    "    ############################\n",
    "\n",
    "    plt.figure(figsize=(15,5))    \n",
    "    clu, cluBreak = plot_crosscorrelogram_behaviour(data, shank, cluster, behaviorType=\"lick\", bin_ms=50, half_width_ms=8000)\n",
    "    #print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.11 Waveform for a given unit (get mean waveform and some basic shape caractersitics)\n",
    "  - Load .kwx with h5py / or extract from .dat\n",
    "  - Choose randomly 150 spikes from a cluster\n",
    "  \n",
    "  in kwik/kwx format, spikeIndex are the index in the kwx file\n",
    "  \n",
    "  spikeSample are the index in dat file (once it's reshaped as `nSample*nChannel`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "def read_dat_waveform(data, shank, cluster, subSample=200, extract=50):\n",
    "    \"\"\" extract the waveform of one cluster from a dat file, return array [nSpikes, extract*2, nChannelsInShank]\n",
    "    \n",
    "    subSample: maximum number of spikes returned (nSpikes <= subSample) \n",
    "      If the cluster has more spikes, *subSample* spikes are chosen randomly\n",
    "    extract: number of points to extract before and after the spike time\n",
    "    \"\"\"\n",
    "    path = data.fullPath + '.dat'\n",
    "    if not os.path.exists(path):\n",
    "        print(\"No .dat file\")\n",
    "        return False\n",
    "    #memory map to dat file\n",
    "    dtype = np.int16\n",
    "    size = os.stat(path).st_size\n",
    "    row_size = data.nChannels * np.dtype(dtype).itemsize\n",
    "    if size % row_size != 0:\n",
    "        raise ValueError((\"Shape error: the file {f} has S={s} bytes, \"\n",
    "                          \"but there are C={c} channels. C should be a divisor of S.\"\n",
    "                          \"\").format(f=filename, s=size, c=data.nChannels))\n",
    "    nsamples = size // row_size\n",
    "    shape = (nsamples, data.nChannels)\n",
    "    datFile = np.memmap(data.fullPath + '.dat', dtype = dtype, mode = 'r', offset = 0, shape = shape)\n",
    "    #indexes of spikes for this cluster\n",
    "    spikeID = data.spikeSample[shank][cluster]\n",
    "    nSpike = len(spikeID)\n",
    "    if nSpike > subSample:\n",
    "        spikeID = [int(s) for s in np.random.choice(spikeID, subSample, replace = False)]\n",
    "        nSpike = subSample\n",
    "    #get waveform for each index\n",
    "    waveform = np.zeros(shape=(nSpike, extract * 2, len(data.channelGroupList[shank])), dtype=dtype )\n",
    "    for index, spike in enumerate(spikeID):\n",
    "        waveform[index, :, :] = datFile[spike-extract : spike+extract, data.channelGroupList[shank]]\n",
    "    return waveform\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def read_kwx_waveform(data, shank, cluster, subSample=200, filtered=False):\n",
    "    \"\"\" read the waveform for one cluster in the .kwx file, return array [nSpikes, nDataPoints, nChannelsInShank]\n",
    "    \n",
    "    In the kwx we read an array [ spikes indexes, n data points, n channels]\n",
    "    \n",
    "    subSample: maximum number of spikes returned (nSpikes <= subSample) \n",
    "      If the cluster has more spikes, *subSample* spikes are chosen randomly\n",
    "    filtered: whether to extract the filtered waveforms or the raw ones\n",
    "    \"\"\"\n",
    "    if not os.path.exists(data.fullPath+\".kwx\"):\n",
    "        print(\"No .kwx file\")\n",
    "        return False\n",
    "    with h5py.File(data.fullPath+\".kwx\",\"r\") as kwx:  \n",
    "        if filtered:\n",
    "            waveform = kwx.get('channel_groups/%s/waveforms_filtered' % shank)[()]\n",
    "        else:\n",
    "            waveform = kwx.get('channel_groups/%s/waveforms_raw' % shank)[()]\n",
    "    #index of spikes where cluster==X\n",
    "    spikeID=data.spikeIndex[shank][cluster]\n",
    "    if len(spikeID) > subSample:\n",
    "        spikeID = np.random.choice(spikeID, subSample, replace = False)\n",
    "    return waveform[spikeID, :, :]\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def get_mean_waveform_and_caracteristics(data, shank, cluster, sample=200, kwx=True, filtered=False,redo=True):\n",
    "    \n",
    "    #print(\"redo: %s\" %redo )\n",
    "    picklePath = os.path.join(data.analysisPath, \"waveforms.p\")\n",
    "    #load spikes or previously saved file\n",
    "    if (not redo) and (os.path.exists(picklePath)):\n",
    "        print(\"load waveforms from %s\"%(picklePath))\n",
    "        with open(picklePath, 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "            res=res[shank][cluster]\n",
    "            caracteristics=res[0] \n",
    "            peakIndex=res[1]\n",
    "            valleyIndex=res[2]\n",
    "            baseIndex=res[3]\n",
    "            widthEnd=res[4]\n",
    "            widthStart=res[5]\n",
    "        \n",
    "        return caracteristics, peakIndex, valleyIndex, baseIndex, widthEnd, widthStart\n",
    "    \n",
    "    #original from typhaine (but buggy, at least on MOU074_2015_07_17_12_54)\n",
    "    elif os.path.exists(data.fullPath + '.kwx') and kwx:\n",
    "        waveform = read_kwx_waveform(data, shank, cluster, sample, filtered)\n",
    "    if os.path.exists(data.fullPath + '.dat'):\n",
    "        waveform = read_dat_waveform(data, shank, cluster, sample)\n",
    "    else:\n",
    "        print('no spike')\n",
    "        return {}\n",
    "        \n",
    "    #compute the mean waveform in each channel\n",
    "    #find the best channel (=the one where we see the best waveform =the bigger y range)\n",
    "    diffMinMax = 0\n",
    "    meanWaveforms = []\n",
    "    bestChannel = 0\n",
    "    for channel in range(waveform.shape[2]):\n",
    "        meanChannel = np.mean(waveform[:, :, channel], axis = 0)\n",
    "        minMaxChannel = np.max(meanChannel) - np.min(meanChannel)\n",
    "        meanWaveforms.append(meanChannel)\n",
    "        if minMaxChannel > diffMinMax:\n",
    "            diffMinMax = minMaxChannel\n",
    "            bestChannel = channel\n",
    "            \n",
    "    #compute caracteristics of waveform, on the best channel\n",
    "    wave = meanWaveforms[bestChannel]\n",
    "    #l = len(meanWaveform)\n",
    "    #interpolation to get more precise results (for the half width)\n",
    "    #step = 1\n",
    "    #wave = np.interp(np.arange(0, l, step), range(l), meanWaveform)    \n",
    "    wave=np.interp(np.arange(0,len(wave),0.02),np.arange(0,len(wave)),wave)\n",
    "    \n",
    "    \n",
    "    #peak: minimum, in the middle of the waveform\n",
    "    middleIndex = int(len(wave) / 2)\n",
    "    middleWave = wave[middleIndex-3 : middleIndex+3]\n",
    "    peakValue = middleWave.min()\n",
    "    peakIndex = middleIndex-3 + middleWave.argmin()\n",
    "    #print(peakValue)\n",
    "    \n",
    "    if peakValue>0:\n",
    "        wave=-wave\n",
    "        peakValue=-peakValue\n",
    "\n",
    "    ############    \n",
    "    #return wave\n",
    "\n",
    "\n",
    "    #valley: maximum, after the peak\n",
    "    #valleyIndex = peakIndex + wave[peakIndex : -1000].argmax()   [Not use as first option because does not find the first max]\n",
    "\n",
    "    try:\n",
    "        valleyIndex = peakIndex +argrelextrema(wave[peakIndex : -1000], np.greater)[0][0]\n",
    "    except IndexError:\n",
    "        valleyIndex = peakIndex + wave[peakIndex : -1000].argmax()\n",
    "    \n",
    "    valleyValue = wave[valleyIndex]\n",
    "    #print(\"valley value %s\" %valleyValue)\n",
    "\n",
    "    #baseline: maximum, before the peak\n",
    "    baseIndex = wave[1 : peakIndex].argmax() \n",
    "    baseValue = wave[1 + baseIndex]\n",
    "    \n",
    "    # different way of getting baseline value, made by david : better :)\n",
    "    baseValue=np.nanmean(wave[0:1000])\n",
    "    #print(\"baseline %s\" %baseValue)\n",
    "    \n",
    "    #amplitude: baseline - peak (values)\n",
    "    amp = abs(wave[peakIndex]-baseValue)\n",
    "    \n",
    "    #middle width\n",
    "    middleY = baseValue - amp/2.0 # Y value at half height\n",
    "    widthStart = np.abs(wave[:peakIndex] - middleY).argmin() #left index of the half-width\n",
    "    widthEnd = peakIndex + np.abs(wave[peakIndex:] - middleY).argmin() #right index of the half-width\n",
    "    # voltage factor to return amplitudes in mV (x1000 at the end)\n",
    "    VoltageFactor=data.voltageRange/(2**data.nBits)/data.amplification*1000\n",
    "    \n",
    "    #print(widthStart,widthEnd)\n",
    "    \n",
    "    peaktovalleytime=(valleyIndex - peakIndex)/(data.spikeSamplingRate*50)*1000\n",
    "    #print(\"peaktovalleytime : %s ms\" %peaktovalleytime)\n",
    "    halfwidth=(widthEnd - widthStart)/(data.spikeSamplingRate*50)*1000\n",
    "    #print(\"halfwidth : %s ms\" %halfwidth)\n",
    "    \n",
    "    \n",
    "    #spike assymetrie. Correlation between the 1000 point after peak vs 1000 points before peak\n",
    "    RMat=np.corrcoef(wave[1501:2501],wave[3500:2500:-1])\n",
    "    asymetrie=RMat[0,1]\n",
    "    #save the data\n",
    "    \n",
    "    caracteristics = {\n",
    "        \"mean waveforms\": meanWaveforms,\n",
    "        \"baseline-peak amplitude\": amp,\n",
    "        \"peak to valley amplitude\":  abs(peakValue-valleyValue),\n",
    "        \"HalfWidth\": halfwidth,\n",
    "        \"best channel\": bestChannel,\n",
    "        \"PeakToValley\": peaktovalleytime,\n",
    "        \"asymetrie\": asymetrie\n",
    "    }\n",
    "    \n",
    "    return caracteristics, peakIndex, valleyIndex, baseIndex, widthEnd, widthStart\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-------------------------------------------------------------------------------------------------------\n",
    "def plot_mean_waveform(data, shank, cluster, group=\"undefined\", sample=200, kwx=True, filtered=False,redo=True,plotAllSpikes=False,ax1=None,ax2=None):\n",
    "    \"\"\" plot the mean waveform and its caracteristics for one cluster \"\"\"\n",
    "    \n",
    "    \n",
    "    if ax1 is None:\n",
    "        plt.figure(figsize = (10, 5))\n",
    "        ax1=plt.subplot(1,2,1)\n",
    "        \n",
    "    \n",
    "    caracteristics, p, v, b, e, s = get_mean_waveform_and_caracteristics(data, shank, cluster, sample, kwx, filtered,redo)\n",
    "    best = caracteristics.pop(\"best channel\")\n",
    "    meanWaveforms = caracteristics.pop(\"mean waveforms\")\n",
    "    wave = meanWaveforms[best]\n",
    "    wave=np.interp(np.arange(0,len(wave),0.02),np.arange(0,len(wave)),wave)\n",
    "    \n",
    "    if wave[p]>0:\n",
    "        wave=-wave\n",
    "    \n",
    "    timeAxis=np.arange(0,len(wave)/(data.spikeSamplingRate/0.02),1/(data.spikeSamplingRate/0.02))\n",
    "    timeAxis=timeAxis*1000-timeAxis[-1]*1000/2\n",
    "    \n",
    "    \n",
    "       \n",
    "    ax1.plot(timeAxis,wave)\n",
    "    title = \"Mean waveform - Shank %s Cluster %s channel %s\" %(shank, cluster, best)\n",
    "    title += \"\\n\" + \" - \".join([\"%.2f\"%(caracteristics[v]) for v in sorted(caracteristics)])\n",
    "    ax1.set_title(title)\n",
    "    \n",
    "\n",
    "    ax1.axvspan(timeAxis[s], timeAxis[e], color=\"lightgrey\", alpha=0.5)\n",
    "    \n",
    "    baseline = caracteristics[\"baseline-peak amplitude\"] + wave[p]\n",
    "    ax1.plot((timeAxis[0], timeAxis[p-1]), (baseline, baseline))\n",
    "    \n",
    "    ax1.plot((timeAxis[p], timeAxis[len(wave)-1]), (wave[v], wave[v]))\n",
    "    #plt.xlim(-1, len(wave) +1)\n",
    "    \n",
    "    #plot half width\n",
    "    ax1.plot((timeAxis[e],timeAxis[s]),(wave[e],wave[s]),'x-k')\n",
    "    ax1.plot((timeAxis[p],timeAxis[v]),(wave[p],wave[p]),'x-k')\n",
    "    ax1.set_xlim(-2,2)\n",
    "    ax1.set_ylim(wave.min()-10, wave.max()+10)\n",
    "    \n",
    "    \n",
    "    ###plot half width and peak to valley of the cluster versus all\n",
    "    AllUnitspicklePath=os.path.join(root,\"ALLMOU_Analysis\",\"AllUnitsHalfWidthPeakToValley.p\")\n",
    "    if (os.path.isfile(AllUnitspicklePath)) and (plotAllSpikes):\n",
    "#         plt.figure(figsize = (5, 5))\n",
    "        if ax2 is None:\n",
    "            ax2=plt.subplot(1,2,2)\n",
    "        with open(AllUnitspicklePath, 'rb') as f:\n",
    "            AllUnitsCharacteristics = pickle.load(f)\n",
    "            \n",
    "        AllUnitsCharacteristics\n",
    "        AllUnitsCharacteristicsData=AllUnitsCharacteristics[\"Characteristics\"]\n",
    "        AllUnitsCharacteristicsData\n",
    "        ax2.scatter(AllUnitsCharacteristicsData[\"HalfWidth\"],AllUnitsCharacteristicsData[\"PeakToValley\"])\n",
    "        ax2.scatter(caracteristics[\"HalfWidth\"],caracteristics[\"PeakToValley\"],marker='x',s=200)\n",
    "        ax2.set_xlim(0,0.5)\n",
    "        \n",
    "    \n",
    "    return wave, caracteristics\n",
    "        \n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "def plot_waveforms(data, shank, cluster, group=\"not specified\", sample=200, kwx=True, place=None,\n",
    "                   filtered=False):\n",
    "    \"\"\" plot the waveforms on every channel for one cluster \"\"\"\n",
    "    \n",
    "    if os.path.exists(data.fullPath + '.kwx') and kwx:\n",
    "        waveform = read_kwx_waveform(data, shank, cluster, sample, filtered)\n",
    "    elif os.path.exists(data.fullPath + '.dat'):\n",
    "        waveform = read_dat_waveform(data, shank, cluster, sample)\n",
    "    else:\n",
    "        print('no spikes')  \n",
    "        return\n",
    "    if place is None:\n",
    "        place = [[0, 1], [1, 0], [2, 1], [3, 0], [4, 1], [5, 0], [6, 1], [7, 0]]           \n",
    "    plt.figure(figsize = (3*2, 6))\n",
    "    plt.suptitle(\"Cluster %s\" % cluster, fontsize = 14)\n",
    "    gs = gridspec.GridSpec(8, 3, hspace = -0.3, wspace = 0)    \n",
    "    for channel in range(waveform.shape[2]):\n",
    "        channelWaveform = waveform[:, :, channel] \n",
    "        x = place[channel][0]\n",
    "        y = place[channel][1]\n",
    "        ax = plt.subplot(gs[x,y])\n",
    "        for spike in channelWaveform:\n",
    "            ax.plot(spike, color = \"darkred\");\n",
    "        ax.set_title(\"%s\" % channel)\n",
    "        ax.set_axis_off() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "\n",
    "# below you can change shank and clu\n",
    "##############################\n",
    "\n",
    "\n",
    "##############################\n",
    "    #plot_waveforms(data, shank, cluster, kwx=False)\n",
    "    plt.figure()\n",
    "    wave,k= plot_mean_waveform(data, shank, cluster,redo=True,plotAllSpikes=True,kwx=False)\n",
    "    #ax1.set_title(\"\")\n",
    "    #print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.12 Mean waveform and caracteristic for all the units of this session (And SAVE!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_all_mean_waveforms(data, groupList=[\"Good\"], sample=200, redo=True, saveAsPickle=True):\n",
    "    picklePath = os.path.join(data.analysisPath, \"waveforms.p\")\n",
    "    #load spikes or previously saved file\n",
    "    if (not redo) and (os.path.exists(picklePath)):\n",
    "        print(\"load waveforms from %s\"%(picklePath))\n",
    "        with open(picklePath, 'rb') as f:\n",
    "            res = pickle.load(f)\n",
    "        return res\n",
    "        \n",
    "    #dictionary to save results\n",
    "    res = {}\n",
    "    #loop on all cluster for this session\n",
    "    for shank in data.clusterGroup:\n",
    "        print(\"Shank\", shank)\n",
    "        res[shank] = {}\n",
    "        for group in data.clusterGroup[shank]:\n",
    "            if group not in groupList:\n",
    "                continue\n",
    "            for clu in data.clusterGroup[shank][group]:\n",
    "                print(clu, end=\" \")\n",
    "                res[shank][clu] = get_mean_waveform_and_caracteristics(data, shank, clu, sample=sample)\n",
    "                #res[shank][clu] = get_mean_waveform_and_caracteristics(data, shank, clu, sample=sample)[0]\n",
    "        print()\n",
    "        \n",
    "    #save as a pickle file\n",
    "    if saveAsPickle:\n",
    "        with open(picklePath, 'wb') as f:\n",
    "            pickle.dump(res, f)\n",
    "        print(\"Saved pickle: %s\"%picklePath)\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.13 Waveforms caracteristics and Firing rate characteristics for all units of this sessions (and save)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getUnitsMainCharacteristics(data, groupList=[\"Good\"], sample=200, redofiringrate=False, redowaveform=True,saveAsPickle=True):\n",
    "    FiringCharacteristics=[\"meanFRate\",\"PropISI\",\"runFRate\",\"immoFRate\"]\n",
    "    WaveformCharacteristics=[\"HalfWidth\",\"PeakToValley\",\"asymetrie\"]\n",
    "    print(\"redofiring: %s\" %(redofiringrate))\n",
    "    print(\"redowaveform: %s\" %(redowaveform))\n",
    "    picklePath = os.path.join(data.analysisPath, \"UnitsWaveFormAndFiringRateCharacteristics.p\")\n",
    "    \n",
    "    if (not redofiringrate) and (not redowaveform) and (os.path.exists(picklePath)):\n",
    "        print(\"load waveform and firing rate characteristics from %s\"%(picklePath))\n",
    "        with open(picklePath, 'rb') as f:\n",
    "            allCharacteristics = pickle.load(f)\n",
    "        return allCharacteristics\n",
    "    \n",
    "    \n",
    "    \n",
    "    if not data.hasSpike:\n",
    "        print(\"no spike\")\n",
    "        return\n",
    "    \n",
    "    ## get firing rate caractersitics (mean Firing rate, proportion of long ISI, mean Firing rate during run, mean Firing rate during Immo)\n",
    "    \n",
    "    \n",
    "    allFrate=[]\n",
    "    allpropISI=[]\n",
    "    AllFrateRun=[]\n",
    "    AllFrateImmo=[]\n",
    "    allCharacteristics={}\n",
    "    \n",
    "    resFiring = meanFiringRatevsRunandImmoNorm(data, groupList=groupList, redo=redofiringrate, saveAsPickle=saveAsPickle)\n",
    "    \n",
    "    for FiringCharacteristic in FiringCharacteristics:\n",
    "        if (FiringCharacteristic==\"meanFRate\") or (FiringCharacteristic==\"PropISI\"):\n",
    "            allCharacteristics[FiringCharacteristic]={}\n",
    "            allrateData=[]\n",
    "            for shankID in data.channelGroupList:\n",
    "                for clusterID in data.clusterGroup[shankID]['Good']:\n",
    "                    SpikeTimes=data.spikeTime[shankID][clusterID]\n",
    "                    ISI=[j-i for i, j in zip(SpikeTimes[:-1], SpikeTimes[1:])] \n",
    "                    if FiringCharacteristic==\"meanFRate\":\n",
    "                        Frate=1/np.nanmean(ISI)\n",
    "                        allrateData.append(Frate)\n",
    "                    if FiringCharacteristic==\"PropISI\":\n",
    "                        PropISI=sum([x for x in ISI if x>5])/sum(ISI)\n",
    "                        allrateData.append(PropISI)\n",
    "\n",
    "            allCharacteristics[FiringCharacteristic]=allrateData\n",
    "\n",
    "        \n",
    "        if (FiringCharacteristic==\"immoFRate\") or (FiringCharacteristic==\"runFRate\"):  \n",
    "            print('Run or Immo Firing rate is beeing computed or loaded')\n",
    "            \n",
    "            nSideBin = resFiring[\"nSideBinForNormPlots\"]\n",
    "\n",
    "            if FiringCharacteristic==\"immoFRate\":\n",
    "                meanFiring = resFiring[\"meanFiringRateDuringImmoNorm\"]\n",
    "                for shank in meanFiring:\n",
    "                    for clu in meanFiring[shank]:\n",
    "                        FrateImmo = np.mean(meanFiring[shank][clu][nSideBin+1 : -nSideBin])\n",
    "                        AllFrateImmo.append(FrateImmo)\n",
    "                \n",
    "                allCharacteristics[FiringCharacteristic]=AllFrateImmo\n",
    "            if FiringCharacteristic==\"runFRate\":\n",
    "                meanFiring = resFiring[\"meanFiringRateDuringRunNorm\"]\n",
    "                \n",
    "                for shank in meanFiring:\n",
    "                    for clu in meanFiring[shank]:\n",
    "                        FrateRun = np.mean(meanFiring[shank][clu][nSideBin+1 : -nSideBin])\n",
    "                        AllFrateRun.append(FrateRun)\n",
    "                        \n",
    "                allCharacteristics[FiringCharacteristic]=AllFrateRun\n",
    "\n",
    "        \n",
    "    ## get Waveform charecteristics\n",
    "    resWave = get_all_mean_waveforms(data, groupList=groupList, sample=sample, redo=redowaveform, saveAsPickle=saveAsPickle)\n",
    "    for caracteristic in WaveformCharacteristics:\n",
    "        allCharacteristics[caracteristic]={}\n",
    "        allsw=[]\n",
    "        for shankID in data.channelGroupList:\n",
    "            for cluID in data.clusterGroup[shankID]['Good']:\n",
    "                    sw = resWave[shankID][cluID][0][caracteristic]\n",
    "                    allsw.append(sw)\n",
    "                    allCharacteristics[caracteristic]=allsw\n",
    "                    \n",
    "\n",
    "    if saveAsPickle:\n",
    "        with open(picklePath, 'wb') as f:\n",
    "            pickle.dump(allCharacteristics, f)\n",
    "        print(\"Saved pickle: %s\"%picklePath)\n",
    "        \n",
    "    return allCharacteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    #print(data.clusterGroup)\n",
    "     allCharacteristics= getUnitsMainCharacteristics(data, groupList=[\"Good\"], sample=200, redowaveform=True,redofiringrate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.14 Plot units carateristics (variable input from waveform and firing rate) across animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plot waveform characteristics \n",
    "def PlotUnitsCharacteristics(animalList=[],groupList=[\"Good\"], sample=200, redowaveform=False,redofiringrate=False, \n",
    "                             CharacteristicsToPlot=[\"PropISI\",\"HalfWidth\",\"PeakToValley\"],redoPreprocess=False,saveAsPickle=False):\n",
    "    sessionindex=0\n",
    "    allCharacteristicsAcrossSessions={}\n",
    "    \n",
    "    \n",
    "    if not animalList:\n",
    "        animalList = [os.path.basename(path) for path in sorted(glob.glob(root+\"/MOU*\"))]\n",
    "    \n",
    "    #animalList=[\"MOU025\",\"MOU026\",\"MOU027\",\"MOU035\",\"MOU074\"]\n",
    "    #animalList=[\"MOU074\"]\n",
    "    \n",
    "    for CharacteristicToPlot in CharacteristicsToPlot:\n",
    "        allCharacteristicsAcrossSessions[CharacteristicToPlot]=[]\n",
    "    \n",
    "    print(allCharacteristicsAcrossSessions)\n",
    "\n",
    "    #list of tags (tag = empty file in the session folder with a specific name)\n",
    "    #leave empty for no tag\n",
    "    tagList = [\"GoodPerfo\"]\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    \n",
    "    if len(CharacteristicsToPlot)>2:\n",
    "        #import numpy as np\n",
    "        from mpl_toolkits.mplot3d import Axes3D\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "    else:\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "            \n",
    "    \n",
    "    for animal in animalList:\n",
    "        print(\"Animal\", animal)\n",
    "        #Get the list of all session\n",
    "        sessionList = [os.path.basename(expPath) for expPath in glob.glob(root+\"/\"+animal+\"/Experiments/MOU*\")]\n",
    "        sessionList = sorted(sessionList)\n",
    "        if not sessionList:\n",
    "            print(\"no sessions\")\n",
    "            return\n",
    "\n",
    "        #loop through sessions\n",
    "        for session in sessionList:  \n",
    "    \n",
    "            #if tag list is not emtpy\n",
    "            if tagList:\n",
    "                #check if the session has one of the tag\n",
    "                if not has_tag(root, animal, session, tagList):\n",
    "                    continue\n",
    "    \n",
    "                print(session)\n",
    "                #load data for this session (add redoPreprocess=True to overwrite preprocess)\n",
    "                sessionData = Data(root, animal, session, paramCarola, redoPreprocess=redoPreprocess)\n",
    "                \n",
    "                if not sessionData.hasSpike:\n",
    "                    print(\"########\")\n",
    "                    print(\"%s has no spike\" %sessionData.experiment)\n",
    "                    print(\"########\")\n",
    "                    continue\n",
    "            \n",
    "                \n",
    "                \n",
    "                allCharacteristics=getUnitsMainCharacteristics(sessionData, groupList=groupList,sample=sample,redofiringrate=redofiringrate,redowaveform=redowaveform,saveAsPickle=saveAsPickle)\n",
    "\n",
    "                for CharacteristicToPlot in CharacteristicsToPlot:\n",
    "                    allCharacteristicsAcrossSessions[CharacteristicToPlot]=allCharacteristicsAcrossSessions[CharacteristicToPlot]+allCharacteristics[CharacteristicToPlot]\n",
    "                \n",
    "                \n",
    "                sessionindex+=1\n",
    "\n",
    "\n",
    "        \n",
    "    #return allCharacteristicsAcrossSessions,sessionindex\n",
    "\n",
    "    if len(CharacteristicsToPlot)>2:\n",
    "        #import numpy as np\n",
    "\n",
    "        ax.scatter(allCharacteristicsAcrossSessions[CharacteristicsToPlot[0]],allCharacteristicsAcrossSessions[CharacteristicsToPlot[1]],allCharacteristicsAcrossSessions[CharacteristicsToPlot[2]])\n",
    "\n",
    "        ax.set_xlabel(CharacteristicsToPlot[0])\n",
    "        ax.set_ylabel(CharacteristicsToPlot[1])\n",
    "        ax.set_zlabel(CharacteristicsToPlot[2])\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        ax.scatter(allCharacteristicsAcrossSessions[CharacteristicsToPlot[0]],allCharacteristicsAcrossSessions[CharacteristicsToPlot[1]])\n",
    "\n",
    "        ax.set_xlabel(CharacteristicsToPlot[0])\n",
    "        ax.set_ylabel(CharacteristicsToPlot[1])\n",
    "        ax.set_xlim(0,0.5)\n",
    "\n",
    "\n",
    "            #def randrange(n, vmin, vmax):\n",
    "            #    return (vmax - vmin)*np.random.rand(n) + vmin\n",
    "\n",
    "\n",
    "\n",
    "            #\n",
    "            #ax.scatter(allFr, allhalfwidth, allpeaktovalley)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    if saveAsPickle:\n",
    "        VariableNamesForPickelName=''.join(CharacteristicsToPlot)\n",
    "        PickleName=\"AllUnits\" + VariableNamesForPickelName + \".p\"\n",
    "        picklePath = os.path.join(root,\"ALLMOU_Analysis\",PickleName)\n",
    "        AllUnitsCharacteristics={}\n",
    "        AllUnitsCharacteristics[\"Characteristics\"]=allCharacteristicsAcrossSessions\n",
    "        AllUnitsCharacteristics[\"Animals\"]=animalList\n",
    "        with open(picklePath, 'wb') as f:\n",
    "            pickle.dump(AllUnitsCharacteristics, f)\n",
    "        print(\"Saved pickle: %s\"%picklePath)\n",
    "        \n",
    "    \n",
    "    return allCharacteristicsAcrossSessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    #animalList=[\"MOU025\",\"MOU026\",\"MOU027\",\"MOU035\",\"MOU074\",\"MOU075\",\"MOU093\"]\n",
    "    \n",
    "    ##############################\n",
    "\n",
    "    ##############################\n",
    "    \n",
    "    allCharacteristicsAcrossSessions=PlotUnitsCharacteristics(groupList=[\"Good\"], sample=200, redowaveform=True,redofiringrate=False,CharacteristicsToPlot=[\"HalfWidth\",\"PeakToValley\"],redoPreprocess=False,saveAsPickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3.17 Mean waveforms and autocorrelogram  for all units of this session \n",
    "\n",
    "For all the clusters of one session. Each spike halfwidth and peak to valley is plotted verus all spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotWaveformAndACGPerSession(data, groupList=[\"Good\"], sample=200, redowaveform=False):\n",
    "    \n",
    "    \n",
    "    ###get spike halfwidth and peak-to-valley for all the units already characterzied \n",
    "    AllUnitspicklePath=  os.path.join(root,\"ALLMOU_Analysis\",\"AllUnitsHalfWidthPeakToValley.p\")\n",
    "    if os.path.isfile(AllUnitspicklePath):\n",
    "        with open(AllUnitspicklePath, 'rb') as f:\n",
    "            AllUnitsCharacteristics = pickle.load(f)\n",
    "            \n",
    "        AllUnitsCharacteristics\n",
    "        AllUnitsCharacteristicsData=AllUnitsCharacteristics[\"Characteristics\"]\n",
    "        \n",
    "        \n",
    "        \n",
    "    else:\n",
    "        print(\"no waveform carateristics saved in ALLMOU_Analysis folder\")\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    groupList=[\"Good\"]\n",
    "    TotalNClusters=0\n",
    "    for shank in data.clusterGroup:\n",
    "            #print(\"Shank\", shank)\n",
    "            for group in data.clusterGroup[shank]:\n",
    "                if group not in groupList:\n",
    "                    continue\n",
    "                for clu in data.clusterGroup[shank][group]:\n",
    "                    #print(\"Clu\",clu)\n",
    "                    TotalNClusters+=1\n",
    "\n",
    "    plt.figure(figsize=(15,3*TotalNClusters))    \n",
    "    ClusterN=0\n",
    "    for shank in data.clusterGroup:\n",
    "            #print(\"Shank\", shank)\n",
    "            for group in data.clusterGroup[shank]:\n",
    "                if group not in groupList:\n",
    "                    continue\n",
    "\n",
    "                for clu in data.clusterGroup[shank][group]:\n",
    "                    ClusterN+=1\n",
    "                    plt.subplot(TotalNClusters,3,ClusterN*3-2)\n",
    "                    plot_autocorrelogram(data,shank,clu,1,30)\n",
    "                    thisax=plt.subplot(TotalNClusters,3,ClusterN*3-1)\n",
    "                    caracteristics=plot_mean_waveform(data, shank, clu,redo=redowaveform,plotAllSpikes=False,ax1=thisax)[1]\n",
    "                    plt.subplot(TotalNClusters,3,ClusterN*3)\n",
    "                    plt.scatter(AllUnitsCharacteristicsData[\"HalfWidth\"],AllUnitsCharacteristicsData[\"PeakToValley\"])\n",
    "                    plt.scatter(caracteristics[\"HalfWidth\"],caracteristics[\"PeakToValley\"],marker='x',s=200)\n",
    "                    plt.xlim(0,0.5)\n",
    "                    #print(\"clu\",clu)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    plotWaveformAndACGPerSession(data, groupList=[\"Good\"], sample=200, redowaveform=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.18 For a given cluster find singificant modulations during run \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetModulatedPortionsDuringRun(data,shank,cluster,runType=\"all\",printoutput=True,plotoutput=True,ax=None):\n",
    " \n",
    "    plotaxinfo,meanFiring,nSideBin,spikeHist,allSpeed,allDuration = plot_normalized_running_periods_firing_rate(data,shank,cluster,binSize=0.25,SideLength=3,runType=runType,ax=ax)\n",
    "    percentileShuffle,allMeanShuffledFiring = plot_shuffled_running_firing_rate(data,shank,cluster,binSize=0.25,SideLength=3,ax=plotaxinfo,nShuffle=500,runType=runType)\n",
    "    allMeanShuffledFiring = np.array(allMeanShuffledFiring)\n",
    "\n",
    "    \n",
    "    MeanFiringRateDuringRun=np.nanmean(meanFiring[nSideBin+1:-nSideBin])\n",
    "    #print(MeanFiringRateDuringRun)\n",
    "    if MeanFiringRateDuringRun<0.1:\n",
    "        if printoutput:\n",
    "            print(\"warning !!! there is not enough spike\")\n",
    "        ModulationSign=\"not enough spikes\"\n",
    "        ModulationResults={\n",
    "            \"AllModulatedIndexes\":[],\n",
    "            \"BiggestModulationIndexes\":[],\n",
    "            \"MeanFiringRateZscored\":[],\n",
    "            \"AllSignificantModulationIndexInRunSameSignThanBiggest\":[],\n",
    "            \"AllModulatedPortionsSameSignThatBiggest\":[],\n",
    "            \"ModulatedFractionSameSignThatBiggest\":[],\n",
    "            \"BiggestModulationSign\":ModulationSign\n",
    "            }\n",
    "        \n",
    "        \n",
    "        InsideRunIndexes=[]\n",
    "        if not plotoutput:\n",
    "            plt.close()\n",
    "        \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        MeanFiringRateZscored=stats.zscore(meanFiring)\n",
    "        #####\n",
    "        # now find the biggest modulation\n",
    "        RunBins=(np.arange(len(meanFiring))>=nSideBin) & (np.arange(len(meanFiring))<=(len(meanFiring)-(nSideBin+1)))\n",
    "        InsideRunIndexes=[x for x,value in enumerate(RunBins.tolist()) if value]\n",
    "\n",
    "        ## find all the beg and end of all periods of modulation beyond global bands of confidence during run (ModulationBeyondGlobalLimit)\n",
    "\n",
    "        globalpercentile=np.percentile(allMeanShuffledFiring,[0.5,99.5])\n",
    "\n",
    "        ModulatedFiringRateEpoch=(meanFiring>=globalpercentile[-1]) | (meanFiring<=globalpercentile[0])\n",
    "        ModulatedIndexes=[i for i,value in enumerate(ModulatedFiringRateEpoch) if value]\n",
    "        ModulationBeyondGlobalLimit=contiguous_regions(ModulatedFiringRateEpoch)\n",
    "\n",
    "        #now the modulation beyond global bands are extended to pointwise limits \n",
    "        \n",
    "        PointConfidenceBand=np.percentile(allMeanShuffledFiring,[2.5,97.5],axis=0)\n",
    "        LowPointLimit=PointConfidenceBand[0].tolist()\n",
    "        HighPointLimit=PointConfidenceBand[1].tolist()\n",
    "\n",
    "        AllModulatedIndexes=[]\n",
    "        Modulationmagnitude=[]\n",
    "        for globallimits in ModulationBeyondGlobalLimit:\n",
    "            if printoutput:\n",
    "                print(\"significant portion beyond globallimits: %s\"%globallimits)\n",
    "            if np.nanmean(meanFiring[globallimits[0]:globallimits[1]]) >= globalpercentile[-1]: # case positive modulation\n",
    "                ModulationBeyondPointWiseLimit=contiguous_regions(meanFiring>HighPointLimit)\n",
    "                \n",
    "            else:   # case negative modulation         \n",
    "                ModulationBeyondPointWiseLimit=contiguous_regions(meanFiring<LowPointLimit)\n",
    "                \n",
    "            ModulationBeyondPointWiseLimit=ModulationBeyondPointWiseLimit.tolist()\n",
    "\n",
    "\n",
    "            for pointwiselimits in ModulationBeyondPointWiseLimit:\n",
    "                Intersection=np.intersect1d(np.arange(pointwiselimits[0],pointwiselimits[1]),np.arange(globallimits[0],globallimits[1]))\n",
    "                Intersection=Intersection.tolist()\n",
    "                if Intersection:\n",
    "                    RunIntersection=np.intersect1d(np.arange(pointwiselimits[0],pointwiselimits[1]),InsideRunIndexes)\n",
    "                    RunIntersection=RunIntersection.tolist()\n",
    "                    if RunIntersection:\n",
    "                        PointWiseSignificantModulation=list(range(pointwiselimits[0],pointwiselimits[1]))       \n",
    "                        AllModulatedIndexes.append(PointWiseSignificantModulation)\n",
    "                        ModulatedPortionDuringRunOnly=[i for i in PointWiseSignificantModulation if i in InsideRunIndexes]\n",
    "                        if printoutput: \n",
    "                            print (\"ModulatedPortionDuringRunOnly\",ModulatedPortionDuringRunOnly,\"PointWiseSignificantModulation\",PointWiseSignificantModulation,\"InsideRunIndexes\",InsideRunIndexes)\n",
    "\n",
    "                        #ThisModulationMagnitude=np.nanmean(meanFiring[ModulatedPortionDuringRunOnly]- np.asarray(HighPointLimit)[ModulatedPortionDuringRunOnly])\n",
    "                        \n",
    "                        #print(\"zscorevalues= \",MeanFiringRateZscored[ModulatedPortionDuringRunOnly])\n",
    "                        ThisModulationMagnitude=np.nanmax(np.abs(MeanFiringRateZscored[ModulatedPortionDuringRunOnly]))\n",
    "                        #Modulationmagnitude.append(ThisModulationMagnitude)\n",
    "                        if np.nanmean(meanFiring[globallimits[0]:globallimits[1]]) >= globalpercentile[-1]: # case positive modulation\n",
    "                            NameForPrint=\"This is a positive modulation\"\n",
    "                            Modulationmagnitude.append(ThisModulationMagnitude)\n",
    "\n",
    "                        else:  # case negative modulation \n",
    "                            NameForPrint=\"This is a negative modulation\"\n",
    "                            Modulationmagnitude.append(ThisModulationMagnitude*-1)\n",
    "                            \n",
    "                        if printoutput:    \n",
    "                            print(\"%s of inside run amplitude %s Zscored\" %(NameForPrint,Modulationmagnitude[-1]))\n",
    "                            print(\"###########\")\n",
    "                            print(\"\")\n",
    "                            \n",
    "                            \n",
    "                            \n",
    "                            \n",
    "        #remove duplicate\n",
    "        AllModulatedIndexes=list(AllModulatedIndexes for AllModulatedIndexes,_ in itertools.groupby(AllModulatedIndexes))\n",
    "        Modulationmagnitude=pd.unique(Modulationmagnitude)\n",
    "\n",
    "        \n",
    "        if printoutput:           \n",
    "            print(\"\")\n",
    "            print(\"###\")\n",
    "            print (\"Modulationmagnitude\",Modulationmagnitude)\n",
    "            print(\"AllModulatedIndexes\",AllModulatedIndexes)\n",
    "            print(\"\")\n",
    "        AllSignificantModulationIndexInRunSameSignThanBiggest=[]\n",
    "        \n",
    "        if len(Modulationmagnitude)>0:\n",
    "            ## this finds the index of strongest modulation (if there are multiple points of modulation)\n",
    "            ModulationmagnitudeAbs=list(np.abs(Modulationmagnitude))\n",
    "            Index=ModulationmagnitudeAbs.index(max(np.abs(Modulationmagnitude)))\n",
    "\n",
    "            ## now find all the modulation of the same sign than the same modulation max\n",
    "\n",
    "            if Modulationmagnitude[Index]<0:\n",
    "                ModulationSign=\"negative\"                \n",
    "            else:\n",
    "                ModulationSign=\"positive\"\n",
    "                \n",
    "                \n",
    "            for count,value in enumerate(AllModulatedIndexes):\n",
    "                if Modulationmagnitude[count] * Modulationmagnitude[Index] > 0 : # we only append modulation of the same sign than the biggest modulation\n",
    "                    AllSignificantModulationIndexInRunSameSignThanBiggest=AllSignificantModulationIndexInRunSameSignThanBiggest+value\n",
    "                    thisxaxis=[x+0.5 for x in value]                    \n",
    "                    if count==Index:\n",
    "                        plt.plot(thisxaxis,meanFiring[value],'go',markersize=12)\n",
    "                        BiggestModulationIndexes=value\n",
    "\n",
    "                    else:\n",
    "                        plt.plot(thisxaxis,meanFiring[value],'ro',markersize=12)\n",
    "                else:\n",
    "                    thisxaxis=[x+0.5 for x in value]\n",
    "                    plt.plot(thisxaxis,meanFiring[value],'r+',markersize=12)\n",
    "\n",
    "\n",
    "\n",
    "            #remove duplicate\n",
    "            #AllModulatedIndexes=list(AllModulatedIndexes for AllModulatedIndexes,_ in itertools.groupby(AllModulatedIndexes))\n",
    "\n",
    "            AllSignificantModulationIndexInRunSameSignThanBiggest=set(AllSignificantModulationIndexInRunSameSignThanBiggest) ## return unique element (index) of the list only same signe modulation than biggestone\n",
    "            AllSignificantModulationIndexInRunSameSignThanBiggest=list(AllSignificantModulationIndexInRunSameSignThanBiggest)\n",
    "            #AllSignificantModulationIndexInRunSameSignThanBiggest=AllSignificantModulationIndexInRunSameSignThanBiggest.sort()\n",
    "            AllSignificantModulationIndexInRunSameSignThanBiggest.sort()\n",
    "            \n",
    "            AllModulatedPortionsSameSignThatBiggest=[(x-InsideRunIndexes[0])/(len(InsideRunIndexes)-1) for x in AllSignificantModulationIndexInRunSameSignThanBiggest]\n",
    "            AllModulatedPortionsSameSignThatBiggest.sort()\n",
    "\n",
    "            ModulatedFractionSameSignThatBiggest=len(AllSignificantModulationIndexInRunSameSignThanBiggest)/len(InsideRunIndexes)\n",
    "            if printoutput:\n",
    "                print(\"Biggest Modulation Sign: %s\" %ModulationSign)\n",
    "                print(\"AllSignificantModulationIndexInRunSameSignThanBiggest\", AllSignificantModulationIndexInRunSameSignThanBiggest)\n",
    "                print(\"\")\n",
    "                print(\"Modulated Portions: %s\"%AllModulatedPortionsSameSignThatBiggest)\n",
    "                print(\"\")\n",
    "                print(\"Modulated Fraction (relative to run): %s\"%ModulatedFractionSameSignThatBiggest)\n",
    "            \n",
    "\n",
    "            \n",
    "            ModulationResults={\n",
    "                \"AllModulatedIndexes\":AllModulatedIndexes,\n",
    "                \"BiggestModulationIndexes\":BiggestModulationIndexes,\n",
    "                \"MeanFiringRateZscored\":MeanFiringRateZscored,\n",
    "                \"AllSignificantModulationIndexInRunSameSignThanBiggest\":AllSignificantModulationIndexInRunSameSignThanBiggest,\n",
    "                \"AllModulatedPortionsSameSignThatBiggest\":AllModulatedPortionsSameSignThatBiggest,\n",
    "                \"ModulatedFractionSameSignThatBiggest\":ModulatedFractionSameSignThatBiggest,\n",
    "                \"BiggestModulationSign\":ModulationSign\n",
    "                }\n",
    "            \n",
    "            \n",
    "         \n",
    "\n",
    "                \n",
    "#             ModulationResults=[AllModulatedIndexes,\n",
    "#                         AllModulatedPortionsSameSignThatBiggest,\n",
    "#                         AllSignificantModulationIndexInRunSameSignThanBiggest,                     \n",
    "#                         BiggestModulationIndexes,\n",
    "#                         MeanFiringRateZscored,\n",
    "#                         ModulatedFractionSameSignThatBiggest,\n",
    "#                         ModulationSign]\n",
    "                \n",
    "            \n",
    "            \n",
    "        else: # case there is no modulation\n",
    "            ModulationSign=\"not modulated\"\n",
    "            ModulationResults={\n",
    "                \"AllModulatedIndexes\":[],\n",
    "                \"BiggestModulationIndexes\":[],\n",
    "                \"MeanFiringRateZscored\":[],\n",
    "                \"AllSignificantModulationIndexInRunSameSignThanBiggest\":[],\n",
    "                \"AllModulatedPortionsSameSignThatBiggest\":[],\n",
    "                \"ModulatedFractionSameSignThatBiggest\":[],\n",
    "                \"BiggestModulationSign\":ModulationSign\n",
    "                }\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            if printoutput:\n",
    "                print(\"This unit is %s during run\" %ModulationResults)\n",
    "            \n",
    "        if not plotoutput:\n",
    "            plt.close()\n",
    "#                 from IPython.display import clear_output\n",
    "#                 clear_output()\n",
    "        \n",
    "    return ModulationResults,InsideRunIndexes,spikeHist,allSpeed,allDuration,nSideBin,plotaxinfo\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    \n",
    "    ##############################\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    #GetModulatedPortionsDuringRun(data,shank,cluster,printoutput=True,plotoutput=True)\n",
    "    ThisModulationResults,InsideRunIndexes,spikeHist,allSpeed,allDuration,nSideBin,plotaxinfo=GetModulatedPortionsDuringRun(data,shank,cluster,printoutput=True,plotoutput=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.21 For modulated portions correlate firing rate vs run kinematic trial by trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def correlationFrateVsKinematic(data,shank,cluster,runType=\"all\",redoModulation=False,showplot=True):\n",
    "    \n",
    "    FiringRateCorrelationsWith={\n",
    "            \"SpeedDuringModulatedEpoch\":{},\n",
    "            \"AccelDuringModulatedEpoch\":{},\n",
    "            \"RunDistance\":{},\n",
    "            \"RunDuration\":{}\n",
    "            }\n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(10,15))\n",
    "    gs1 = gridspec.GridSpec(1, 1)\n",
    "    ax1=fig.add_subplot(gs1[0,0])\n",
    "\n",
    "\n",
    "\n",
    "    ## load if exist and not redoModulation the saved modulation data\n",
    "    picklePath=os.path.join(root,data.animal,\"Experiments\",data.experiment,\"Analysis\",\"ModulationDuring\" + runType + \"Runs.p\")\n",
    "    #print(picklePath)\n",
    "\n",
    "    if os.path.exists(picklePath) and not redoModulation:\n",
    "        ModulationResults=pickle.load(open(picklePath,\"rb\"))\n",
    "        print(\"spiking modulation  data loaded from %s\"%picklePath)\n",
    "\n",
    "        if (ModulationResults[\"BiggestModulationSign\"][shank][cluster]==\"not modulated\") or (ModulationResults[\"BiggestModulationSign\"][shank][cluster]==\"not enough spikes\"):\n",
    "            FiringRateCorrelationsWith={\n",
    "            \"SpeedDuringModulatedEpoch\":[],\n",
    "            \"AccelDuringModulatedEpoch\":[],\n",
    "            \"RunDistance\":[],\n",
    "            \"RunDuration\":[]\n",
    "            }\n",
    "            \n",
    "            \n",
    "            if not showplot:\n",
    "                plt.close()\n",
    "            return FiringRateCorrelationsWith\n",
    "\n",
    "        # get and plot behavioral and neural data for all the behaviral epochs\n",
    "        plotaxinfo,meanFiring,nSideBin,spikeHist,allSpeed,allDuration = plot_normalized_running_periods_firing_rate(data,shank,cluster,binSize=0.25,SideLength=3,runType=runType,ax=ax1)\n",
    "        ModulatedBins=ModulationResults[\"BiggestModulationIndexes\"][shank][cluster]\n",
    "        plotaxinfo.plot([i+0.5 for i in ModulatedBins],meanFiring[ModulatedBins],'og')\n",
    "\n",
    "    else:\n",
    "        ModulationResults,InsideRunIndexes,spikeHist,allSpeed,allDuration,nSideBin,plotaxinfo=GetModulatedPortionsDuringRun(data,shank,cluster,runType=\"all\",printoutput=False,plotoutput=True,ax=ax1)\n",
    "        ModulatedBins=ModulationResults[\"BiggestModulationIndexes\"]\n",
    "\n",
    "    gs1.tight_layout(fig, rect=[0.25, 0.66, 0.75, 1],h_pad=0.33)\n",
    "\n",
    "\n",
    "\n",
    "    FRateDuringModulatedEpoch=[]\n",
    "    SpeedDuringModulatedEpoch=[]\n",
    "    AccelDuringModulatedEpoch=[]\n",
    "    DistanceRun=[]\n",
    "    for x,r in enumerate(allDuration):\n",
    "        if sum(spikeHist[x][ModulatedBins])>=0:\n",
    "\n",
    "            #frate trial by trial\n",
    "            FRateDuringModulatedEpoch.append(np.nanmean(spikeHist[x][ModulatedBins]))\n",
    "\n",
    "            #running speed\n",
    "            SpeedDuringModulatedEpoch.append(np.nanmean(allSpeed[x][ModulatedBins]))\n",
    "\n",
    "            #acceleration\n",
    "            BinDuration=r/(len(allSpeed[x])-2*nSideBin)\n",
    "            SpeedInModulatedBins=allSpeed[x][ModulatedBins]\n",
    "            Acceleration=(SpeedInModulatedBins[-1]-SpeedInModulatedBins[0])/((sum(ModulatedBins)-1)*BinDuration) \n",
    "            AccelDuringModulatedEpoch.append(Acceleration)\n",
    "\n",
    "            #run distance\n",
    "\n",
    "            TotalDistanceRun=sum(allSpeed[x][int(nSideBin):-int(nSideBin)]*BinDuration)\n",
    "            DistanceRun.append(TotalDistanceRun)\n",
    "\n",
    "\n",
    "\n",
    "    AllKinematicParameters={\n",
    "            \"SpeedDuringModulatedEpoch\":SpeedDuringModulatedEpoch,\n",
    "            \"AccelDuringModulatedEpoch\":AccelDuringModulatedEpoch,\n",
    "            \"RunDistance\":DistanceRun,\n",
    "            \"RunDuration\":allDuration\n",
    "            }\n",
    "   # AllKinematicParameters=sorted(AllKinematicParameters)\n",
    "    AllYLabels=[\"Running accel $\\mathregular{(cm/s^2)}$\",\"Run distance (cm)\",\"Run duration (s)\",\"Running speed (cm/s)\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gs2 = gridspec.GridSpec(2, 2)\n",
    "    subplotcoordinates=[[0,1],[1,0],[1,1],[0,0]]\n",
    "\n",
    "    x=FRateDuringModulatedEpoch\n",
    "    \n",
    "    if np.median(x)<0.2: # case when firing rate during modulation is very low accross trials \n",
    "        FiringRateCorrelationsWith={\n",
    "            \"SpeedDuringModulatedEpoch\":[],\n",
    "            \"AccelDuringModulatedEpoch\":[],\n",
    "            \"RunDistance\":[],\n",
    "            \"RunDuration\":[]\n",
    "            }\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"firing rate too low for correlation stats\")\n",
    "        print(\"\")\n",
    "        if not showplot:\n",
    "            plt.close()\n",
    "        return FiringRateCorrelationsWith\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for count,param in enumerate(sorted(AllKinematicParameters)):\n",
    "        ax = fig.add_subplot(gs2[subplotcoordinates[count][0],subplotcoordinates[count][1]])\n",
    "        print(param)\n",
    "        y=AllKinematicParameters[param]\n",
    "        \n",
    "        fit = np.polyfit(x,y,1)\n",
    "        fit_fn = np.poly1d(fit) \n",
    "        #plt.plot(FRateDuringModulatedEpoch,RSpeedDuringModulatedEpoch,'o')\n",
    "\n",
    "        ax.plot(x,y, 'ro', x, fit_fn(x), '-k',linewidth=2)\n",
    "        #ax.ylim(7,30)\n",
    "        ax.set_xlabel(\"Firing rate (Hz)\",fontsize=20,weight=\"bold\")\n",
    "        ax.set_ylabel(AllYLabels[count],fontsize=20,weight=\"bold\")\n",
    "\n",
    "        MinMaxForPlot=np.percentile(AllKinematicParameters[param],[2,98])\n",
    "        ax.set_ylim(MinMaxForPlot)\n",
    "        SpearManResults=stats.spearmanr(x,y)\n",
    "        rvalue=str(round(SpearManResults[0],2));\n",
    "        if SpearManResults[1]<0.0001:\n",
    "            pvalue=\"p<0.0001\"\n",
    "        else:\n",
    "            pvalue=\"p=\"+ str(round(SpearManResults[1],4))\n",
    "\n",
    "        title=\"r=%s, %s\"%(rvalue,pvalue)\n",
    "        ax.set_title(title,fontsize=20,weight=\"bold\")\n",
    "        ax.tick_params(axis='both',which='major',labelsize=20,width=2) \n",
    "        FiringRateCorrelationsWith[param]=([SpearManResults[0],SpearManResults[1]])\n",
    "\n",
    "    gs2.tight_layout(fig, rect=[0, 0, 1, 0.66],h_pad=0.33)\n",
    "\n",
    "    if not showplot:\n",
    "        plt.close()\n",
    "    \n",
    "    return FiringRateCorrelationsWith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():   \n",
    "    #x,y,AllKinematicParameters,meanFiring=correlationFrateVsKinematic(data,shank,cluster)\n",
    "    #######################\n",
    "\n",
    "    \n",
    "    #######################\n",
    "    FiringRateCorrelationsWith=correlationFrateVsKinematic(data,shank,cluster,showplot=True,redoModulation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.19 For all clusters of this session, finds singificant modulations during run AND SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def GetAllSignificantModulation(data, groupList=[\"Good\"], saveAsPickle=True, redo=False, binSize=0.25, \n",
    "                                   SideLength=3, nShuffle=500,runType=\"all\",printoutput=False,plotoutput=False):\n",
    "    \"\"\"\n",
    "    Loops through the units of a session and run GetModulatedPortionsDuringRun\n",
    "    to detect signigicant modulation during distinct type of behavioral epochs\n",
    "    then save the data in a pickles\n",
    "    \n",
    "    \"\"\"\n",
    "    #load and return the pickle if it exists (and redo=False)\n",
    "    \n",
    "    if not data.hasSpike:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    picklePath=os.path.join(data.analysisPath,\"ModulationDuring\" + runType + \"Runs.p\")\n",
    "    \n",
    "\n",
    "    if (not redo) and os.path.exists(picklePath):\n",
    "        with open(picklePath, 'rb') as f:\n",
    "            print(\"loaded pickle %s\"%picklePath)\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    #check that groupList is a list\n",
    "    if not isinstance(groupList,list):\n",
    "        groupList=[groupList]\n",
    "        \n",
    "    #Get mean firing rates for each cluster\n",
    "    SavedInsideRunIdex=[]\n",
    "    \n",
    "    ModulationSavedData={\n",
    "        \"AllModulatedIndexes\":{},\n",
    "        \"BiggestModulationIndexes\":{},\n",
    "        \"MeanFiringRateZscored\":{},\n",
    "        \"AllSignificantModulationIndexInRunSameSignThanBiggest\":{},\n",
    "        \"AllModulatedPortionsSameSignThatBiggest\":{},\n",
    "        \"ModulatedFractionSameSignThatBiggest\":{},\n",
    "        \"BiggestModulationSign\":{},\n",
    "        }\n",
    "\n",
    "    nSideBin = None\n",
    "    for shank in sorted(data.clusterGroup):               \n",
    "        for key in ModulationSavedData:\n",
    "            ModulationSavedData[key][shank]={}\n",
    "        for group in data.clusterGroup[shank]:\n",
    "            if (groupList is not None) and (group not in groupList):\n",
    "                continue\n",
    "            for cluster in sorted(data.clusterGroup[shank][group]):\n",
    "                print(\"Shank %s  Cluster %s\" %(shank,cluster))\n",
    "                ModulationResults,InsideRunIndexes=GetModulatedPortionsDuringRun(data,shank,cluster,printoutput=printoutput,plotoutput=plotoutput)[0:2]\n",
    "                if ModulationResults:\n",
    "                    for key in sorted(ModulationSavedData):\n",
    "                        ModulationSavedData[key][shank][cluster]=ModulationResults[key]\n",
    "                if InsideRunIndexes and not SavedInsideRunIdex:\n",
    "                    SavedInsideRunIdex=InsideRunIndexes\n",
    "\n",
    "    \n",
    "    \n",
    "    ModulationSavedData[\"InsideRunIndexese\"]=InsideRunIndexes\n",
    "    ## Create and save pickle\n",
    "    if saveAsPickle:\n",
    "        with open(picklePath, 'wb') as f:\n",
    "            pickle.dump(ModulationSavedData, f)\n",
    "        print(\"Saved pickle: %s\"%picklePath)\n",
    "\n",
    "    \n",
    "    \n",
    "    return ModulationSavedData\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():   \n",
    "\n",
    "    ModulationSavedData=GetAllSignificantModulation(data,redo=True,plotoutput=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():   \n",
    "\n",
    "    runType=\"all\"\n",
    "    picklePath=os.path.join(data.analysisPath,\"ModulationDuring\" + runType + \"Runs.p\")\n",
    "    print(picklePath)\n",
    "    with open(picklePath, 'rb') as f:\n",
    "        print(\"loaded pickle %s\"%picklePath)\n",
    "        test=pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.21 For modulated portions of all units in this session correlate firing rate vs run kinematic trial by trial AND SAVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def GetAllCorrCoefFRateVsKinematic(data, groupList=[\"Good\"], saveAsPickle=False, redo=False, redoModulation=False, binSize=0.25, \n",
    "                                  runType=\"all\",showplot=False):\n",
    "    \"\"\"\n",
    "    Loops through the units of a session and run correlationFrateVsKinematic\n",
    "    to returm correlation beteen firing rate and kinematic, trial by trial, \n",
    "    at the time of the maximum modulation\n",
    "    then save the data in a pickles\n",
    "    \n",
    "    \"\"\"\n",
    "    #load and return the pickle if it exists (and redo=False)\n",
    "    \n",
    "    if not data.hasSpike:\n",
    "        return\n",
    "    \n",
    "    \n",
    "    picklePath=os.path.join(data.analysisPath,\"FRateVsKinematciCorrDuring\" + runType + \"Runs.p\")\n",
    "    \n",
    "\n",
    "    if (not redo) and os.path.exists(picklePath):\n",
    "        with open(picklePath, 'rb') as f:\n",
    "            print(\"loaded pickle %s\"%picklePath)\n",
    "            return pickle.load(f)\n",
    "    \n",
    "    #check that groupList is a list\n",
    "    if not isinstance(groupList,list):\n",
    "        groupList=[groupList]\n",
    "        \n",
    "    #Get mean firing rates for each cluster\n",
    "    SavedInsideRunIdex=[]\n",
    "    \n",
    "    FRateCorrelationWithSavedData={\n",
    "            \"SpeedDuringModulatedEpoch\":{},\n",
    "            \"AccelDuringModulatedEpoch\":{},\n",
    "            \"RunDistance\":{},\n",
    "            \"RunDuration\":{}\n",
    "            }\n",
    "\n",
    "    nSideBin = None\n",
    "    for shank in sorted(data.clusterGroup):               \n",
    "        for key in FRateCorrelationWithSavedData:\n",
    "            FRateCorrelationWithSavedData[key][shank]={}\n",
    "        for group in data.clusterGroup[shank]:\n",
    "            if (groupList is not None) and (group not in groupList):\n",
    "                continue\n",
    "            for cluster in sorted(data.clusterGroup[shank][group]):\n",
    "                print(\"Shank %s  Cluster %s\" %(shank,cluster))\n",
    "                FiringRateCorrelationsWith=correlationFrateVsKinematic(data,shank,cluster,runType=\"all\",redoModulation=redoModulation,showplot=showplot)\n",
    "                \n",
    "                if FiringRateCorrelationsWith:\n",
    "                    for key in sorted(FRateCorrelationWithSavedData):\n",
    "                        #print(key)\n",
    "                        ###\n",
    "                        #return FRateCorrelationWithSavedData,FiringRateCorrelationsWith\n",
    "                        ###\n",
    "                        FRateCorrelationWithSavedData[key][shank][cluster]=FiringRateCorrelationsWith[key]\n",
    "                else:\n",
    "                    for key in sorted(FRateCorrelationWithSavedData):\n",
    "                        FRateCorrelationWithSavedData[key][shank][cluster]=[]\n",
    "                    \n",
    "                \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Create and save pickle\n",
    "    if saveAsPickle:\n",
    "        with open(picklePath, 'wb') as f:\n",
    "            pickle.dump(FRateCorrelationWithSavedData, f)\n",
    "        print(\"Saved pickle: %s\"%picklePath)\n",
    "\n",
    "    \n",
    "    \n",
    "    return FRateCorrelationWithSavedData\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():   \n",
    "\n",
    "    FRateCorrelationWithSavedData=GetAllCorrCoefFRateVsKinematic(data,redo=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.22 Loop through saved modulation data accross animals and plot some population statistic about modulations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():  \n",
    "    \n",
    "    animalList=[os.path.basename(path) for path in sorted(glob.glob(root+\"/MOU*\"))]\n",
    "\n",
    "    # Redefine animalList if you don't want all the animals\n",
    "    # animalList=[\"MOU015\",\"MOU016\",\"MOU017\",\"MOU018\",\"MOU019\",\"MOU028\",\"MOU029\"]\n",
    "\n",
    "    #print(\"List of animal to loop through: %s\"%animalList)\n",
    "    tagList = [\"GoodPerfo\"]\n",
    "\n",
    "\n",
    "    #Whether to read the existing pickle files (redo=False) or to reload from raw text files (redo=True)\n",
    "    redo=False\n",
    "\n",
    "\n",
    "    runType=\"all\"\n",
    "    binmiddles=np.round(np.arange(-0.95,2.05,.1)*100)/100\n",
    "    binedgesinput=np.round(np.arange(-1,2.1,.1)*10)/10\n",
    "    AllModulatedPortion=[]\n",
    "    AllSignsOfModulation=[]\n",
    "    AllModulatedFraction=[]\n",
    "    AllPeakModulationPositionRelativeToRun=[]\n",
    "    ### loop throught the session and load the saned data on modulation to find the length of average firing rate for the different sessions\n",
    "\n",
    "\n",
    "    #loop on animal\n",
    "    for animal in animalList:\n",
    "\n",
    "\n",
    "        #Get the list of all session\n",
    "        sessionList=[os.path.basename(expPath) for expPath in glob.glob(root+\"/\"+animal+\"/Experiments/MOU*\")]\n",
    "        sessionList=sorted(sessionList)\n",
    "        nbSession=len(sessionList)   \n",
    "\n",
    "\n",
    "        #loop through sessions\n",
    "        #pdb.set_trace()\n",
    "        for index,session in enumerate(sessionList):\n",
    "\n",
    "\n",
    "            if not has_tag(root, animal, session, tagList):\n",
    "                continue       \n",
    "\n",
    "            picklePath=os.path.join(root,animal,\"Experiments\",session,\"Analysis\",\"ModulationDuring\" + runType + \"Runs.p\")\n",
    "            if os.path.exists(picklePath):\n",
    "                ModulationResults=pickle.load(open(picklePath,\"rb\"))\n",
    "    #             print(\"spiking modulation  data loaded from %s\"%picklePath)\n",
    "    #             print(\"\")\n",
    "                for shank in ModulationResults[\"AllModulatedPortionsSameSignThatBiggest\"]:\n",
    "                    for cluster in ModulationResults[\"AllModulatedPortionsSameSignThatBiggest\"][shank]:\n",
    "                        ThisClusterModulationsResults=ModulationResults[\"AllModulatedPortionsSameSignThatBiggest\"][shank][cluster]\n",
    "\n",
    "                        if ThisClusterModulationsResults:\n",
    "                            histcount=np.clip(np.histogram(ThisClusterModulationsResults,binedgesinput)[0],0,1)\n",
    "    #                         print(histcount)\n",
    "\n",
    "                            AllModulatedPortion.append(histcount)\n",
    "\n",
    "                            AllModulatedFraction.append(ModulationResults[\"ModulatedFractionSameSignThatBiggest\"][shank][cluster])\n",
    "\n",
    "\n",
    "                            SignifiantModulationAmplitude = np.abs(ModulationResults[\"MeanFiringRateZscored\"][shank][cluster][ModulationResults['AllSignificantModulationIndexInRunSameSignThanBiggest'][shank][cluster]])\n",
    "                            AllPeakModulationPositionRelativeToRun.append(ModulationResults[\"AllModulatedPortionsSameSignThatBiggest\"][shank][cluster][SignifiantModulationAmplitude.argmax()])                \n",
    "\n",
    "\n",
    "                        ThisClusterSignOfModulation=ModulationResults[\"BiggestModulationSign\"][shank][cluster]\n",
    "                        if ThisClusterSignOfModulation==\"positive\":\n",
    "                            AllSignsOfModulation.append(3)\n",
    "                        elif ThisClusterSignOfModulation==\"negative\":\n",
    "                            AllSignsOfModulation.append(1)\n",
    "                        else:\n",
    "                            AllSignsOfModulation.append(2)\n",
    "\n",
    "            else:            \n",
    "                print(\"no pickle data at %s\"%picklePath)\n",
    "                print(\"\")\n",
    "                continue\n",
    "\n",
    "    plt.figure()            \n",
    "    AllModulatedPortion=np.vstack(AllModulatedPortion)\n",
    "    #print(test.sum(0))\n",
    "    plt.bar(binmiddles,AllModulatedPortion.sum(0),width=0.1)\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    [histcount,binedges]=np.histogram(AllModulatedFraction,np.arange(0,1.5,0.1))\n",
    "    plt.bar(np.arange(0.05,1.45,0.1),histcount,width=0.1)\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    [histcount,binedges]=np.histogram(AllPeakModulationPositionRelativeToRun,binedgesinput)\n",
    "    plt.bar(binmiddles,histcount,width=0.1)\n",
    "\n",
    "\n",
    "    [histcount,binedges]=np.histogram(AllSignsOfModulation,[1,2,3,4])\n",
    "    print(\"Nber of unit negatively/not/posively modulated during run %s/%s/%s\"%(histcount[0],histcount[1],histcount[2]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.23 Loop through saved correlation data accross animals and plot some population statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():  \n",
    "    \n",
    "    animalList=[os.path.basename(path) for path in sorted(glob.glob(root+\"/MOU*\"))]\n",
    "\n",
    "    # Redefine animalList if you don't want all the animals\n",
    "    #animalList=[\"MOU035\",\"MOU026\",\"MOU027\",\"MOU025\"]\n",
    "\n",
    "    #print(\"List of animal to loop through: %s\"%animalList)\n",
    "    tagList = [\"GoodPerfo\"]\n",
    "\n",
    "\n",
    "    #Whether to read the existing pickle files (redo=False) or to reload from raw text files (redo=True)\n",
    "    redo=False\n",
    "\n",
    "\n",
    "    runType=\"all\"\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    AllUNitsFRateCorrelation={\n",
    "        \"SpeedDuringModulatedEpoch\":[],\n",
    "        \"AccelDuringModulatedEpoch\":[],\n",
    "        \"RunDistance\":[],\n",
    "        \"RunDuration\":[]\n",
    "        }\n",
    "    \n",
    "    AllUNitsFRateCorrelationPValue={\n",
    "        \"SpeedDuringModulatedEpoch\":[],\n",
    "        \"AccelDuringModulatedEpoch\":[],\n",
    "        \"RunDistance\":[],\n",
    "        \"RunDuration\":[]\n",
    "        }\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ### loop throught the session and load the saned data on modulation to find the length of average firing rate for the different sessions\n",
    "\n",
    "\n",
    "    #loop on animal\n",
    "    for animal in animalList:\n",
    "\n",
    "\n",
    "        #Get the list of all session\n",
    "        sessionList=[os.path.basename(expPath) for expPath in glob.glob(root+\"/\"+animal+\"/Experiments/MOU*\")]\n",
    "        sessionList=sorted(sessionList)\n",
    "        nbSession=len(sessionList)   \n",
    "\n",
    "\n",
    "        #loop through sessions\n",
    "        #pdb.set_trace()\n",
    "        for index,session in enumerate(sessionList):\n",
    "\n",
    "\n",
    "            if not has_tag(root, animal, session, tagList):\n",
    "                continue       \n",
    "\n",
    "            picklePath=os.path.join(root,animal,\"Experiments\",session,\"Analysis\",\"FRateVsKinematciCorrDuring\" + runType + \"Runs.p\")\n",
    "            if os.path.exists(picklePath):\n",
    "                CorrelationResults=pickle.load(open(picklePath,\"rb\"))\n",
    "                print(\"spiking modulation  data loaded from %s\"%picklePath)\n",
    "    #             print(\"\")\n",
    "                for kinematicvariable in CorrelationResults:\n",
    "                    for shank in CorrelationResults[kinematicvariable]:\n",
    "                        for cluster in CorrelationResults[kinematicvariable][shank]:\n",
    "                            ThisClusterCorrelationResult=CorrelationResults[kinematicvariable][shank][cluster]\n",
    "\n",
    "                            if ThisClusterCorrelationResult:\n",
    "                                AllUNitsFRateCorrelation[kinematicvariable].append(ThisClusterCorrelationResult[0])\n",
    "                                AllUNitsFRateCorrelationPValue[kinematicvariable].append(ThisClusterCorrelationResult[1])\n",
    "                   \n",
    "\n",
    "\n",
    "                            \n",
    "\n",
    "            else:            \n",
    "                print(\"no pickle data at %s\"%picklePath)\n",
    "                print(\"\")\n",
    "                continue\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    binmiddles = np.round(np.arange(-0.95,1.05,.1)*100)/100\n",
    "    binedgesinput = np.round(np.arange(-1,1.1,.1)*10)/10\n",
    "    \n",
    "    for count,kinematicvariable in enumerate(AllUNitsFRateCorrelation):\n",
    "        [histcount,binutput]=np.histogram(AllUNitsFRateCorrelation[kinematicvariable],binedgesinput)\n",
    "        plt.subplot(2,2,count+1)\n",
    "        plt.bar(binmiddles,histcount,width=0.1)\n",
    "        \n",
    "        \n",
    "        PValues=AllUNitsFRateCorrelationPValue[kinematicvariable]\n",
    "        Pthreshold=0.01\n",
    "        SignificantPValues=[x for x in PValues if x<Pthreshold]\n",
    "        PercentSignificant=len(SignificantPValues)/len(PValues)*100\n",
    "        plt.ylabel('count')\n",
    "        plt.xlabel('cor coef')\n",
    "        \n",
    "        plt.title(\"%s, \\n %3.2f percent units are singificantly correlated \" %(kinematicvariable,PercentSignificant))\n",
    "        \n",
    "        \n",
    "\n",
    "#     AllModulatedPortion=np.vstack(AllModulatedPortion)\n",
    "#     #print(test.sum(0))\n",
    "#     plt.bar(binmiddles,AllModulatedPortion.sum(0),width=0.1)\n",
    "\n",
    "\n",
    "#     plt.figure()\n",
    "#     [histcount,binedges]=np.histogram(AllModulatedFraction,np.arange(0,1.5,0.1))\n",
    "#     plt.bar(np.arange(0.05,1.45,0.1),histcount,width=0.1)\n",
    "\n",
    "\n",
    "#     plt.figure()\n",
    "#     [histcount,binedges]=np.histogram(AllPeakModulationPositionRelativeToRun,binedgesinput)\n",
    "#     plt.bar(binmiddles,histcount,width=0.1)\n",
    "\n",
    "\n",
    "#     [histcount,binedges]=np.histogram(AllSignsOfModulation,[1,2,3,4])\n",
    "#     print(\"Nber of unit negatively/not/posively modulated during run %s/%s/%s\"%(histcount[0],histcount[1],histcount[2]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.24 Summary plot for a given cluster. Basic information + modulation + correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cluster_ratevskinematicplot(data,shank,cluster,group,binSize=0.25,SideLength=3,printoutput=False):    \n",
    "    \n",
    "    fig=plt.figure(figsize=(25,30))\n",
    "    \n",
    "    ## trial by trial raster and behavior\n",
    "    gs1 = gridspec.GridSpec(5, 1)\n",
    "    \n",
    "    # Left vertical plot: wheel/lick detection and spike rasters trial by trial\n",
    "    ax1=fig.add_subplot(gs1[0:4,0])\n",
    "    plot_break_cluster(data,shank,cluster,group=\"Good\",legend=False,lick=True)\n",
    "    xlimvalue=ax1.get_xlim()\n",
    "    ax1.set_title(\"\")\n",
    "    \n",
    "     # Below vertical plot : mean firing rate and running speed\n",
    "    ax2 = fig.add_subplot(gs1[4,0])\n",
    "    ax2=plot_mean_breaks_firing_rate(data,shank,cluster,trialType = \"good\")\n",
    "    ax2.set_xlim(xlimvalue)\n",
    "    ax2.set_title(\"\")\n",
    "    \n",
    "    gs1.tight_layout(fig, rect=[0, 0, 0.33, 1])\n",
    "    \n",
    "    ##autocorrelagrams\n",
    "    gs2 = gridspec.GridSpec(1, 3)\n",
    "    \n",
    "    ax3 = fig.add_subplot(gs2[0,0])\n",
    "    plot_autocorrelogram(data,shank,cluster,1,30)\n",
    "    ax3.set_title(\"\")\n",
    "    \n",
    "    ax4 = fig.add_subplot(gs2[0,1])\n",
    "    plot_autocorrelogram_period(data, shank, cluster, 5, 1000)\n",
    "    \n",
    "    ax5= fig.add_subplot(gs2[0,2])\n",
    "    plot_autocorrelogram_period(data, shank, cluster, 5, 1000, immobility=True)\n",
    "    \n",
    "    gs2.tight_layout(fig, rect=[0.33, 0.85, 1, 1],h_pad=0.33)\n",
    "    \n",
    "    ## waveform\n",
    "    gs4 = gridspec.GridSpec(1, 2)\n",
    "    ax9=fig.add_subplot(gs4[0,0])\n",
    "    ax10=fig.add_subplot(gs4[0,1])\n",
    "    wave,k= plot_mean_waveform(data, shank, cluster,redo=False,plotAllSpikes=True,ax1=ax9,ax2=ax10)\n",
    "    \n",
    "    gs4.tight_layout(fig,rect=[0.45, 0.7, 0.9, 0.85],h_pad=0.33)\n",
    "    \n",
    "    \n",
    "    ###mean firing rate during run and immo and correlation plots\n",
    "    gs3 = gridspec.GridSpec(4, 2)\n",
    "    \n",
    "    #2nd raw 2nd plot : firing rate vs good run\n",
    "    ax6 = fig.add_subplot(gs3[0,0])\n",
    "    ax6=plot_normalized_running_periods_firing_rate(data,shank,cluster,binSize=binSize,runType=\"trial good run\",ax=ax6)[0]\n",
    "    ylimvalue=ax6.get_ylim()\n",
    "    \n",
    "    \n",
    "    # 2nd raw 3nd plot : firing rate vs non rewarded runs\n",
    "    ax7 = fig.add_subplot(gs3[0,1])\n",
    "    ax7=plot_normalized_running_periods_firing_rate(data,shank,cluster,binSize=binSize,runType=\"unrewarded\",ax=ax7)[0]\n",
    "    if ylimvalue>ax7.get_ylim():\n",
    "        ax7.set_ylim(ylimvalue)\n",
    "    else:\n",
    "        ylimvalue=ax7.get_ylim()\n",
    "        ax6.set_ylim(ylimvalue)\n",
    "   \n",
    "    \n",
    "    # 2n line left plot : firing rate vs all immo + shuffling + modulated bin\n",
    "    ax7 = fig.add_subplot(gs3[1,0])\n",
    "    ax7,meanFiring,nSideBin,spikeHist,allSpeed,allDuration = plot_normalized_immobility_periods_firing_rate(data,shank,cluster,binSize=binSize,SideLength=SideLength,ax=ax7)\n",
    "    percentileShuffle,allMeanShuffledFiring = plot_shuffled_immobility_firing_rate(data,shank,cluster,binSize=binSize,SideLength=SideLength,ax=ax7)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 2n line right plot : firing rate vs all run + shuffling + modulated bin\n",
    "    ax8 = fig.add_subplot(gs3[1,1])\n",
    "    \n",
    "    \n",
    "    ModulationResults,InsideRunIndexes,spikeHist,allSpeed,allDuration,nSideBin,plotaxinfo=GetModulatedPortionsDuringRun(data,shank,cluster,runType=\"all\",printoutput=printoutput,ax=ax8)\n",
    "    \n",
    "    \n",
    "    #return ModulationResults\n",
    "    if (ModulationResults[\"BiggestModulationSign\"]==\"not modulated\") or (ModulationResults[\"BiggestModulationSign\"]==\"not enough spikes\") or (len(ModulationResults[\"BiggestModulationIndexes\"])<3):\n",
    "        print(\"no correlation possible\")\n",
    "        gs3.tight_layout(fig, rect=[0.35, 0, 1, 0.7],h_pad=0.33)\n",
    "        \n",
    "        return\n",
    "    # Generate Correlations Plot\n",
    "    ModulatedBins=ModulationResults[\"BiggestModulationIndexes\"]\n",
    "    FRateDuringModulatedEpoch=[]\n",
    "    RSpeedDuringModulatedEpoch=[]\n",
    "    RAccelDuringModulatedEpoch=[]\n",
    "    DistanceRun=[]\n",
    "    for x,r in enumerate(allDuration):\n",
    "        if sum(spikeHist[x][ModulatedBins])>=0:\n",
    "\n",
    "            #frate trial by trial\n",
    "            FRateDuringModulatedEpoch.append(np.nanmean(spikeHist[x][ModulatedBins]))\n",
    "\n",
    "            #running speed\n",
    "            RSpeedDuringModulatedEpoch.append(np.nanmean(allSpeed[x][ModulatedBins]))\n",
    "\n",
    "            #acceleration\n",
    "            BinDuration=r/(len(allSpeed[x])-2*nSideBin)\n",
    "            SpeedInModulatedBins=allSpeed[x][ModulatedBins]\n",
    "            Acceleration=(SpeedInModulatedBins[-1]-SpeedInModulatedBins[0])/((sum(ModulatedBins)-1)*BinDuration) \n",
    "            RAccelDuringModulatedEpoch.append(Acceleration)\n",
    "\n",
    "            #run distance\n",
    "\n",
    "            TotalDistanceRun=sum(allSpeed[x][int(nSideBin):-int(nSideBin)]*BinDuration)\n",
    "            DistanceRun.append(TotalDistanceRun)\n",
    "\n",
    "\n",
    "\n",
    "    AllKinematicParameters=[RSpeedDuringModulatedEpoch,RAccelDuringModulatedEpoch,DistanceRun,allDuration]\n",
    "    AllYLabels=[\"Running speed (cm/s)\",\"Running accel $\\mathregular{(cm/s^2)}$\",\"Run distance (cm)\",\"Run duration (s)\"]\n",
    "\n",
    "\n",
    "    subplotcoordinates=[[2,0],[2,1],[3,0],[3,1]]\n",
    "\n",
    "    x=FRateDuringModulatedEpoch\n",
    "\n",
    "    RValues=[]\n",
    "\n",
    "    for count,values in enumerate(AllKinematicParameters):\n",
    "        ax = fig.add_subplot(gs3[subplotcoordinates[count][0],subplotcoordinates[count][1]])\n",
    "\n",
    "        y=AllKinematicParameters[count]\n",
    "        fit = np.polyfit(x,y,1)\n",
    "        fit_fn = np.poly1d(fit) \n",
    "        #plt.plot(FRateDuringModulatedEpoch,RSpeedDuringModulatedEpoch,'o')\n",
    "\n",
    "        ax.plot(x,y, 'ro', x, fit_fn(x), '-k',linewidth=2)\n",
    "        #ax.ylim(7,30)\n",
    "        ax.set_xlabel(\"Firing rate (Hz)\",fontsize=20,weight=\"bold\")\n",
    "        ax.set_ylabel(AllYLabels[count],fontsize=20,weight=\"bold\")\n",
    "\n",
    "        MinMaxForPlot=np.percentile(AllKinematicParameters[count],[2,98])\n",
    "        ax.set_ylim(MinMaxForPlot)\n",
    "        SpearManResults=stats.spearmanr(x,y)\n",
    "        rvalue=str(round(SpearManResults[0],2));\n",
    "        if SpearManResults[1]<0.0001:\n",
    "            pvalue=\"p<0.0001\"\n",
    "        else:\n",
    "            pvalue=\"p=\"+ str(round(SpearManResults[1],4))\n",
    "\n",
    "        title=\"r=%s, %s\"%(rvalue,pvalue)\n",
    "        ax.set_title(title,fontsize=20,weight=\"bold\")\n",
    "        ax.tick_params(axis='both',which='major',labelsize=20,width=2) \n",
    "        RValues.append([SpearManResults[0],SpearManResults[1]])\n",
    "\n",
    "    \n",
    "    \n",
    "    gs3.tight_layout(fig, rect=[0.35, 0, 1, 0.7],h_pad=0.33)\n",
    "        \n",
    "    return #ModulationDuringRun\n",
    "    \n",
    "    #x2Bis=plot_mean_breaks_firing_rate(data,shank,cluster,ax=ax2,align=\"trial end\",removeBadTrials=True,minTime=-20)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():    \n",
    "    if data.hasBehavior and data.hasSpike:\n",
    "        # below you can change shank and clu\n",
    "        ##############################\n",
    "\n",
    "        \n",
    "        ##############################\n",
    "        ModulationDuringRun=cluster_ratevskinematicplot(data,shank,cluster,group=\"Good\",printoutput=True);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
