{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import phy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from phy import session\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading raw spike data\n",
    "\n",
    "#### Paths: same as behavior\n",
    "\n",
    "#### Acquisition parameters (integers or floats):\n",
    "  - **amplification**: voltage amplification\n",
    "  - **nbits**: floats in .dat file are stored on nbits\n",
    "  - **nChannels**: total number of channels, needed to read .dat file\n",
    "  - **offset**: always 0\n",
    "  - **spikeSamplingRate**: number of acquisition per seconds for the spikes   \n",
    "    other names: SampleRate, sample_rate   \n",
    "  - **voltageRange**\n",
    "  \n",
    "#### Other\n",
    "  - **channelGroupList**: dictionary of channel groups (shank), ex: {1: [0,1,2,3,4,...], 2: [8,9,..], ..}\n",
    "  - **clusterGroup**: dictionary {channel_group: cluster_group}  \n",
    "       with cluster_group a dictionnary {\"Good\":[list of clu],\"Noise\":[List of clu],\"MUA\":...,\"Unsorted\":...}   \n",
    "  \n",
    "#### Nested dictionary { channelGroup: { clu : 1D numpy array} }  \n",
    "    For each channelGroup, a dictionary  \n",
    "    In this dictionary, for each clu, the list of the spike times or spike sample\n",
    "    \n",
    "   - **spikeSample**: sample of the spikes\n",
    "   - **spikeTime**: sample/spikeSamplingRate \n",
    "   - **spikeIndex**: index in the list, usefull to get waveform In .kwx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BaseRawSpikeData:\n",
    "    def __init__(self,rootFolder,rat,experiment,parameters={},saveAsPickle=True):             \n",
    "        #clean name of folders (remove unnecessary slash or backslash)\n",
    "        rootFolder=os.sep+rootFolder.strip(os.sep)\n",
    "        rat=rat.strip(os.sep)\n",
    "        experiment=experiment.strip(os.sep)\n",
    "        \n",
    "        #paths\n",
    "        self.sessionPath=os.path.join(rootFolder,rat,\"Experiments\",experiment)\n",
    "        self.fullPath=os.path.join(rootFolder,rat,\"Experiments\",experiment,experiment)\n",
    "        \n",
    "        #parameters dict\n",
    "        self.parameters=parameters #dictionnary with \n",
    "        acquisitionParameters=self.read_acquisitionSystem_parameters()\n",
    "        acquisitionParameters.update(self.parameters)\n",
    "        \n",
    "        #integers or floats\n",
    "        self.amplification=None\n",
    "        self.nBits=None\n",
    "        self.nChannels=None\n",
    "        self.offset=None\n",
    "        self.spikeSamplingRate=None\n",
    "        self.voltageRange=None\n",
    "        try :\n",
    "            self.amplification=acquisitionParameters[\"amplification\"]\n",
    "            self.nBits=int(acquisitionParameters[\"nBits\"])\n",
    "            self.nChannels=int(acquisitionParameters[\"nChannels\"])\n",
    "            self.offset=acquisitionParameters[\"offset\"]\n",
    "            self.spikeSamplingRate=acquisitionParameters[\"spikeSamplingRate\"]\n",
    "            self.voltageRange=acquisitionParameters[\"voltageRange\"]\n",
    "        except KeyError:\n",
    "            pass\n",
    "        \n",
    "        #spikes dict{ key(channelGroup): {key(clu): spike time or sample} }\n",
    "        self.spikeSample, self.clusterGroup=self.read_spikes_sample_and_clu()       \n",
    "        self.channelGroupList = self.read_probe()\n",
    "\n",
    "        sr=float(self.spikeSamplingRate)\n",
    "        self.spikeTime={}\n",
    "        for shank in self.spikeSample:\n",
    "            self.spikeTime[shank]={clu:self.spikeSample[shank][clu]/sr for clu in self.spikeSample[shank]}\n",
    "        \n",
    "        if saveAsPickle:\n",
    "            self.save_as_pickle()\n",
    "        \n",
    "    def get_dict(self):\n",
    "        return self.__dict__\n",
    "    \n",
    "    def save_as_pickle(self,folder=\"Analysis\",name=\"rawspikedata.p\"):\n",
    "        import pickle\n",
    "        folderPath=os.path.join(self.sessionPath,folder)\n",
    "        if not os.path.exists(folderPath):\n",
    "            os.mkdir(folderPath)\n",
    "        filePath=os.path.join(folderPath,name)\n",
    "        pickle.dump(self.__dict__, open(filePath, \"wb\" ))\n",
    "\n",
    "    def read_acquisitionSystem_parameters(self):\n",
    "        defaultParam={\n",
    "            \"amplification\":1000,\n",
    "            \"nBits\":16,\n",
    "            \"offset\":0,\n",
    "            \"spikeSamplingRate\":20000,\n",
    "            \"voltageRange\":10\n",
    "        }\n",
    "        return defaultParam\n",
    "    \n",
    "    def read_spikes_time_and_sample(self):        \n",
    "        raise NotImplementedError(\"reading spikes is not implemented in base class\")\n",
    "        \n",
    "    def read_probe(self):\n",
    "        \"\"\"\n",
    "        read channels number for each group (shank)\n",
    "        \"\"\"\n",
    "        ch = int(self.nChannels / len(self.spikeSample))\n",
    "        return {key: list(range(index*ch, index*ch +ch)) for index, key in enumerate(sorted(self.spikeSample))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pavel spike data (Kluster, clu,res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Kluster_RawSpikeData(BaseRawSpikeData):\n",
    "    '''\n",
    "    Data obtained with Kluster (.clu, .res)\n",
    "    Only one group: \"good\" clusters. Every other cluster (noise, mua) is merge into cluster 0.\n",
    "    '''\n",
    "    def read_acquisitionSystem_parameters(self):\n",
    "        with open(self.fullPath+'.xml', \"rb\") as f:    # notice the \"rb\" mode\n",
    "            d = xmltodict.parse(f, xml_attribs=True)\n",
    "        acquisitionParam = d['parameters']['acquisitionSystem']\n",
    "        acquisitionParam={key: float(acquisitionParam[key]) for key in acquisitionParam}\n",
    "        acquisitionParam[\"spikeSamplingRate\"]=acquisitionParam[\"samplingRate\"]\n",
    "        #number of shank (\"channel group\")\n",
    "        \n",
    "        g = d['parameters']['spikeDetection']['channelGroups']['group']\n",
    "        if not isinstance(g, list):\n",
    "            g = [g]\n",
    "        self.nChGroup= len(g)\n",
    "        self.channelGroupList = {index+1: [int(s) for s in shank['channels']['channel']] \n",
    "                                 for index, shank in enumerate(g)}\n",
    "        \n",
    "        return acquisitionParam\n",
    "    \n",
    "    def read_probe(self):\n",
    "        return self.channelGroupList\n",
    "    \n",
    "    def read_spikes_sample_and_clu(self):\n",
    "        '''  \n",
    "        A function to load from the .clu and .res files of all the shank\n",
    "        Removes the spikes from 0 and 1 clusters (noise). \n",
    "        '''\n",
    "        spikeSample,clusterGroup={},{}\n",
    "\n",
    "        for chGroup in range(1,self.nChGroup+1):\n",
    "            #load clu and res\n",
    "            if not os.path.exists(self.fullPath+'.clu.'+str(chGroup)):\n",
    "                continue  \n",
    "            clu = pd.read_csv(self.fullPath+'.clu.'+str(chGroup)).values #header=first line = number of clusters\n",
    "            res = pd.read_csv(self.fullPath+'.res.'+str(chGroup),header=None).values\n",
    "\n",
    "            #get all cluster id\n",
    "            clusterIDList=list(np.unique(clu))\n",
    "            #remove 0 and 1\n",
    "            for value in [0,1]:\n",
    "                if value in clusterIDList:\n",
    "                    clusterIDList.remove(value)\n",
    "\n",
    "            #load in dictionary\n",
    "            spikeSample[chGroup]={}\n",
    "            clusterGroup[chGroup]={\"Good\":clusterIDList}\n",
    "            \n",
    "            for clusterID in clusterIDList:\n",
    "                spikeSample[chGroup][clusterID]=res[clu==clusterID]             \n",
    "        return spikeSample,clusterGroup\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run only if inside this notebook (do not execute if \"%run this_notebook\")\n",
    "if \"__file__\" not in dir():\n",
    "    #root,animal,a,experiment=\"data/Rat034/Experiments/Rat034_2015_03_04_10_04\".split(\"/\")\n",
    "    ROOT=\"/data\"\n",
    "    ANIMAL=\"MOU035\"\n",
    "    SESSION=\"MOU035_2014_12_11_11_08\"\n",
    "\n",
    "    data=Kluster_RawSpikeData(ROOT,ANIMAL,SESSION)\n",
    "    print(data.channelGroupList)\n",
    "    print(data.nChannels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teresa spike data (klusta, .kwik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Klusta_RawSpikeData(BaseRawSpikeData):\n",
    "    '''\n",
    "    Data obtained with Klusta Suite\n",
    "    Can be read with phy\n",
    "    4 default group: noise(0), MUA(1), Good(2) and Unsorted(3)\n",
    "       User could have created new groups (>=4), but I didn't find the new names in the kwik file. \n",
    "       So new groups are merged with Good\n",
    "    '''\n",
    "    def read_acquisitionSystem_parameters(self):\n",
    "        import phy \n",
    "        from phy.session import Session\n",
    "        self.session= Session(self.fullPath+\".kwik\")\n",
    "        param={\n",
    "            \"amplification\":1000,\n",
    "            \"nBits\":self.session.model.metadata[\"nbits\"],\n",
    "            \"offset\":0,\n",
    "            \"spikeSamplingRate\":self.session.model.sample_rate,\n",
    "            \"voltageRange\":self.session.model.metadata[\"voltage_gain\"],\n",
    "            \"nChannels\":self.session.model.metadata[\"nchannels\"]\n",
    "        }\n",
    "        return param\n",
    "    \n",
    "    def read_probe(self):        \n",
    "        del self.session\n",
    "        return self.channelGroupList\n",
    "           \n",
    "    def read_spikes_sample_and_clu(self):\n",
    "        #chgroup is channel group (shank)\n",
    "        #cluGroup is the group of the cluster (noise, mua, good...)\n",
    "        spikeSample,clusterGroup={},{}\n",
    "        spikeIndex={}\n",
    "        self.channelGroupList = {}\n",
    "        first = True\n",
    "        for chgroup in self.session.model.channel_groups:\n",
    "            if first:\n",
    "                first = False\n",
    "            else:\n",
    "                self.session.change_channel_group(chgroup)\n",
    "            spikeSample[chgroup]={}\n",
    "            clusterGroup[chgroup]={}\n",
    "            spikeIndex[chgroup]={}\n",
    "            self.channelGroupList[chgroup] = list(self.session.model.channel_order)\n",
    "            #default cluster group names (0: 'Noise', 1: 'MUA', 2: 'Good', 3: 'Unsorted')\n",
    "            for cluGroupName in self.session.model.default_cluster_groups.values():\n",
    "                clusterGroup[chgroup][cluGroupName]=[]\n",
    "            \n",
    "            #for every cluster in the channel_group\n",
    "            for clu in self.session.model.cluster_ids:\n",
    "                if clu == 0:\n",
    "                    continue\n",
    "                spikeSample[chgroup][clu]=self.session.model.spike_samples[self.session.model.spike_clusters==clu]  \n",
    "                spikeIndex[chgroup][clu]=self.session.model.spike_ids[self.session.model.spike_clusters==clu]\n",
    "                cluGroupID=self.session.model.cluster_metadata.group(clu)\n",
    "                if isinstance(cluGroupID, np.ndarray):\n",
    "                    print(\"Warning- cluster group of cluster %s is an array (%s), taking first value\"%(clu,cluGroupID))\n",
    "                    cluGroupID=cluGroupID[0]\n",
    "                if isinstance(cluGroupID, bytes):\n",
    "                    print(\"Warning- cluster group of cluster %s is a bytes, putting it in 'unsorted'\"%(clu))\n",
    "                    cluGroupID = 3\n",
    "                # if a group was created (ID>3), put it in \"Good\"(2)\n",
    "                if cluGroupID>3:\n",
    "                    cluGroupID=2\n",
    "                cluGroupName=self.session.model.default_cluster_groups[cluGroupID]        \n",
    "                clusterGroup[chgroup][cluGroupName].append(clu)\n",
    "\n",
    "        self.spikeIndex=spikeIndex\n",
    "        return spikeSample,clusterGroup\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#run only if inside this notebook (do not execute if \"%run this_notebook\")\n",
    "if \"__file__\" not in dir():\n",
    "    #root,animal,a,experiment=\"data/Rat034/Experiments/Rat034_2015_03_04_10_04\".split(\"/\")\n",
    "    root=\"data\"\n",
    "    animal=\"MOU102\"\n",
    "    experiment=\"MOU102_2016_01_21_11_16/\"\n",
    "\n",
    "    data=Klusta_RawSpikeData(root,animal,experiment)\n",
    "    print(data.channelGroupList)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
