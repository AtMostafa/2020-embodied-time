{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook aims to convert Open Ephys output files (.raw.kwd) and folders to proper names and formats.\n",
    "# This notebook contains the following sections:\n",
    "\n",
    "## Rename Directories and files altogether\n",
    "## Convert .raw.kwd files and save .dat files\n",
    "## Save .prm files\n",
    "## Save .eeg files\n",
    "\n",
    "### Run all the cells one time then put the preferred values in the last cell and run it. It will call all the functions in the previous cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tables as tb\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from datetime import datetime,timedelta\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "\n",
    "def find_file(path, extension=['.raw.kwd']):\n",
    "    \"\"\"\n",
    "    This function finds all the file types specified by 'extension' (ex: *.dat) in the 'path' directory\n",
    "    and all its subdirectories and their sub-subdirectories etc., \n",
    "    and returns a list of all dat file paths\n",
    "    'extension' is a list of desired file extensions: ['.dat','.prm']\n",
    "    \"\"\"\n",
    "    if type(extension) is str:\n",
    "        extension=extension.split()   #turning extension into a list with a single element\n",
    "    return [os.path.join(walking[0],goodfile) for walking in list(os.walk(path)) \n",
    "         for goodfile in walking[2] for ext in extension if goodfile.endswith(ext)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming\n",
    "\n",
    "-Functions and classes required for renaming all the OpenEphys files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rename_folder(folderToRemove,newFolder):\n",
    "    '''\n",
    "    folderToRemove (=old), newFolder: full paths to folders\n",
    "    merge the folders if \"newFolder\" already exists\n",
    "    files are renamed (example: folderToRemove_1.txt becomes newFolder_1.txt)\n",
    "    '''\n",
    "    #if the new folder doesn't exist, create it\n",
    "    if not os.path.exists(newFolder):\n",
    "        os.mkdir(newFolder) \n",
    "    #move and rename the files \n",
    "    for f in os.listdir(folderToRemove):\n",
    "        oldpath=os.path.join(folderToRemove,f)\n",
    "        oldFolderName=os.path.basename(folderToRemove.rstrip(os.sep))\n",
    "        newFolderName=os.path.basename(newFolder.rstrip(os.sep))\n",
    "        f=f.replace(oldFolderName,newFolderName)\n",
    "        newpath=os.path.join(newFolder,f)\n",
    "        if os.path.exists(newpath):\n",
    "            os.remove(oldpath)\n",
    "        else:\n",
    "            os.rename(oldpath,newpath)\n",
    "    os.rmdir(folderToRemove)\n",
    "    \n",
    "def get_folders_matching_format(rootFolder,globFormat):\n",
    "    '''\n",
    "    rootFolder: full path where to look for session folders (ex: \"/data/Rat034/Experiments\")\n",
    "    '''\n",
    "    fullFormat=os.path.join(rootFolder,globFormat)\n",
    "    return [os.path.basename(f) for f in glob.glob(fullFormat)]\n",
    "\n",
    "def get_regular_name(wrongName,wrongTimeFormat,regularTimeFormat):\n",
    "    '''\n",
    "    from a session name in wrong format, return the regular format\n",
    "    '''\n",
    "    date=datetime.strptime(wrongName,wrongTimeFormat)\n",
    "    return date.strftime(regularTimeFormat)\n",
    "\n",
    "def get_date(name,timeFormat):\n",
    "    '''\n",
    "    return a datetime object from a session name and a format\n",
    "    '''\n",
    "    date=datetime.strptime(name,timeFormat)\n",
    "    return date\n",
    "\n",
    "def get_regular_name_from_date(date,regularTimeFormat):\n",
    "    '''\n",
    "    return a name given a date and a format\n",
    "    '''\n",
    "    return date.strftime(regularTimeFormat)\n",
    "\n",
    "def rename_all_in_rootFolder(rootFolder,wrongGlobFormat,wrongTimeFormat,regularGlobFormat,\n",
    "                             regularTimeFormat,minuteDelay=2,verbose=False):\n",
    "    '''\n",
    "    rootFolder: full path where to look for session folders (ex: \"/data/Rat034/Experiments\")\n",
    "    wrongGlobFormat: glob pattern to search for the wrong session names\n",
    "    wrongTimeFormat: datetime pattern to read date from wrong session names\n",
    "    regularGlobFormat, regularTimeFormat: glob and datetime patterns for the right session names\n",
    "    minuteDelay: if two folders have the same date (give or take minuteDelay), merge them\n",
    "    '''\n",
    "    delay=timedelta(minutes=minuteDelay)\n",
    "\n",
    "    #get names and date of the regular folders\n",
    "    regularFolders=get_folders_matching_format(rootFolder,regularGlobFormat)\n",
    "    allRegularDates=[get_date(name,regularTimeFormat) for name in regularFolders]\n",
    "    \n",
    "    #get names of wrong folders\n",
    "    fList=get_folders_matching_format(rootFolder,wrongGlobFormat)\n",
    "    for f in fList:\n",
    "        merge=False\n",
    "        #check if there is a regular folder around the same date\n",
    "        date=get_date(f,wrongTimeFormat)\n",
    "        for otherDate in allRegularDates:\n",
    "            if abs(date-otherDate)<delay:\n",
    "                date=otherDate\n",
    "                merge=True\n",
    "                break\n",
    "        #new name\n",
    "        newFolder=get_regular_name_from_date(date,regularTimeFormat)\n",
    "        #rename/merge\n",
    "        newPath=os.path.join(rootFolder,newFolder)\n",
    "        oldPath=os.path.join(rootFolder,f)\n",
    "        rename_folder(oldPath,newPath)\n",
    "        if verbose:\n",
    "            if merge:\n",
    "                print(\"Merged %s into %s\"%(f,newFolder))\n",
    "            else:\n",
    "                print(\"Renamed %s in %s\"%(f,newFolder))\n",
    "                \n",
    "def rename_files_to_match_folder(folderPath,extensionList):\n",
    "    '''\n",
    "    folderPath= \"/data/Rat034/Experiments/Rat034_2015_etc\"\n",
    "    extensionList= [\".dat\", \".prm\", \".kwik\"]\n",
    "    --> Renames \"someFile.dat\" into \"Rat024_2015_etc.dat\"\n",
    "    '''\n",
    "    folderName=os.path.basename(folderPath.rstrip(os.sep))\n",
    "    extensionList=[ext if ext.startswith(\".\") else \".\"+ext for ext in extensionList]\n",
    "    for f in os.listdir(folderPath):\n",
    "        for ext in extensionList:\n",
    "            if f.endswith(ext):\n",
    "                oldPath=os.path.join(folderPath,f)\n",
    "                newName=folderName+ext\n",
    "                newPath=os.path.join(folderPath,newName)\n",
    "                os.rename(oldPath,newPath)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to rename the files and folders of OpenEphys as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rename_batch(root,animalList,verbose,minuteDelay,extensionList):\n",
    "    \n",
    "    WRONG_FORMATS={\n",
    "    \"openEphys\":(\n",
    "        \"animal_20{0}-{0}-{0}_{0}-{0}-{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2022-02-21_11-28-17'\n",
    "        \"animal_%Y-%m-%d_%H-%M-%S\"\n",
    "    ),\n",
    "    \"seconds\":(\n",
    "        \"animal_20{0}_{0}_{0}_{0}_{0}_{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2013_12_26_20_47_54'\n",
    "        \"animal_%Y_%m_%d_%H_%M_%S\"\n",
    "    ),\n",
    "    \"tiret\":(\n",
    "        \"animal_20{0}_{0}_{0}-{0}_{0}\".format(\"[00-99]?\"),        #ex: 'Rat001_2039_10_15-06_27'\n",
    "        \"animal_%Y_%m_%d-%H_%M\"\n",
    "    ),\n",
    "    \"tiretSeconds\":(\n",
    "        \"animal_20{0}_{0}_{0}-{0}_{0}_{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2033_06_23-21_24_46'\n",
    "        \"animal_%Y_%m_%d-%H_%M_%S\"\n",
    "    ),\n",
    "    \"openEphys2\":(\n",
    "        \"20{0}-{0}-{0}_{0}-{0}-{0}\".format(\"[00-99]?\"),           #ex: '2015-02-15_11-59-48'\n",
    "        \"%Y-%m-%d_%H-%M-%S\"\n",
    "    ),\n",
    "    }\n",
    "    # The right format \n",
    "    REGULAR_FORMAT=(\"animal_20{0}_{0}_{0}_{0}_{0}\".format(\"?[00-99]\"),  #ex: 'Rat001_2039_10_15_06_27'\n",
    "                \"animal_%Y_%m_%d_%H_%M\")\n",
    "      \n",
    "    for animal in animalList:\n",
    "        rawFileList=find_file(os.path.join(root,animal),extensionList)\n",
    "        rootFolderList=[os.path.split(os.path.split(rawFile)[0])[0] for rawFile in rawFileList]\n",
    "        #rootFolderList keeps paths to the directories within 'root' which contain session folders\n",
    "        rootFolderList=list(set(rootFolderList))#to keep unique members\n",
    "        \n",
    "        for rootFolder in rootFolderList:\n",
    "            print(\"animal %s: %s\"%(animal,rootFolder))\n",
    "    \n",
    "            #RENAME\n",
    "            regularGlob=REGULAR_FORMAT[0].replace(\"animal\",animal)\n",
    "            regularTime=REGULAR_FORMAT[1].replace(\"animal\",animal)\n",
    "            for name in WRONG_FORMATS:\n",
    "                globFormat=WRONG_FORMATS[name][0].replace(\"animal\",animal)\n",
    "                timeFormat=WRONG_FORMATS[name][1].replace(\"animal\",animal)       \n",
    "                #print(get_folders_matching_format(rootFolder,globFormat))\n",
    "                rename_all_in_rootFolder(rootFolder,globFormat,timeFormat,regularGlob,regularTime,\n",
    "                                         minuteDelay=minuteDelay,verbose=verbose)\n",
    "    \n",
    "            #CONVERT\n",
    "            #for every session folder of the animal\n",
    "            for folder in os.listdir(rootFolder):\n",
    "                if not folder.startswith(animal):\n",
    "                    continue\n",
    "                print(\"*\"+folder)\n",
    "                path=os.path.join(rootFolder,folder)\n",
    "                #rename files\n",
    "                rename_files_to_match_folder(path,extensionList)\n",
    "\n",
    "                for f in os.listdir(path):\n",
    "                    fpath=os.path.join(path,f)\n",
    "                    #convert raw.kwd\n",
    "                    if f==\"settings.xml\":\n",
    "                        newPath=os.path.join(path,\"settingsOpenEphys.xml\")\n",
    "                        os.rename(fpath,newPath)\n",
    "    \n",
    "            print(\"\\nRenaming done\")\n",
    "            print(\"--------\")\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    root=\"/data/SWI002/22/\"\n",
    "    #Animal where to run the script\n",
    "    animalList=[\"SWI002\"]\n",
    "\n",
    "    #Whether to print \"X renamed in Y\"\n",
    "    verbose=True  \n",
    "\n",
    "    #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "    # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "    minuteDelay=0\n",
    "\n",
    "    #File to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "    #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "    extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwx\",\".kwe\",\".eeg\"]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    rename_batch(root,animalList,verbose,minuteDelay,extensionList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5df7411-916f-4b42-85e5-c42b806ff1dd"
    }
   },
   "source": [
    "## Converting\n",
    "-Required functions and classes to open .raw.kwd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "nbpresent": {
     "id": "b23eca88-6789-483b-9d4b-538e8bed2682"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------- chunck class from Klusta\n",
    "# reads data chunk by chunk (instead of all at once, as the files are big)\n",
    "class Chunk(object):\n",
    "    def __init__(self, data=None, nsamples=None, nchannels=None,\n",
    "                 bounds=None, dtype=None, recording=0, nrecordings=1):\n",
    "        self._data = data\n",
    "        if nsamples is None and nchannels is None:\n",
    "            nsamples, nchannels = data.shape\n",
    "        self.nsamples = nsamples\n",
    "        self.nchannels = nchannels\n",
    "        self.dtype = dtype\n",
    "        self.recording = recording\n",
    "        self.nrecordings = nrecordings\n",
    "        self.s_start, self.s_end, self.keep_start, self.keep_end = bounds\n",
    "        self.window_full = self.s_start, self.s_end\n",
    "        self.window_keep = self.keep_start, self.keep_end      \n",
    "    @property\n",
    "    def data_chunk_full(self):\n",
    "        chunk = self._data[self.s_start:self.s_end,:]\n",
    "        return convert_dtype(chunk, self.dtype)   \n",
    "    @property\n",
    "    def data_chunk_keep(self):\n",
    "        chunk =  self._data[self.keep_start:self.keep_end,:]\n",
    "        return convert_dtype(chunk, self.dtype)\n",
    "\n",
    "def chunk_bounds(nsamples, chunk_size, overlap=0):\n",
    "    s_start = 0\n",
    "    s_end = chunk_size\n",
    "    keep_start = s_start\n",
    "    keep_end = s_end - overlap // 2\n",
    "    yield s_start, s_end, keep_start, keep_end    \n",
    "    while s_end - overlap + chunk_size < nsamples:\n",
    "        s_start = s_end - overlap\n",
    "        s_end = s_start + chunk_size\n",
    "        keep_start = keep_end\n",
    "        keep_end = s_end - overlap // 2\n",
    "        if s_start < s_end:\n",
    "            yield s_start, s_end, keep_start, keep_end        \n",
    "    s_start = s_end - overlap\n",
    "    s_end = nsamples\n",
    "    keep_start = keep_end\n",
    "    keep_end = s_end\n",
    "    if s_start < s_end:\n",
    "        yield s_start, s_end, keep_start, keep_end\n",
    "\n",
    "def convert_dtype(data, dtype=None, factor=None):\n",
    "    if not dtype:\n",
    "        return data\n",
    "    if data.shape[0] == 0:\n",
    "        return data.astype(dtype)\n",
    "    dtype_old = data.dtype\n",
    "    if dtype_old == dtype:\n",
    "        return data\n",
    "    key = (_get_dtype(dtype_old), _get_dtype(dtype))\n",
    "    factor = factor or _dtype_factors.get(key, 1)\n",
    "    if dtype_old in (np.float32, np.float64):\n",
    "        factor = factor/np.abs(data).max()\n",
    "    if factor != 1:\n",
    "        return (data * factor).astype(dtype)\n",
    "    else:\n",
    "        return data.astype(dtype) \n",
    "#------------------------------------------------------------------------------ open a file, from kwiklib \n",
    "def open_file(path,mode=\"r\"):\n",
    "    try:\n",
    "        f = tb.open_file(path, mode)\n",
    "        return f\n",
    "    except IOError as e:\n",
    "        warn(\"IOError: \" + str(e.message))\n",
    "        return\n",
    "# ------------------------------------------------------------------- KWD and .dat data reader from Klusta\n",
    "class BaseRawDataReader(object):\n",
    "    def __init__(self, dtype_to=np.int16):\n",
    "        self.dtype_to = dtype_to\n",
    "    def next_recording(self):\n",
    "        for self.recording in range(self.nrecordings):\n",
    "            yield self.recording, self.get_recording_data(self.recording)    \n",
    "    def get_recording_data(self, recording):\n",
    "        raise NotImplementedError()    \n",
    "    def chunks(self, chunk_size=None, chunk_overlap=0):\n",
    "        for recording, data in self.next_recording():\n",
    "            assert chunk_size is not None, \"You need to specify a chunk size.\"\"\"\n",
    "            for bounds in chunk_bounds(data.shape[0], chunk_size=chunk_size, overlap=chunk_overlap):\n",
    "                yield Chunk(data, bounds=bounds, dtype=self.dtype_to, recording=recording, nrecordings=self.nrecordings)\n",
    "class KwdRawDataReader(BaseRawDataReader):\n",
    "    def __init__(self, filename, dtype_to=np.int16):\n",
    "        self.kwd = open_file(filename, 'r')\n",
    "        self.nrecordings = self.kwd.root.recordings._v_nchildren\n",
    "        super(KwdRawDataReader, self).__init__(dtype_to=dtype_to)        \n",
    "    def get_recording_data(self, recording):\n",
    "        data = self.kwd.root.recordings._f_get_child(str(recording)).data\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to open the .raw.kwd file and save the .dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "``/home/david/Downloads/2016-11-22_09-52-54/test8/test8_2016_11_22_09_52/test8_2016_11_22_09_52.raw.kwd`` does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-36e7a8a16760>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0mconvert_kwd_to_dat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdeleteKWD\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputFile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-36e7a8a16760>\u001b[0m in \u001b[0;36mconvert_kwd_to_dat\u001b[1;34m(filename, outputname, overwrite)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.raw.kwd'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;31m#read number of channels in kwd file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mkwd\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\\nopenning %s\"\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mdataLen\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnchannels\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_node\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"/recordings/0/data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/miniconda/lib/python3.4/site-packages/tables/file.py\u001b[0m in \u001b[0;36mopen_file\u001b[1;34m(filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m     \u001b[1;31m# Finally, create the File instance, and return it\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mroot_uep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/david/miniconda/lib/python3.4/site-packages/tables/file.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, filename, mode, title, root_uep, filters, **kwargs)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m         \u001b[1;31m# Now, it is time to initialize the File extension\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 783\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_new\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    784\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m         \u001b[1;31m# Check filters and set PyTables format version for new files.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mtables/hdf5extension.pyx\u001b[0m in \u001b[0;36mtables.hdf5extension.File._g_new (tables/hdf5extension.c:4256)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m/home/david/miniconda/lib/python3.4/site-packages/tables/utils.py\u001b[0m in \u001b[0;36mcheck_file_access\u001b[1;34m(filename, mode)\u001b[0m\n\u001b[0;32m    154\u001b[0m         \u001b[1;31m# The file should be readable.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mF_OK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 156\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"``%s`` does not exist\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    157\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    158\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"``%s`` is not a regular file\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: ``/home/david/Downloads/2016-11-22_09-52-54/test8/test8_2016_11_22_09_52/test8_2016_11_22_09_52.raw.kwd`` does not exist"
     ]
    }
   ],
   "source": [
    "def convert_kwd_to_dat(filename,outputname=None,overwrite=False):\n",
    "    #output file name\n",
    "    if outputname is None:\n",
    "        outputname=filename[:-7]+\"dat\"\n",
    "    out=os.path.basename(outputname)\n",
    "    \n",
    "    if filename.endswith('.raw.kwd'):\n",
    "        #read number of channels in kwd file\n",
    "        with tb.open_file(filename,\"r\") as kwd:\n",
    "            print(\"\\n\\nopenning %s\"%(filename))\n",
    "            dataLen,nchannels=kwd.get_node(\"/recordings/0/data\").read().shape\n",
    "            sampling_rate=kwd.get_node(\"/recordings/0/\")._v_attrs.__getitem__('sample_rate')\n",
    "            \n",
    "            if dataLen<nchannels:\n",
    "                dataLen,nchannels=nchannels,dataLen\n",
    "            if dataLen<sampling_rate:    #not an important condition! Could be ignored!\n",
    "                raise ValueError('Data Length too short')\n",
    "            \n",
    "            print(\"number of channels: %s\"%(nchannels))\n",
    "            print(\"sampling rate: %sHz\"%(sampling_rate))\n",
    "            print(\"Converting...\")\n",
    "\n",
    "        if os.path.exists(outputname):\n",
    "            if overwrite:\n",
    "                print(\"\\n%s already exist, it will be overwritten\"%(out))\n",
    "            else:\n",
    "                print(\"\\n%s already exists, no conversion to do\"%(out))\n",
    "                return [nchannels, sampling_rate]\n",
    "        #instantiate reader\n",
    "        kwd_data=KwdRawDataReader(filename)\n",
    "    else:\n",
    "        print(\"Error: the raw data file doesn't end with '.raw.kwd'\")\n",
    "        return False\n",
    "\n",
    "    #create dat file, open write only in binary\n",
    "    try:\n",
    "        with open(outputname,'wb') as output:\n",
    "            chunk_size=sampling_rate\n",
    "            for chunk in kwd_data.chunks(chunk_size):\n",
    "                data=chunk.data_chunk_full\n",
    "                for i in range(len(data)):\n",
    "                    newFileByteArray=bytearray(data[i])\n",
    "                    output.write(newFileByteArray)\n",
    "        kwd_data.kwd.close()\n",
    "        print(\"wrote %s\"%(outputname))\n",
    "        print(\"done\")\n",
    "        return [nchannels, sampling_rate]\n",
    "    except:\n",
    "        print(\"Dat file writing failed!\")\n",
    "        return False\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    inputFile=\"/home/david/Downloads/2016-11-22_09-52-54/test8/test8_2016_11_22_09_52/test8_2016_11_22_09_52.raw.kwd\"\n",
    "    deleteKWD=False\n",
    "    overwrite=False\n",
    "\n",
    "    convert_kwd_to_dat(inputFile,overwrite=overwrite)\n",
    "\n",
    "    if deleteKWD and os.path.exists(inputFile) and False:\n",
    "        print(\"Delete %s\"%os.path.basename(inputFile))\n",
    "        os.remove(inputFile)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Save the .prm file with correct experiment name, sampling rate and channel number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRM file failed!\n"
     ]
    }
   ],
   "source": [
    "def save_prm_file(filepath,experiment,sampling_rate,n_channels,overwrite=True):\n",
    "    output_name=os.path.join(filepath,experiment+\".prm\")\n",
    "    if os.path.exists(output_name):\n",
    "        if overwrite:\n",
    "            print(\"%s already exists, it will be overwritten\"%(output_name))\n",
    "        else:\n",
    "            print(\"%s already exists, no conversion to do\"%(output_name))\n",
    "            return False\n",
    "    \n",
    "    PlaceHolders={\n",
    "        'exp_name':experiment,\n",
    "        'Fs':str(sampling_rate),\n",
    "        'Nch':str(n_channels),\n",
    "    }\n",
    "    prm_content=\"\"\"experiment_name = '%(exp_name)s'\n",
    "prb_file = experiment_name + '.prb'\n",
    "\n",
    "traces=dict(\n",
    "    raw_data_files  = [experiment_name + '.dat'],\n",
    "    voltage_gain    = 10,\n",
    "    nbits           = 16,\n",
    "    dtype           = 'int16',\n",
    "    sample_rate     = %(Fs)s,\n",
    "    n_channels      = %(Nch)s,\n",
    "    )\n",
    "\n",
    "nbits          = 16\n",
    "voltage_gain   = traces['voltage_gain']\n",
    "sample_rate    = traces['sample_rate']\n",
    "nchannels      = traces['n_channels']\n",
    "\n",
    "spikedetekt=dict(\n",
    "    #######################################################################\n",
    "    # SpikeDetekt parameters\n",
    "    #######################################################################\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Raw data filtering and saving\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Whether to save the .raw.kwd file if a non-HDF5 raw data format is used.\n",
    "    # This is needed to visualise the data in TraceView etc, and speeds up\n",
    "    # future runs of SpikeDetekt. If a .raw.kwd file is used as the input,\n",
    "    # it will never be overwritten.\n",
    "    save_raw = True,\n",
    "    # Whether to save the .high.kwd file with HPF data used for spike\n",
    "    # detection. This is processed using a Butterworth band-pass filter.\n",
    "    save_high = False,\n",
    "    # Bandpass filter low corner frequency\n",
    "    filter_low = 500.,\n",
    "    # Bandpass filter high corner frequency\n",
    "    filter_high = 0.95 * .5 * sample_rate,\n",
    "    # Order of Butterworth filter.\n",
    "    filter_butter_order = 3,\n",
    "    # Whether to save a .low.kwd file; this is processed using a Hamming\n",
    "    # window FIR filter, then subsampled 16x to save space when storing.\n",
    "    save_low = False,\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # Chunks\n",
    "    # ---------------------------------------------------------------------\n",
    "    # SpikeDetekt processes the raw data in chunks with small overlaps to\n",
    "    # catch spikes which would otherwise span two chunks. These options\n",
    "    # will change the default chunk size and overlap.\n",
    "    chunk_size = int(1. * sample_rate), # 1 second\n",
    "    chunk_overlap = int(.015 * sample_rate), # 15 ms\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Threshold setting for spike detection\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Change this to 'positive' to detect positive spikes.\n",
    "    detect_spikes = 'negative',\n",
    "    # SpikeDetekt takes a set of uniformly distributed chunks throughout\n",
    "    # the high-pass filtered data to estimate its standard deviation. These\n",
    "    # parameters select how many excerpts are used and how long each of them are.\n",
    "    nexcerpts = 50,\n",
    "    excerpt_size = int(1. * sample_rate), # 1 second\n",
    "    # This is then used to calculate a base threshold which is multiplied\n",
    "    # by the two parameters below for the two-threshold detection process.\n",
    "    threshold_strong_std_factor = 4.5,\n",
    "    threshold_weak_std_factor = 2.,\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Spike extraction\n",
    "    # ---------------------------------------------------------------------\n",
    "    # The number of samples to extract before and after the centre of the\n",
    "    # spike for waveforms. Then, waveforms_nsamples is calculated using the\n",
    "    # formula: waveforms_nsamples = extract_s_before + extract_s_after\n",
    "    extract_s_before = int(0.0008* sample_rate),\n",
    "    extract_s_after  = int(0.0008* sample_rate),\n",
    "\n",
    "    #---------------------------------------------------------------------\n",
    "    # Features\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Number of features (PCs) per channel.\n",
    "    nfeatures_per_channel = 3,\n",
    "    # The number of spikes used to determine the PCs\n",
    "    pca_nwaveforms_max = 10000,\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Advanced\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Number of samples to use in floodfill algorithm for spike detection\n",
    "    #connected_component_join_size = 1, # 1 sample\n",
    "    connected_component_join_size = int(.00005 * sample_rate), # 0.05ms\n",
    "    # Waveform alignment\n",
    "    weight_power = 2,\n",
    "    # Whether to make the features array contiguous\n",
    "    features_contiguous = True,\n",
    "    )\n",
    "\n",
    "#Mostafa: Don't know if the following section is working and what does it do?!\n",
    "##############################################################\n",
    "# KlustaKwik parameters (must be prefixed by KK_). Uncomment to override\n",
    "# the defaults, which can be shown by running 'klustakwik' with no options\n",
    "###############################################################\n",
    "\n",
    "# This causes KlustaKwik to perform clustering on a subset of spikes and\n",
    "# estimate the assignment of the other spikes. This causes a speedup in\n",
    "# computational time (by a rough factor of KK_Subset), though will not\n",
    "# significantly decrease RAM usage. For long runs where you are unsure of\n",
    "# the data quality, you can first use KK_Subset = 50 to check the\n",
    "# clustering quality before performing a Subset 1 (all spikes) run.\n",
    "KK_Subset = 1\n",
    "\n",
    "# The largest permitted number of clusters, so cluster splitting can produce\n",
    "# no more than n clusters. Note: This must be set higher than MaskStarts.\n",
    "KK_MaxPossibleClusters = 1000\n",
    "\n",
    "# Maximum number of iterations. ie. it won't try more than n iterations\n",
    "# from any starting point.\n",
    "KK_MaxIter = 10000\n",
    "\n",
    "# You can start with a chosen fixed number of clusters derived from the\n",
    "# mask vectors, set by KK_MaskStarts.\n",
    "KK_MaskStarts = 500\n",
    "\n",
    "# The number of iterations after which KlustaKwik first attempts to split\n",
    "# existing clusters. KlustaKwik then splits every SplitEvery iterations.\n",
    "KK_SplitFirst = 20\n",
    "\n",
    "# The number of iterations after which KlustaKwik attempts to split existing\n",
    "# clusters. When using masked initializations, to save time due to excessive\n",
    "# splitting, set SplitEvery to a large number, close to the number of distinct\n",
    "# masks or the number of chosen starting masks.\n",
    "KK_SplitEvery = 40\n",
    "\n",
    "# KlustaKwik uses penalties to reduce the number of clusters fit. The parameters PenaltyK and PenaltyKLogN are \n",
    "# given positive values. The higher the values, the fewer clusters you obtain. Higher penalties\n",
    "# discourage cluster splitting. PenaltyKLogN also increases penalty when there are more points. \n",
    "#-PenaltyK 0 -PenaltyKLogN 1 is the default, corresponding to the \"Bayesian Information Criterion\".\n",
    "# -PenaltyK 1 -PenaltyKLogN 0 corresponds to \"Akaike's Information Criterion\". This produces a larger number \n",
    "# of clusters, and is recommended if you are find that clusters corresponding to different neurons are incorrectly merged.\n",
    "KK_PenaltyK = 0.\n",
    "KK_PenaltyKLogN = 1.\n",
    "\n",
    "# Specifies a seed for the random number generator.\n",
    "KK_RandomSeed = 1\n",
    "\n",
    "# The number of unmasked spikes on a certain channel needed to unmask that\n",
    "# channel in the cluster. This prevents a single noisy spike, or coincident\n",
    "# noise on adjacent channels from slowing down computation time.\n",
    "KK_PointsForClusterMask = 10\n",
    "\n",
    "# Setting this saves a .temp.clu file every iteration. This slows the runtime\n",
    "# down reasonably significantly for small runs with many iterations, but allows\n",
    "# to recover where KlustaKwik left off; useful in case of large runs where you\n",
    "# are not confident that the run will be uninterrupted.\n",
    "KK_SaveTempCluEveryIter = 0\n",
    "\n",
    "# This is an integer N when, used in combination with the empty string\n",
    "# for UseFeatures above, omits the last N features. This should always\n",
    "# be used with KK_UseFeatures = \"\"\n",
    "KK_DropLastNFeatures = 0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Classic 'all channels unmasked always' mode | DO NOT uncomment\n",
    "# ---------------------------------------------------------------------\n",
    "# To use KlustaKwik in \"unmasked\" mode, set this to 0.\n",
    "# This disables the use of the new `masked Expectation-Maximization'\n",
    "# algorithm, and sets all the channels to be unmasked on all spikes.\n",
    "#KK_UseDistributional = 1\n",
    "\n",
    "# In classic mode, KlustaKwik starts from random cluster assignments,\n",
    "# running a new random start for every integer between MinClusters and\n",
    "# MaxClusters. For these values to take effect, MaskStarts must be set to 0.\n",
    "#KK_MinClusters = 100\n",
    "#KK_MaxClusters = 110\n",
    "\n",
    "# By default, this is an empty string, which means 'use all features'.\n",
    "# Or, you can you can specify a string with 1's for features you want to\n",
    "# use, and 0's for features you don't want to use. In classic mode,\n",
    "# you use this option to take out bad channels. In masked mode,\n",
    "# you should instead take bad channels out from the .PRB file.\n",
    "#KK_UseFeatures = \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Advanced\n",
    "# ---------------------------------------------------------------------\n",
    "# The algorithm will be started n times for each initial cluster count\n",
    "# between MinClusters and MaxClusters.\n",
    "KK_nStarts = 1\n",
    "\n",
    "# Saves means and covariance matrices. Stops computation at each iteration.\n",
    "# Manual input required for continuation.\n",
    "KK_SaveCovarianceMeans = 0\n",
    "\n",
    "# Saves a .clu file with masks sorted lexicographically.\n",
    "KK_SaveSorted = 0\n",
    "\n",
    "# Initialises using distinct derived binary masks. Use together with\n",
    "# AssignToFirstClosestMask below.\n",
    "KK_UseMaskedInitialConditions = 0\n",
    "\n",
    "# If starting with a number of clusters fewer than the number of distinct\n",
    "# derived binary masks, it will assign the rest of the points to the cluster\n",
    "# with the nearest mask.\n",
    "KK_AssignToFirstClosestMask = 0\n",
    "\n",
    "# All log-likelihoods are recalculated every KK_FullStepEvery steps\n",
    "# (see DistThresh).\n",
    "KK_FullStepEvery = 20\n",
    "KK_MinMaskOverlap = 0.\n",
    "KK_AlwaysSplitBimodal = 0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Debugging\n",
    "# ---------------------------------------------------------------------\n",
    "# Turns miscellaneous debugging information on.\n",
    "KK_Debug = 0\n",
    "# Increasing this to 2 increases the amount of information logged to\n",
    "# the console and the log.\n",
    "KK_Verbose = 1\n",
    "# Outputs more debugging information.\n",
    "KK_DistDump = 0\n",
    "# Time-saving parameter. If a point has log likelihood more than\n",
    "# DistThresh worse for a given class than for the best class, the log\n",
    "# likelihood for that class is not recalculated. This saves an awful lot\n",
    "# of time.\n",
    "KK_DistThresh = 6.907755\n",
    "# All log-likelihoods are recalculated if the fraction of instances\n",
    "# changing class exceeds ChangedThresh (see DistThresh).\n",
    "KK_ChangedThresh = 0.05\n",
    "# Produces .klg log file (default is yes, to switch off do -Log 0).\n",
    "KK_Log = 1\n",
    "# Produces parameters and progress information on the console. Set to\n",
    "# 0 to suppress output in batches.\n",
    "KK_Screen = 1\n",
    "# Helps normalize covariance matrices.\n",
    "KK_PriorPoint = 1\n",
    "# Outputs number of initial clusters.\n",
    "KK_SplitInfo = 1\n",
    "\n",
    "#No Ram Limit\n",
    "KK_RamLimitGB = -1\n",
    "    \"\"\"%PlaceHolders\n",
    "    try:\n",
    "        with open(output_name,'w') as f:\n",
    "            f.write(prm_content)\n",
    "            print(\"PRM file created!\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"PRM file failed!\")\n",
    "        return False\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "if \"__file__\" not in dir():\n",
    "    FilePath=\"/data/SWI0022/3/2016-09-27_15-47-38/\"\n",
    "    Exp='experiment1_100'\n",
    "    Fs=30000\n",
    "    nCh=27\n",
    "    \n",
    "    save_prm_file(FilePath,Exp,Fs,nCh,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read and convert .continuous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadFolderToArray(folderpath, channels = 'all', dtype = float, source = '100'):\n",
    "    '''Load CH continuous files in specified folder to a single numpy array. By default all \n",
    "    CH continous files are loaded in numerical order, ordering can be specified with\n",
    "    optional channels argument which should be a list of channel numbers.'''\n",
    "\n",
    "    if channels == 'all': \n",
    "        channels = _get_sorted_channels(folderpath,'_CH')\n",
    "        aux      = _get_sorted_channels(folderpath,'_AUX')\n",
    "\n",
    "    filelist = [source + '_CH' + x + '.continuous' for x in map(str,channels)]\n",
    "    filelist.extend([source + '_AUX' + x + '.continuous' for x in map(str,aux)])\n",
    "    numFiles = 1\n",
    "\n",
    "    print(\"Loading continuous files...\")\n",
    "    channel_1_data = loadContinuous(os.path.join(folderpath, filelist[0]), dtype)['data']\n",
    "\n",
    "    n_samples  = len(channel_1_data)\n",
    "    n_channels = len(filelist)\n",
    "\n",
    "    data_array = np.zeros([n_samples, n_channels], dtype)\n",
    "    data_array[:,0] = channel_1_data\n",
    "\n",
    "    for i, f in enumerate(filelist[1:]):\n",
    "            data_array[:, i + 1] = loadContinuous(os.path.join(folderpath, f), dtype)['data']\n",
    "            numFiles += 1\n",
    "           \n",
    "    return data_array\n",
    "\n",
    "def loadContinuous(filepath, dtype = float):\n",
    "\n",
    "    assert dtype in (float, np.int16), \\\n",
    "      'Invalid data type specified for loadContinous, valid types are float and np.int16'\n",
    "\n",
    "\n",
    "    ch = { }\n",
    "    recordNumber = np.intp(-1)\n",
    "    \n",
    "    samples = np.zeros(MAX_NUMBER_OF_CONTINUOUS_SAMPLES, dtype)\n",
    "    timestamps = np.zeros(MAX_NUMBER_OF_RECORDS)\n",
    "    recordingNumbers = np.zeros(MAX_NUMBER_OF_RECORDS)\n",
    "    indices = np.arange(0,MAX_NUMBER_OF_RECORDS*SAMPLES_PER_RECORD, SAMPLES_PER_RECORD, np.dtype(np.int64))\n",
    "    \n",
    "    #read in the data\n",
    "    f = open(filepath,'rb')\n",
    "    \n",
    "    header = readHeader(f)\n",
    "    \n",
    "    fileLength = os.fstat(f.fileno()).st_size\n",
    "   \n",
    "    while f.tell() < fileLength:\n",
    "        \n",
    "        recordNumber += 1        \n",
    "        \n",
    "        timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer \n",
    "        N = np.fromfile(f,np.dtype('<u2'),1)[0] # little-endian 16-bit unsigned integer\n",
    "        \n",
    "        #print index\n",
    "\n",
    "        if N != SAMPLES_PER_RECORD:\n",
    "            raise Exception('Found corrupted record in block ' + str(recordNumber))\n",
    "        \n",
    "        recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n",
    "        \n",
    "        if dtype == float: # Convert data to float array and convert bits to voltage.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N) * float(header['bitVolts']) # big-endian 16-bit signed integer, multiplied by bitVolts   \n",
    "        else:  # Keep data in signed 16 bit integer format.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N)  # big-endian 16-bit signed integer\n",
    "        try:\n",
    "            samples[indices[recordNumber]:indices[recordNumber+1]] = data            \n",
    "        except Exception as e:\n",
    "            print(\"error reading \",filepath)\n",
    "            print(repr(e))\n",
    "            print(\"replacing missing values with zeros.\")\n",
    "            #raise\n",
    "        \n",
    "        marker = f.read(10) # dump\n",
    "        \n",
    "        \n",
    "    ch['header'] = header \n",
    "    ch['timestamps'] = timestamps[0:recordNumber]\n",
    "    ch['data'] = samples[0:indices[recordNumber]]  # OR use downsample(samples,1), to save space\n",
    "    ch['recordingNumber'] = recordingNumbers[0:recordNumber]\n",
    "    f.close()\n",
    "    return ch\n",
    "    \n",
    "def readHeader(f):\n",
    "    header = { }\n",
    "    h = f.read(1024).decode().replace('\\n','').replace('header.','')\n",
    "    for i,item in enumerate(h.split(';')):\n",
    "        if '=' in item:\n",
    "            header[item.split(' = ')[0]] = item.split(' = ')[1]\n",
    "    return header\n",
    "    \n",
    "def _get_sorted_channels(folderpath,sep='_CH'):\n",
    "    return sorted([int(f.split(sep)[1].split('.')[0]) for f in os.listdir(folderpath) \n",
    "                    if '.continuous' in f and sep in f]) \n",
    "\n",
    "\n",
    "\n",
    "# constants\n",
    "NUM_HEADER_BYTES = 1024\n",
    "SAMPLES_PER_RECORD = 1024\n",
    "RECORD_SIZE = 8 + 16 + SAMPLES_PER_RECORD*2 + 10 # size of each continuous record in bytes\n",
    "RECORD_MARKER = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 255])\n",
    "\n",
    "# constants for pre-allocating matrices:\n",
    "MAX_NUMBER_OF_SPIKES = int(1e6)\n",
    "MAX_NUMBER_OF_RECORDS = int(1e6)\n",
    "MAX_NUMBER_OF_CONTINUOUS_SAMPLES = int(1e8)\n",
    "MAX_NUMBER_OF_EVENTS = int(1e6)\n",
    "\n",
    "def pack_continuous_files(folderpath, filename = 'openephys.dat', source='100', channels = 'all', dref = None):\n",
    "\n",
    "    '''Alternative version of pack which uses numpy's tofile function to write data.\n",
    "    pack_2 is much faster than pack and avoids quantization noise incurred in pack due\n",
    "    to conversion of data to float voltages during loadContinous followed by rounding\n",
    "    back to integers for packing.  \n",
    "    source: string name of the source that openephys uses as the prefix. It is usually 100, \n",
    "            if the headstage is the first source added, but can specify something different.\n",
    "    channels:  List of channel numbers specifying order in which channels are packed. By default\n",
    "               all CH continous files are packed in numerical order.\n",
    "    dref:  Digital referencing - either supply a channel number or 'ave' to reference to the \n",
    "           average of packed channels.\n",
    "    '''\n",
    "\n",
    "    data_array = loadFolderToArray(folderpath, channels, np.int16, source)\n",
    "\n",
    "    if dref: \n",
    "        if dref == 'ave':\n",
    "            print('Digital referencing to average of all channels.')\n",
    "            reference = np.mean(data_array,1)\n",
    "        else:\n",
    "            print('Digital referencing to channel ' + str(dref))\n",
    "            if channels == 'all': \n",
    "                channels = _get_sorted_channels(folderpath)\n",
    "            reference = deepcopy(data_array[:,channels.index(dref)])\n",
    "        for i in range(data_array.shape[1]):\n",
    "            data_array[:,i] = data_array[:,i] - reference\n",
    "\n",
    "    print('Packing data to file: ' + filename)\n",
    "    data_array.tofile(os.path.join(folderpath,filename))\n",
    "    print(\".dat file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to create .eeg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_eeg(inputName,nChannels=None,downsample=24,inputSamplingRate=30000):\n",
    "    '''\n",
    "    downsample the input file (raw.kwd or dat) and save it in .eeg\n",
    "    eegData= rawData[::16,:] (save every 16 point for each channel)\n",
    "    '''\n",
    "    downsample=int(downsample)\n",
    "    if inputName.endswith(\".raw.kwd\"):\n",
    "        raw_data=KwdRawDataReader(inputName)\n",
    "        outputName=inputName[:-7]+\"eeg\"\n",
    "    elif inputName.endswith(\".dat\"):\n",
    "        raw_data=DatRawDataReader(inputName,nChannels=nChannels)\n",
    "        outputName=inputName[:-3]+\"eeg\"\n",
    "        if nChannels is None:\n",
    "            print(\"Error in create_eeg: nChannel snot defined, can't read .dat\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Error in create_eeg: input file doesn't end with '.raw.kwd' or '.dat'\")\n",
    "        return False\n",
    "    \n",
    "    chunk_size=inputSamplingRate # 1 second\n",
    "    with open(outputName,'wb') as output:\n",
    "        for chunk in raw_data.chunks(chunk_size=chunk_size):\n",
    "            chunk_raw = chunk.data_chunk_full # shape: (nsamples, nchannels)\n",
    "            chunk_test=chunk_raw[::downsample,:]\n",
    "            for k in range(len(chunk_test)):\n",
    "                newFileByteArray=bytearray(chunk_test[k])\n",
    "                output.write(newFileByteArray)\n",
    "    print('\\n.eeg file created at: %s'%(outputName))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to save .dat, .prm, and .eeg files as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Converting/saving done\n"
     ]
    }
   ],
   "source": [
    "def dat_prm_eeg_file_save_batch(path,animalList,overwrite=False,saveEEG=False):\n",
    "    \n",
    "    for animal in animalList:\n",
    "        animalFolder=os.path.join(path,animal)\n",
    "        rawfile_list = find_file(animalFolder,['.raw.kwd'])\n",
    "        for rawfile in rawfile_list:\n",
    "            try:\n",
    "                conversion_result= convert_kwd_to_dat(rawfile,overwrite=overwrite)\n",
    "                if isinstance(conversion_result,bool):\n",
    "                    print(\".raw.kwd to .dat conversion failed:%s\"%(rawfile))\n",
    "                else:\n",
    "                    nchannels, sampling_rate=conversion_result\n",
    "                    experiment=os.path.splitext(os.path.splitext(os.path.split(rawfile)[1])[0])[0]\n",
    "                    save_prm_file(os.path.split(rawfile)[0],experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                    if saveEEG:\n",
    "                        create_eeg(rawfile,downsample=int(sampling_rate/1250),\n",
    "                                   inputSamplingRate=sampling_rate)\n",
    "            except Exception as e:\n",
    "                print('Converting .dat/ saving .prm/.eeg file failed at:')\n",
    "                print(rawfile)\n",
    "                print(repr(e))\n",
    "                print(\"\\nConverting/saving failed\")\n",
    "                return False\n",
    "        \n",
    "        #check existence of .continuous files\n",
    "        rawfile_list = find_file(animalFolder,['.continuous'])\n",
    "        folder_list = list(set([os.path.dirname(f) for f in rawfile_list]))\n",
    "        if len(folder_list) >0:\n",
    "            for continuousFolder in folder_list:\n",
    "                experiment=os.path.basename(continuousFolder)\n",
    "                filename= experiment + \".dat\"\n",
    "                settingsPath=os.path.join(continuousFolder,\"Continuous_Data.openephys\")\n",
    "                \n",
    "                if not os.path.exists(settingsPath):\n",
    "                    print (settingsPath)\n",
    "                    raise FileNotFoundError(\n",
    "                        \"There must be a \\\"Continuous_Data.openephys.xml\\\"\",\n",
    "                        \" file in the same folder as .continuous files to load the settings.\")\n",
    "                \n",
    "                with open(settingsPath,'r') as f:\n",
    "                    openEphysSettings=xmltodict.parse(f.read())\n",
    "                    openEphysSettings=openEphysSettings['EXPERIMENT']['RECORDING']\n",
    "                    if isinstance(openEphysSettings, list):\n",
    "                        openEphysSettings=openEphysSettings[0]\n",
    "                    sampling_rate =openEphysSettings['@samplerate']\n",
    "                    source        =openEphysSettings['PROCESSOR']['@id']\n",
    "                    nchannels     =len(openEphysSettings['PROCESSOR']['CHANNEL'])\n",
    "                    \n",
    "                    \n",
    "                #convert to and save as .DAT file\n",
    "                pack_continuous_files(continuousFolder, filename = filename, source=source, channels = 'all', dref = None)\n",
    "                #save PRM file\n",
    "                save_prm_file(continuousFolder,experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                if saveEEG:\n",
    "                    create_eeg(rawfile,downsample=int(sampling_rate/1250),\n",
    "                               inputSamplingRate=sampling_rate)        \n",
    "    print(\"\\nConverting/saving done\")\n",
    "    \n",
    "        \n",
    "#------------------------------------------------------------------------------------------\n",
    "if \"__file__\" not in dir():\n",
    "    Path=\"/data/SWI002/22/\"\n",
    "    animalList=[]\n",
    "    \n",
    "    dat_prm_eeg_file_save_batch(Path,animalList)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# Script to Rename and Convert as a Batch\n",
    "##### 'root' must be a directory containing seperate folders for each animal(ex: /data/ containing /data/Rat001, /data/Rat002, ...)\n",
    "##### 'animalList' determines on which folders within the 'root' this notebook will operate(ex: Rat001, Rat002, ...).\n",
    "##### renaming MUST precede converting/saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal Rat105: /data/Rat105/Experiments\n",
      "*Rat105_2016_12_07_17_37\n",
      "*Rat105_2016_12_07_15_09\n",
      "*Rat105_2016_12_05_17_57\n",
      "*Rat105_2016_12_06_11_13\n",
      "*Rat105_2016_12_07_17_04\n",
      "\n",
      "Renaming done\n",
      "--------\n",
      "\n",
      "\n",
      "openning /data/Rat105/Experiments/Rat105_2016_12_07_17_37/Rat105_2016_12_07_17_37.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 20000.0Hz\n",
      "Converting...\n",
      "wrote /data/Rat105/Experiments/Rat105_2016_12_07_17_37/Rat105_2016_12_07_17_37.dat\n",
      "done\n",
      "PRM file created!\n",
      "\n",
      ".eeg file created at: /data/Rat105/Experiments/Rat105_2016_12_07_17_37/Rat105_2016_12_07_17_37.eeg\n",
      "\n",
      "\n",
      "openning /data/Rat105/Experiments/Rat105_2016_12_07_15_09/Rat105_2016_12_07_15_09.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 30000.0Hz\n",
      "Converting...\n",
      "wrote /data/Rat105/Experiments/Rat105_2016_12_07_15_09/Rat105_2016_12_07_15_09.dat\n",
      "done\n",
      "PRM file created!\n",
      "\n",
      ".eeg file created at: /data/Rat105/Experiments/Rat105_2016_12_07_15_09/Rat105_2016_12_07_15_09.eeg\n",
      "\n",
      "\n",
      "openning /data/Rat105/Experiments/Rat105_2016_12_05_17_57/Rat105_2016_12_05_17_57.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 30000.0Hz\n",
      "Converting...\n",
      "\n",
      "Rat105_2016_12_05_17_57.dat already exists, no conversion to do\n",
      "/data/Rat105/Experiments/Rat105_2016_12_05_17_57/Rat105_2016_12_05_17_57.prm already exists, it will be overwritten\n",
      "PRM file created!\n",
      "\n",
      ".eeg file created at: /data/Rat105/Experiments/Rat105_2016_12_05_17_57/Rat105_2016_12_05_17_57.eeg\n",
      "\n",
      "\n",
      "openning /data/Rat105/Experiments/Rat105_2016_12_06_11_13/Rat105_2016_12_06_11_13.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 30000.0Hz\n",
      "Converting...\n",
      "\n",
      "Rat105_2016_12_06_11_13.dat already exists, no conversion to do\n",
      "/data/Rat105/Experiments/Rat105_2016_12_06_11_13/Rat105_2016_12_06_11_13.prm already exists, it will be overwritten\n",
      "PRM file created!\n",
      "\n",
      ".eeg file created at: /data/Rat105/Experiments/Rat105_2016_12_06_11_13/Rat105_2016_12_06_11_13.eeg\n",
      "\n",
      "\n",
      "openning /data/Rat105/Experiments/Rat105_2016_12_07_17_04/Rat105_2016_12_07_17_04.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 20000.0Hz\n",
      "Converting...\n",
      "wrote /data/Rat105/Experiments/Rat105_2016_12_07_17_04/Rat105_2016_12_07_17_04.dat\n",
      "done\n",
      "PRM file created!\n",
      "\n",
      ".eeg file created at: /data/Rat105/Experiments/Rat105_2016_12_07_17_04/Rat105_2016_12_07_17_04.eeg\n",
      "\n",
      "Converting/saving done\n"
     ]
    }
   ],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    #The directory containing separate folders for each animal\n",
    "    root=\"/data/\"\n",
    "    #in Windows paths must be like this: root=\"C:\\\\Data\\\\Recordings\\\\\" (double backslash instead of single)\n",
    "    \n",
    "    #Animal where to run the script\n",
    "    animalList=[\"Rat105\"]\n",
    "    \n",
    "    #Whether to save .eeg files for each .raw.kwd file\n",
    "    saveEEG=True\n",
    "    \n",
    "    #Whether to overwrite the existing .dat files\n",
    "    overwrite=False\n",
    "\n",
    "    #Whether to print \"X renamed in Y\"\n",
    "    verbose=True  \n",
    "\n",
    "    #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "    # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "    minuteDelay=0\n",
    "\n",
    "    #Files to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "    #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "    extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwe\",\".eeg\"]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "    rename_batch(root,animalList,verbose,minuteDelay,extensionList)\n",
    "    dat_prm_eeg_file_save_batch(root,animalList,overwrite,saveEEG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
