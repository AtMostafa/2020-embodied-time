{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook aims to convert Open Ephys output files (.raw.kwd) and folders to proper names and formats.\n",
    "# This notebook contains the following sections:\n",
    "\n",
    "## Rename Directories and files altogether\n",
    "## Convert .raw.kwd files and save .dat files\n",
    "## Save .prm files\n",
    "## Save .eeg files\n",
    "\n",
    "### Run all the cells one time then put the preferred values in the last cell and run it. It will call all the functions in the previous cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import  subprocess\n",
    "import tables as tb\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from datetime import datetime,timedelta\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "from shutil import copy,rmtree\n",
    "\n",
    "def find_file(path, extension=['.raw.kwd']):\n",
    "    \"\"\"\n",
    "    This function finds all the file types specified by 'extension' (ex: *.dat) in the 'path' directory\n",
    "    and all its subdirectories and their sub-subdirectories etc., \n",
    "    and returns a list of all dat file paths\n",
    "    'extension' is a list of desired file extensions: ['.dat','.prm']\n",
    "    \"\"\"\n",
    "    if type(extension) is str:\n",
    "        extension=extension.split()   #turning extension into a list with a single element\n",
    "    return [os.path.join(walking[0],goodfile) for walking in list(os.walk(path)) \n",
    "         for goodfile in walking[2] for ext in extension if goodfile.endswith(ext)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming\n",
    "\n",
    "-Functions and classes required for renaming all the OpenEphys files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_folder(folderToRemove,newFolder):\n",
    "    '''\n",
    "    folderToRemove (=old), newFolder: full paths to folders\n",
    "    merge the folders if \"newFolder\" already exists\n",
    "    files are renamed (example: folderToRemove_1.txt becomes newFolder_1.txt)\n",
    "    '''\n",
    "    #if the new folder doesn't exist, create it\n",
    "    if not os.path.exists(newFolder):\n",
    "        os.mkdir(newFolder) \n",
    "    #move and rename the files \n",
    "    for f in os.listdir(folderToRemove):\n",
    "        oldpath=os.path.join(folderToRemove,f)\n",
    "        oldFolderName=os.path.basename(folderToRemove.rstrip(os.sep))\n",
    "        newFolderName=os.path.basename(newFolder.rstrip(os.sep))\n",
    "        f=f.replace(oldFolderName,newFolderName)\n",
    "        newpath=os.path.join(newFolder,f)\n",
    "        if os.path.exists(newpath):\n",
    "            os.remove(oldpath)\n",
    "        else:\n",
    "            os.rename(oldpath,newpath)\n",
    "    os.rmdir(folderToRemove)\n",
    "    \n",
    "def get_folders_matching_format(rootFolder,globFormat):\n",
    "    '''\n",
    "    rootFolder: full path where to look for session folders (ex: \"/data/Rat034/Experiments\")\n",
    "    '''\n",
    "    fullFormat=os.path.join(rootFolder,globFormat)\n",
    "    return [os.path.basename(f) for f in glob.glob(fullFormat)]\n",
    "\n",
    "def get_regular_name(wrongName,wrongTimeFormat,regularTimeFormat):\n",
    "    '''\n",
    "    from a session name in wrong format, return the regular format\n",
    "    '''\n",
    "    date=datetime.strptime(wrongName,wrongTimeFormat)\n",
    "    return date.strftime(regularTimeFormat)\n",
    "\n",
    "def get_date(name,timeFormat):\n",
    "    '''\n",
    "    return a datetime object from a session name and a format\n",
    "    '''\n",
    "    date=datetime.strptime(name,timeFormat)\n",
    "    return date\n",
    "\n",
    "def get_regular_name_from_date(date,regularTimeFormat):\n",
    "    '''\n",
    "    return a name given a date and a format\n",
    "    '''\n",
    "    return date.strftime(regularTimeFormat)\n",
    "\n",
    "def rename_all_in_rootFolder(rootFolder,wrongGlobFormat,wrongTimeFormat,regularGlobFormat,\n",
    "                             regularTimeFormat,minuteDelay=2,verbose=False):\n",
    "    '''\n",
    "    rootFolder: full path where to look for session folders (ex: \"/data/Rat034/Experiments\")\n",
    "    wrongGlobFormat: glob pattern to search for the wrong session names\n",
    "    wrongTimeFormat: datetime pattern to read date from wrong session names\n",
    "    regularGlobFormat, regularTimeFormat: glob and datetime patterns for the right session names\n",
    "    minuteDelay: if two folders have the same date (give or take minuteDelay), merge them\n",
    "    '''\n",
    "    delay=timedelta(minutes=minuteDelay)\n",
    "\n",
    "    #get names and date of the regular folders\n",
    "    regularFolders=get_folders_matching_format(rootFolder,regularGlobFormat)\n",
    "    allRegularDates=[get_date(name,regularTimeFormat) for name in regularFolders]\n",
    "    \n",
    "    #get names of wrong folders\n",
    "    fList=get_folders_matching_format(rootFolder,wrongGlobFormat)\n",
    "    for f in fList:\n",
    "        merge=False\n",
    "        #check if there is a regular folder around the same date\n",
    "        date=get_date(f,wrongTimeFormat)\n",
    "        for otherDate in allRegularDates:\n",
    "            if abs(date-otherDate)<delay:\n",
    "                date=otherDate\n",
    "                merge=True\n",
    "                break\n",
    "        #new name\n",
    "        newFolder=get_regular_name_from_date(date,regularTimeFormat)\n",
    "        #rename/merge\n",
    "        newPath=os.path.join(rootFolder,newFolder)\n",
    "        oldPath=os.path.join(rootFolder,f)\n",
    "        rename_folder(oldPath,newPath)\n",
    "        if verbose:\n",
    "            if merge:\n",
    "                print(\"Merged %s into %s\"%(f,newFolder))\n",
    "            else:\n",
    "                print(\"Renamed %s in %s\"%(f,newFolder))\n",
    "                \n",
    "def rename_files_to_match_folder(folderPath,extensionList):\n",
    "    '''\n",
    "    folderPath= \"/data/Rat034/Experiments/Rat034_2015_etc\"\n",
    "    extensionList= [\".dat\", \".prm\", \".kwik\"]\n",
    "    --> Renames \"someFile.dat\" into \"Rat024_2015_etc.dat\"\n",
    "    '''\n",
    "    folderName=os.path.basename(folderPath.rstrip(os.sep))\n",
    "    extensionList=[ext if ext.startswith(\".\") else \".\"+ext for ext in extensionList]\n",
    "    for f in os.listdir(folderPath):\n",
    "        for ext in extensionList:\n",
    "            if f.endswith(ext):\n",
    "                oldPath=os.path.join(folderPath,f)\n",
    "                newName=folderName+ext\n",
    "                newPath=os.path.join(folderPath,newName)\n",
    "                os.rename(oldPath,newPath)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to rename the files and folders of OpenEphys as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_batch(root,animalList,verbose,minuteDelay,extensionList):\n",
    "    \n",
    "    WRONG_FORMATS={\n",
    "    \"openEphys\":(\n",
    "        \"animal_20{0}-{0}-{0}_{0}-{0}-{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2022-02-21_11-28-17'\n",
    "        \"animal_%Y-%m-%d_%H-%M-%S\"\n",
    "    ),\n",
    "    \"seconds\":(\n",
    "        \"animal_20{0}_{0}_{0}_{0}_{0}_{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2013_12_26_20_47_54'\n",
    "        \"animal_%Y_%m_%d_%H_%M_%S\"\n",
    "    ),\n",
    "    \"tiret\":(\n",
    "        \"animal_20{0}_{0}_{0}-{0}_{0}\".format(\"[00-99]?\"),        #ex: 'Rat001_2039_10_15-06_27'\n",
    "        \"animal_%Y_%m_%d-%H_%M\"\n",
    "    ),\n",
    "    \"tiretSeconds\":(\n",
    "        \"animal_20{0}_{0}_{0}-{0}_{0}_{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2033_06_23-21_24_46'\n",
    "        \"animal_%Y_%m_%d-%H_%M_%S\"\n",
    "    ),\n",
    "    \"openEphys2\":(\n",
    "        \"20{0}-{0}-{0}_{0}-{0}-{0}\".format(\"[00-99]?\"),           #ex: '2015-02-15_11-59-48'\n",
    "        \"%Y-%m-%d_%H-%M-%S\"\n",
    "    ),\n",
    "    }\n",
    "    # The right format \n",
    "    REGULAR_FORMAT=(\"animal_20{0}_{0}_{0}_{0}_{0}\".format(\"?[00-99]\"),  #ex: 'Rat001_2039_10_15_06_27'\n",
    "                \"animal_%Y_%m_%d_%H_%M\")\n",
    "      \n",
    "    for animal in animalList:\n",
    "        rawFileList=find_file(os.path.join(root,animal),extensionList)\n",
    "        rootFolderList=[os.path.split(os.path.split(rawFile)[0])[0] for rawFile in rawFileList]\n",
    "        #rootFolderList keeps paths to the directories within 'root' which contain session folders\n",
    "        rootFolderList=list(set(rootFolderList))#to keep unique members\n",
    "        \n",
    "        for rootFolder in rootFolderList:\n",
    "            print(\"animal %s: %s\"%(animal,rootFolder))\n",
    "    \n",
    "            #RENAME\n",
    "            regularGlob=REGULAR_FORMAT[0].replace(\"animal\",animal)\n",
    "            regularTime=REGULAR_FORMAT[1].replace(\"animal\",animal)\n",
    "            for name in WRONG_FORMATS:\n",
    "                globFormat=WRONG_FORMATS[name][0].replace(\"animal\",animal)\n",
    "                timeFormat=WRONG_FORMATS[name][1].replace(\"animal\",animal)       \n",
    "                #print(get_folders_matching_format(rootFolder,globFormat))\n",
    "                rename_all_in_rootFolder(rootFolder,globFormat,timeFormat,regularGlob,regularTime,\n",
    "                                         minuteDelay=minuteDelay,verbose=verbose)\n",
    "    \n",
    "            #CONVERT\n",
    "            #for every session folder of the animal\n",
    "            for folder in os.listdir(rootFolder):\n",
    "                if not folder.startswith(animal):\n",
    "                    continue\n",
    "                print(\"*\"+folder)\n",
    "                path=os.path.join(rootFolder,folder)\n",
    "                #rename files\n",
    "                rename_files_to_match_folder(path,extensionList)\n",
    "\n",
    "                for f in os.listdir(path):\n",
    "                    fpath=os.path.join(path,f)\n",
    "                    #convert raw.kwd\n",
    "                    if f==\"settings.xml\":\n",
    "                        newPath=os.path.join(path,\"settingsOpenEphys.xml\")\n",
    "                        os.rename(fpath,newPath)\n",
    "    \n",
    "            print(\"\\nRenaming done\")\n",
    "            print(\"--------\")\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# if \"__file__\" not in dir():\n",
    "#     root=\"/data/SWI002/22/\"\n",
    "#     #Animal where to run the script\n",
    "#     animalList=[\"SWI002\"]\n",
    "\n",
    "#     #Whether to print \"X renamed in Y\"\n",
    "#     verbose=True  \n",
    "\n",
    "#     #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "#     # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "#     minuteDelay=0\n",
    "\n",
    "#     #File to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "#     #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "#     extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwx\",\".kwe\",\".eeg\"]\n",
    "\n",
    "#     #--------------------------------------------------------------------------------\n",
    "#     rename_batch(root,animalList,verbose,minuteDelay,extensionList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5df7411-916f-4b42-85e5-c42b806ff1dd"
    }
   },
   "source": [
    "## Converting\n",
    "-Required functions and classes to open .raw.kwd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "b23eca88-6789-483b-9d4b-538e8bed2682"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------- chunck class from Klusta\n",
    "# reads data chunk by chunk (instead of all at once, as the files are big)\n",
    "class Chunk(object):\n",
    "    def __init__(self, data=None, nsamples=None, nchannels=None,\n",
    "                 bounds=None, dtype=None, recording=0, nrecordings=1):\n",
    "        self._data = data\n",
    "        if nsamples is None and nchannels is None:\n",
    "            nsamples, nchannels = data.shape\n",
    "        self.nsamples = nsamples\n",
    "        self.nchannels = nchannels\n",
    "        self.dtype = dtype\n",
    "        self.recording = recording\n",
    "        self.nrecordings = nrecordings\n",
    "        self.s_start, self.s_end, self.keep_start, self.keep_end = bounds\n",
    "        self.window_full = self.s_start, self.s_end\n",
    "        self.window_keep = self.keep_start, self.keep_end      \n",
    "    @property\n",
    "    def data_chunk_full(self):\n",
    "        chunk = self._data[self.s_start:self.s_end,:]\n",
    "        return convert_dtype(chunk, self.dtype)   \n",
    "    @property\n",
    "    def data_chunk_keep(self):\n",
    "        chunk =  self._data[self.keep_start:self.keep_end,:]\n",
    "        return convert_dtype(chunk, self.dtype)\n",
    "\n",
    "def chunk_bounds(nsamples, chunk_size, overlap=0):\n",
    "    s_start = 0\n",
    "    s_end = chunk_size\n",
    "    keep_start = s_start\n",
    "    keep_end = s_end - overlap // 2\n",
    "    yield s_start, s_end, keep_start, keep_end    \n",
    "    while s_end - overlap + chunk_size < nsamples:\n",
    "        s_start = s_end - overlap\n",
    "        s_end = s_start + chunk_size\n",
    "        keep_start = keep_end\n",
    "        keep_end = s_end - overlap // 2\n",
    "        if s_start < s_end:\n",
    "            yield s_start, s_end, keep_start, keep_end        \n",
    "    s_start = s_end - overlap\n",
    "    s_end = nsamples\n",
    "    keep_start = keep_end\n",
    "    keep_end = s_end\n",
    "    if s_start < s_end:\n",
    "        yield s_start, s_end, keep_start, keep_end\n",
    "\n",
    "def convert_dtype(data, dtype=None, factor=None):\n",
    "    if not dtype:\n",
    "        return data\n",
    "    if data.shape[0] == 0:\n",
    "        return data.astype(dtype)\n",
    "    dtype_old = data.dtype\n",
    "    if dtype_old == dtype:\n",
    "        return data\n",
    "    key = (_get_dtype(dtype_old), _get_dtype(dtype))\n",
    "    factor = factor or _dtype_factors.get(key, 1)\n",
    "    if dtype_old in (np.float32, np.float64):\n",
    "        factor = factor/np.abs(data).max()\n",
    "    if factor != 1:\n",
    "        return (data * factor).astype(dtype)\n",
    "    else:\n",
    "        return data.astype(dtype) \n",
    "#------------------------------------------------------------------------------ open a file, from kwiklib \n",
    "def open_file(path,mode=\"r\"):\n",
    "    try:\n",
    "        f = tb.open_file(path, mode)\n",
    "        return f\n",
    "    except IOError as e:\n",
    "        warn(\"IOError: \" + str(e.message))\n",
    "        return\n",
    "# ------------------------------------------------------------------- KWD and .dat data reader from Klusta\n",
    "class BaseRawDataReader(object):\n",
    "    def __init__(self, dtype_to=np.int16):\n",
    "        self.dtype_to = dtype_to\n",
    "    def next_recording(self):\n",
    "        for self.recording in range(self.nrecordings):\n",
    "            yield self.recording, self.get_recording_data(self.recording)    \n",
    "    def get_recording_data(self, recording):\n",
    "        raise NotImplementedError()    \n",
    "    def chunks(self, chunk_size=None, chunk_overlap=0):\n",
    "        for recording, data in self.next_recording():\n",
    "            assert chunk_size is not None, \"You need to specify a chunk size.\"\"\"\n",
    "            for bounds in chunk_bounds(data.shape[0], chunk_size=chunk_size, overlap=chunk_overlap):\n",
    "                yield Chunk(data, bounds=bounds, dtype=self.dtype_to, recording=recording, nrecordings=self.nrecordings)\n",
    "class KwdRawDataReader(BaseRawDataReader):\n",
    "    def __init__(self, filename, dtype_to=np.int16):\n",
    "        self.kwd = open_file(filename, 'r')\n",
    "        self.nrecordings = self.kwd.root.recordings._v_nchildren\n",
    "        super(KwdRawDataReader, self).__init__(dtype_to=dtype_to)        \n",
    "    def get_recording_data(self, recording):\n",
    "        data = self.kwd.root.recordings._f_get_child(str(recording)).data\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to open the .raw.kwd file and save the .dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_kwd_to_dat(filename,outputname=None,overwrite=False):\n",
    "    samplingRateList=[10000,20000,25000,30000,15000]\n",
    "    #output file name\n",
    "    if outputname is None:\n",
    "        outputname=filename[:-7]+\"dat\"\n",
    "    out=os.path.basename(outputname)\n",
    "    \n",
    "    if os.path.exists(outputname) and not overwrite:\n",
    "        print(\".dat file already exists, skipping this session.\")\n",
    "        return False\n",
    "    \n",
    "    if filename.endswith('.raw.kwd'):\n",
    "        #read number of channels in kwd file\n",
    "        with tb.open_file(filename,\"r\") as kwd:\n",
    "            print(\"\\n\\nopenning %s\"%(filename))\n",
    "            dataLen,nchannels=kwd.get_node(\"/recordings/0/data\").read().shape\n",
    "            sampling_rate=kwd.get_node(\"/recordings/0/\")._v_attrs.__getitem__('sample_rate')\n",
    "        \n",
    "        if int(sampling_rate) not in samplingRateList:  #This condition avoids OpenEphys bug in writing the sampling rate value!\n",
    "            sampling_rate=20000\n",
    "        \n",
    "        if dataLen<nchannels:\n",
    "            dataLen,nchannels=nchannels,dataLen\n",
    "        if dataLen<sampling_rate:    #not an important condition! Could be ignored!\n",
    "            raise ValueError('Data Length too short')\n",
    "\n",
    "        print(\"number of channels: %s\"%(nchannels))\n",
    "        print(\"sampling rate: %sHz\"%(sampling_rate))\n",
    "        print(\"Converting...\")\n",
    "\n",
    "        if os.path.exists(outputname):\n",
    "            if overwrite:\n",
    "                print(\"\\n%s already exist, it will be overwritten\"%(out))\n",
    "            else:\n",
    "                print(\"\\n%s already exists, no conversion to do\"%(out))\n",
    "                return [nchannels, sampling_rate]\n",
    "        #instantiate reader\n",
    "        kwd_data=KwdRawDataReader(filename)\n",
    "    else:\n",
    "        print(\"Error: the raw data file doesn't end with '.raw.kwd'\")\n",
    "        return False\n",
    "\n",
    "    #To correct for single channel recording: add a second zero-value channel\n",
    "    patch=[]\n",
    "    if nchannels==1:\n",
    "        nchannels+=1\n",
    "        patch=[0]\n",
    "\n",
    "    #create dat file, open write only in binary    \n",
    "    try:\n",
    "        with open(outputname,'wb') as output:\n",
    "            chunk_size=sampling_rate\n",
    "            for chunk in kwd_data.chunks(chunk_size):\n",
    "                data=chunk.data_chunk_full\n",
    "                for i in range(len(data)):\n",
    "                    newFileByteArray=bytearray(np.append(data[i],patch).astype(data[i].dtype))\n",
    "                    output.write(newFileByteArray)\n",
    "        kwd_data.kwd.close()\n",
    "        print(\"wrote %s\"%(outputname))\n",
    "        print(\"done\")\n",
    "        return [nchannels, sampling_rate]\n",
    "    except Exception as e:\n",
    "        print(\"Dat file writing failed:\")\n",
    "        print(repr(e))\n",
    "        os.remove(outputname)\n",
    "        return False\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# if \"__file__\" not in dir():\n",
    "#     inputFile=\"\"\n",
    "#     deleteKWD=False\n",
    "#     overwrite=False\n",
    "\n",
    "#     convert_kwd_to_dat(inputFile,overwrite=overwrite)\n",
    "\n",
    "#     if deleteKWD and os.path.exists(inputFile) and False:\n",
    "#         print(\"Delete %s\"%os.path.basename(inputFile))\n",
    "#         os.remove(inputFile)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Save the .prm file with correct experiment name, sampling rate and channel number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prm_file(filepath,experiment,sampling_rate,n_channels,overwrite=True):\n",
    "    output_name=os.path.join(filepath,experiment+\".prm\")\n",
    "    if os.path.exists(output_name):\n",
    "        if overwrite:\n",
    "            print(\"%s already exists, it will be overwritten\"%(output_name))\n",
    "        else:\n",
    "            print(\"%s already exists, no conversion to do\"%(output_name))\n",
    "            return False\n",
    "    \n",
    "    PlaceHolders={\n",
    "        'exp_name':experiment,\n",
    "        'Fs':str(sampling_rate),\n",
    "        'Nch':str(n_channels),\n",
    "    }\n",
    "    prm_content=\"\"\"experiment_name = '%(exp_name)s'\n",
    "prb_file = experiment_name + '.prb'\n",
    "\n",
    "traces=dict(\n",
    "    raw_data_files  = [experiment_name + '.dat'],\n",
    "    voltage_gain    = 10,\n",
    "    nbits           = 16,\n",
    "    dtype           = 'int16',\n",
    "    sample_rate     = %(Fs)s,\n",
    "    n_channels      = %(Nch)s,\n",
    "    )\n",
    "\n",
    "nbits          = 16\n",
    "voltage_gain   = traces['voltage_gain']\n",
    "sample_rate    = traces['sample_rate']\n",
    "nchannels      = traces['n_channels']\n",
    "\n",
    "spikedetekt=dict(\n",
    "    #######################################################################\n",
    "    # SpikeDetekt parameters\n",
    "    #######################################################################\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Raw data filtering and saving\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Whether to save the .raw.kwd file if a non-HDF5 raw data format is used.\n",
    "    # This is needed to visualise the data in TraceView etc, and speeds up\n",
    "    # future runs of SpikeDetekt. If a .raw.kwd file is used as the input,\n",
    "    # it will never be overwritten.\n",
    "    save_raw = True,\n",
    "    # Whether to save the .high.kwd file with HPF data used for spike\n",
    "    # detection. This is processed using a Butterworth band-pass filter.\n",
    "    save_high = False,\n",
    "    # Bandpass filter low corner frequency\n",
    "    filter_low = 500.,\n",
    "    # Bandpass filter high corner frequency\n",
    "    filter_high = 0.95 * .5 * sample_rate,\n",
    "    # Order of Butterworth filter.\n",
    "    filter_butter_order = 3,\n",
    "    # Whether to save a .low.kwd file; this is processed using a Hamming\n",
    "    # window FIR filter, then subsampled 16x to save space when storing.\n",
    "    save_low = False,\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # Chunks\n",
    "    # ---------------------------------------------------------------------\n",
    "    # SpikeDetekt processes the raw data in chunks with small overlaps to\n",
    "    # catch spikes which would otherwise span two chunks. These options\n",
    "    # will change the default chunk size and overlap.\n",
    "    chunk_size = int(1. * sample_rate), # 1 second\n",
    "    chunk_overlap = int(.015 * sample_rate), # 15 ms\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Threshold setting for spike detection\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Change this to 'positive' to detect positive spikes.\n",
    "    detect_spikes = 'negative',\n",
    "    # SpikeDetekt takes a set of uniformly distributed chunks throughout\n",
    "    # the high-pass filtered data to estimate its standard deviation. These\n",
    "    # parameters select how many excerpts are used and how long each of them are.\n",
    "    nexcerpts = 50,\n",
    "    excerpt_size = int(1. * sample_rate), # 1 second\n",
    "    # This is then used to calculate a base threshold which is multiplied\n",
    "    # by the two parameters below for the two-threshold detection process.\n",
    "    threshold_strong_std_factor = 4.5,\n",
    "    threshold_weak_std_factor = 2.,\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Spike extraction\n",
    "    # ---------------------------------------------------------------------\n",
    "    # The number of samples to extract before and after the centre of the\n",
    "    # spike for waveforms. Then, waveforms_nsamples is calculated using the\n",
    "    # formula: waveforms_nsamples = extract_s_before + extract_s_after\n",
    "    extract_s_before = int(0.0008* sample_rate),\n",
    "    extract_s_after  = int(0.0008* sample_rate),\n",
    "\n",
    "    #---------------------------------------------------------------------\n",
    "    # Features\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Number of features (PCs) per channel.\n",
    "    nfeatures_per_channel = 3,\n",
    "    # The number of spikes used to determine the PCs\n",
    "    pca_nwaveforms_max = 10000,\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Advanced\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Number of samples to use in floodfill algorithm for spike detection\n",
    "    #connected_component_join_size = 1, # 1 sample\n",
    "    connected_component_join_size = int(.00005 * sample_rate), # 0.05ms\n",
    "    # Waveform alignment\n",
    "    weight_power = 2,\n",
    "    # Whether to make the features array contiguous\n",
    "    features_contiguous = True,\n",
    "    )\n",
    "\n",
    "#Mostafa: Don't know if the following section is working and what does it do?!\n",
    "##############################################################\n",
    "# KlustaKwik parameters (must be prefixed by KK_). Uncomment to override\n",
    "# the defaults, which can be shown by running 'klustakwik' with no options\n",
    "###############################################################\n",
    "\n",
    "# This causes KlustaKwik to perform clustering on a subset of spikes and\n",
    "# estimate the assignment of the other spikes. This causes a speedup in\n",
    "# computational time (by a rough factor of KK_Subset), though will not\n",
    "# significantly decrease RAM usage. For long runs where you are unsure of\n",
    "# the data quality, you can first use KK_Subset = 50 to check the\n",
    "# clustering quality before performing a Subset 1 (all spikes) run.\n",
    "KK_Subset = 1\n",
    "\n",
    "# The largest permitted number of clusters, so cluster splitting can produce\n",
    "# no more than n clusters. Note: This must be set higher than MaskStarts.\n",
    "KK_MaxPossibleClusters = 1000\n",
    "\n",
    "# Maximum number of iterations. ie. it won't try more than n iterations\n",
    "# from any starting point.\n",
    "KK_MaxIter = 10000\n",
    "\n",
    "# You can start with a chosen fixed number of clusters derived from the\n",
    "# mask vectors, set by KK_MaskStarts.\n",
    "KK_MaskStarts = 500\n",
    "\n",
    "# The number of iterations after which KlustaKwik first attempts to split\n",
    "# existing clusters. KlustaKwik then splits every SplitEvery iterations.\n",
    "KK_SplitFirst = 20\n",
    "\n",
    "# The number of iterations after which KlustaKwik attempts to split existing\n",
    "# clusters. When using masked initializations, to save time due to excessive\n",
    "# splitting, set SplitEvery to a large number, close to the number of distinct\n",
    "# masks or the number of chosen starting masks.\n",
    "KK_SplitEvery = 40\n",
    "\n",
    "# KlustaKwik uses penalties to reduce the number of clusters fit. The parameters PenaltyK and PenaltyKLogN are \n",
    "# given positive values. The higher the values, the fewer clusters you obtain. Higher penalties\n",
    "# discourage cluster splitting. PenaltyKLogN also increases penalty when there are more points. \n",
    "#-PenaltyK 0 -PenaltyKLogN 1 is the default, corresponding to the \"Bayesian Information Criterion\".\n",
    "# -PenaltyK 1 -PenaltyKLogN 0 corresponds to \"Akaike's Information Criterion\". This produces a larger number \n",
    "# of clusters, and is recommended if you are find that clusters corresponding to different neurons are incorrectly merged.\n",
    "KK_PenaltyK = 0.\n",
    "KK_PenaltyKLogN = 1.\n",
    "\n",
    "# Specifies a seed for the random number generator.\n",
    "KK_RandomSeed = 1\n",
    "\n",
    "# The number of unmasked spikes on a certain channel needed to unmask that\n",
    "# channel in the cluster. This prevents a single noisy spike, or coincident\n",
    "# noise on adjacent channels from slowing down computation time.\n",
    "KK_PointsForClusterMask = 10\n",
    "\n",
    "# Setting this saves a .temp.clu file every iteration. This slows the runtime\n",
    "# down reasonably significantly for small runs with many iterations, but allows\n",
    "# to recover where KlustaKwik left off; useful in case of large runs where you\n",
    "# are not confident that the run will be uninterrupted.\n",
    "KK_SaveTempCluEveryIter = 0\n",
    "\n",
    "# This is an integer N when, used in combination with the empty string\n",
    "# for UseFeatures above, omits the last N features. This should always\n",
    "# be used with KK_UseFeatures = \"\"\n",
    "KK_DropLastNFeatures = 0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Classic 'all channels unmasked always' mode | DO NOT uncomment\n",
    "# ---------------------------------------------------------------------\n",
    "# To use KlustaKwik in \"unmasked\" mode, set this to 0.\n",
    "# This disables the use of the new `masked Expectation-Maximization'\n",
    "# algorithm, and sets all the channels to be unmasked on all spikes.\n",
    "#KK_UseDistributional = 1\n",
    "\n",
    "# In classic mode, KlustaKwik starts from random cluster assignments,\n",
    "# running a new random start for every integer between MinClusters and\n",
    "# MaxClusters. For these values to take effect, MaskStarts must be set to 0.\n",
    "#KK_MinClusters = 100\n",
    "#KK_MaxClusters = 110\n",
    "\n",
    "# By default, this is an empty string, which means 'use all features'.\n",
    "# Or, you can you can specify a string with 1's for features you want to\n",
    "# use, and 0's for features you don't want to use. In classic mode,\n",
    "# you use this option to take out bad channels. In masked mode,\n",
    "# you should instead take bad channels out from the .PRB file.\n",
    "#KK_UseFeatures = \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Advanced\n",
    "# ---------------------------------------------------------------------\n",
    "# The algorithm will be started n times for each initial cluster count\n",
    "# between MinClusters and MaxClusters.\n",
    "KK_nStarts = 1\n",
    "\n",
    "# Saves means and covariance matrices. Stops computation at each iteration.\n",
    "# Manual input required for continuation.\n",
    "KK_SaveCovarianceMeans = 0\n",
    "\n",
    "# Saves a .clu file with masks sorted lexicographically.\n",
    "KK_SaveSorted = 0\n",
    "\n",
    "# Initialises using distinct derived binary masks. Use together with\n",
    "# AssignToFirstClosestMask below.\n",
    "KK_UseMaskedInitialConditions = 0\n",
    "\n",
    "# If starting with a number of clusters fewer than the number of distinct\n",
    "# derived binary masks, it will assign the rest of the points to the cluster\n",
    "# with the nearest mask.\n",
    "KK_AssignToFirstClosestMask = 0\n",
    "\n",
    "# All log-likelihoods are recalculated every KK_FullStepEvery steps\n",
    "# (see DistThresh).\n",
    "KK_FullStepEvery = 20\n",
    "KK_MinMaskOverlap = 0.\n",
    "KK_AlwaysSplitBimodal = 0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Debugging\n",
    "# ---------------------------------------------------------------------\n",
    "# Turns miscellaneous debugging information on.\n",
    "KK_Debug = 0\n",
    "# Increasing this to 2 increases the amount of information logged to\n",
    "# the console and the log.\n",
    "KK_Verbose = 1\n",
    "# Outputs more debugging information.\n",
    "KK_DistDump = 0\n",
    "# Time-saving parameter. If a point has log likelihood more than\n",
    "# DistThresh worse for a given class than for the best class, the log\n",
    "# likelihood for that class is not recalculated. This saves an awful lot\n",
    "# of time.\n",
    "KK_DistThresh = 6.907755\n",
    "# All log-likelihoods are recalculated if the fraction of instances\n",
    "# changing class exceeds ChangedThresh (see DistThresh).\n",
    "KK_ChangedThresh = 0.05\n",
    "# Produces .klg log file (default is yes, to switch off do -Log 0).\n",
    "KK_Log = 1\n",
    "# Produces parameters and progress information on the console. Set to\n",
    "# 0 to suppress output in batches.\n",
    "KK_Screen = 1\n",
    "# Helps normalize covariance matrices.\n",
    "KK_PriorPoint = 1\n",
    "# Outputs number of initial clusters.\n",
    "KK_SplitInfo = 1\n",
    "\n",
    "#No Ram Limit\n",
    "KK_RamLimitGB = -1\n",
    "    \"\"\"%PlaceHolders\n",
    "    try:\n",
    "        with open(output_name,'w') as f:\n",
    "            f.write(prm_content)\n",
    "            print(\"PRM file created!\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"PRM file failed!\")\n",
    "        os.remove(output_name)\n",
    "        return False\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "# if \"__file__\" not in dir():\n",
    "#     FilePath=\"/data/SWI0022/3/2016-09-27_15-47-38/\"\n",
    "#     Exp='experiment1_100'\n",
    "#     Fs=30000\n",
    "#     nCh=27\n",
    "    \n",
    "#     save_prm_file(FilePath,Exp,Fs,nCh,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read and convert .continuous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFolderToArray(folderpath, channels = 'all', dtype = float, source = '100'):\n",
    "    '''Load CH continuous files in specified folder to a single numpy array. By default all \n",
    "    CH continous files are loaded in numerical order, ordering can be specified with\n",
    "    optional channels argument which should be a list of channel numbers.'''\n",
    "\n",
    "    if channels == 'all': \n",
    "        channels = _get_sorted_channels(folderpath,'_CH')\n",
    "        aux      = _get_sorted_channels(folderpath,'_AUX')\n",
    "\n",
    "    filelist = [source + '_CH' + x + '.continuous' for x in map(str,channels)]\n",
    "    filelist.extend([source + '_AUX' + x + '.continuous' for x in map(str,aux)])\n",
    "    numFiles = 1\n",
    "\n",
    "    print(\"Loading continuous files...\")\n",
    "    channel_1_data = loadContinuous(os.path.join(folderpath, filelist[0]), dtype)['data']\n",
    "\n",
    "    n_samples  = len(channel_1_data)\n",
    "    n_channels = len(filelist)\n",
    "\n",
    "    data_array = np.zeros([n_samples, n_channels], dtype)\n",
    "    data_array[:,0] = channel_1_data\n",
    "\n",
    "    for i, f in enumerate(filelist[1:]):\n",
    "            data_array[:, i + 1] = loadContinuous(os.path.join(folderpath, f), dtype)['data']\n",
    "            numFiles += 1\n",
    "           \n",
    "    return data_array\n",
    "\n",
    "def loadContinuous(filepath, dtype = float):\n",
    "\n",
    "    assert dtype in (float, np.int16), \\\n",
    "      'Invalid data type specified for loadContinous, valid types are float and np.int16'\n",
    "\n",
    "\n",
    "    ch = { }\n",
    "    recordNumber = np.intp(-1)\n",
    "    \n",
    "    samples = np.zeros(MAX_NUMBER_OF_CONTINUOUS_SAMPLES, dtype)\n",
    "    timestamps = np.zeros(MAX_NUMBER_OF_RECORDS)\n",
    "    recordingNumbers = np.zeros(MAX_NUMBER_OF_RECORDS)\n",
    "    indices = np.arange(0,MAX_NUMBER_OF_RECORDS*SAMPLES_PER_RECORD, SAMPLES_PER_RECORD, np.dtype(np.int64))\n",
    "    \n",
    "    #read in the data\n",
    "    f = open(filepath,'rb')\n",
    "    \n",
    "    header = readHeader(f)\n",
    "    \n",
    "    fileLength = os.fstat(f.fileno()).st_size\n",
    "   \n",
    "    while f.tell() < fileLength:\n",
    "        \n",
    "        recordNumber += 1        \n",
    "        \n",
    "        timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer \n",
    "        N = np.fromfile(f,np.dtype('<u2'),1)[0] # little-endian 16-bit unsigned integer\n",
    "        \n",
    "        #print index\n",
    "\n",
    "        if N != SAMPLES_PER_RECORD:\n",
    "            raise Exception('Found corrupted record in block ' + str(recordNumber))\n",
    "        \n",
    "        recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n",
    "        \n",
    "        if dtype == float: # Convert data to float array and convert bits to voltage.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N) * float(header['bitVolts']) # big-endian 16-bit signed integer, multiplied by bitVolts   \n",
    "        else:  # Keep data in signed 16 bit integer format.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N)  # big-endian 16-bit signed integer\n",
    "        try:\n",
    "            samples[indices[recordNumber]:indices[recordNumber+1]] = data            \n",
    "        except Exception as e:\n",
    "            print(\"error reading \",filepath)\n",
    "            print(repr(e))\n",
    "            print(\"replacing missing values with zeros.\")\n",
    "            #raise\n",
    "        \n",
    "        marker = f.read(10) # dump\n",
    "        \n",
    "        \n",
    "    ch['header'] = header \n",
    "    ch['timestamps'] = timestamps[0:recordNumber]\n",
    "    ch['data'] = samples[0:indices[recordNumber]]  # OR use downsample(samples,1), to save space\n",
    "    ch['recordingNumber'] = recordingNumbers[0:recordNumber]\n",
    "    f.close()\n",
    "    return ch\n",
    "    \n",
    "def readHeader(f):\n",
    "    header = { }\n",
    "    h = f.read(1024).decode().replace('\\n','').replace('header.','')\n",
    "    for i,item in enumerate(h.split(';')):\n",
    "        if '=' in item:\n",
    "            header[item.split(' = ')[0]] = item.split(' = ')[1]\n",
    "    return header\n",
    "    \n",
    "def _get_sorted_channels(folderpath,sep='_CH'):\n",
    "    return sorted([int(f.split(sep)[1].split('.')[0]) for f in os.listdir(folderpath) \n",
    "                    if '.continuous' in f and sep in f]) \n",
    "\n",
    "\n",
    "\n",
    "# constants\n",
    "NUM_HEADER_BYTES = 1024\n",
    "SAMPLES_PER_RECORD = 1024\n",
    "RECORD_SIZE = 8 + 16 + SAMPLES_PER_RECORD*2 + 10 # size of each continuous record in bytes\n",
    "RECORD_MARKER = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 255])\n",
    "\n",
    "# constants for pre-allocating matrices:\n",
    "MAX_NUMBER_OF_SPIKES = int(1e6)\n",
    "MAX_NUMBER_OF_RECORDS = int(1e6)\n",
    "MAX_NUMBER_OF_CONTINUOUS_SAMPLES = int(1e8)\n",
    "MAX_NUMBER_OF_EVENTS = int(1e6)\n",
    "\n",
    "def pack_continuous_files(folderpath, filename = 'openephys.dat', source='100', channels = 'all', dref = None):\n",
    "\n",
    "    '''Alternative version of pack which uses numpy's tofile function to write data.\n",
    "    pack_2 is much faster than pack and avoids quantization noise incurred in pack due\n",
    "    to conversion of data to float voltages during loadContinous followed by rounding\n",
    "    back to integers for packing.  \n",
    "    source: string name of the source that openephys uses as the prefix. It is usually 100, \n",
    "            if the headstage is the first source added, but can specify something different.\n",
    "    channels:  List of channel numbers specifying order in which channels are packed. By default\n",
    "               all CH continous files are packed in numerical order.\n",
    "    dref:  Digital referencing - either supply a channel number or 'ave' to reference to the \n",
    "           average of packed channels.\n",
    "    '''\n",
    "\n",
    "    data_array = loadFolderToArray(folderpath, channels, np.int16, source)\n",
    "\n",
    "    if dref: \n",
    "        if dref == 'ave':\n",
    "            print('Digital referencing to average of all channels.')\n",
    "            reference = np.mean(data_array,1)\n",
    "        else:\n",
    "            print('Digital referencing to channel ' + str(dref))\n",
    "            if channels == 'all': \n",
    "                channels = _get_sorted_channels(folderpath)\n",
    "            reference = deepcopy(data_array[:,channels.index(dref)])\n",
    "        for i in range(data_array.shape[1]):\n",
    "            data_array[:,i] = data_array[:,i] - reference\n",
    "\n",
    "    print('Packing data to file: ' + filename)\n",
    "    data_array.tofile(os.path.join(folderpath,filename))\n",
    "    print(\".dat file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to create .eeg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eeg(inputName,nChannels=None,downsample=24,inputSamplingRate=30000):\n",
    "    '''\n",
    "    downsample the input file (raw.kwd or dat) and save it in .eeg\n",
    "    eegData= rawData[::16,:] (save every 16 point for each channel)\n",
    "    '''\n",
    "    downsample=int(downsample)\n",
    "    if inputName.endswith(\".raw.kwd\"):\n",
    "        raw_data=KwdRawDataReader(inputName)\n",
    "        outputName=inputName[:-7]+\"eeg\"\n",
    "    elif inputName.endswith(\".dat\"):\n",
    "        raw_data=DatRawDataReader(inputName,nChannels=nChannels)\n",
    "        outputName=inputName[:-3]+\"eeg\"\n",
    "        if nChannels is None:\n",
    "            print(\"Error in create_eeg: nChannel snot defined, can't read .dat\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Error in create_eeg: input file doesn't end with '.raw.kwd' or '.dat'\")\n",
    "        return False\n",
    "    \n",
    "    chunk_size=inputSamplingRate # 1 second\n",
    "    with open(outputName,'wb') as output:\n",
    "        for chunk in raw_data.chunks(chunk_size=chunk_size):\n",
    "            chunk_raw = chunk.data_chunk_full # shape: (nsamples, nchannels)\n",
    "            chunk_test=chunk_raw[::downsample,:]\n",
    "            for k in range(len(chunk_test)):\n",
    "                newFileByteArray=bytearray(chunk_test[k])\n",
    "                output.write(newFileByteArray)\n",
    "    print('\\n.eeg file created at: %s'%(outputName))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to save .dat, .prm, and .eeg files as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class safe_copy_from_nas:\n",
    "    \n",
    "    def __init__(self,path):\n",
    "        assert isinstance(path,str)\n",
    "        self.filePath=path\n",
    "    \n",
    "    def start(self,tempName='.NASqwrytdvn2u1r'):\n",
    "        if \"NAS\" in self.filePath:\n",
    "            print(\"Copying from NAS...\")\n",
    "            defaultPath=os.path.expanduser(\"~\") #home folder\n",
    "            self.tempdir=os.path.join(defaultPath,tempName)\n",
    "            try:\n",
    "                os.mkdir(self.tempdir)\n",
    "            except FileExistsError: #in case of multiple files being copied the directory already exists\n",
    "                pass\n",
    "            try:\n",
    "                self.newPath=copy(self.filePath,self.tempdir)\n",
    "            except Exception as e:\n",
    "                print(\"could not copy from NAS to local drive!\")\n",
    "                print(self.filePath)\n",
    "                print(repr(e))\n",
    "                self.newPath=\"\"\n",
    "        else:\n",
    "            self.newPath=self.filePath\n",
    "        return self.newPath\n",
    "    \n",
    "    def stop(self,fileTypes=['.prm','.dat','.eeg']):\n",
    "        if \"NAS\" in self.filePath:\n",
    "            files=find_file(self.tempdir,fileTypes)             \n",
    "            try:    \n",
    "                for newfile in files:\n",
    "                    copy(newfile,os.path.dirname(self.filePath))\n",
    "                    print(\"Uploaded to NAS!\")\n",
    "            except Exception as e:  #Exceptions should not be raised in this level!\n",
    "                print(\"upload to NAS failed!\")\n",
    "                print(repr(e))\n",
    "            finally:\n",
    "                #remove everything, ignore errors\n",
    "                rmtree(self.tempdir,ignore_errors=True)\n",
    "        \n",
    "\n",
    "def dat_prm_eeg_file_save_batch(path,animalList,overwrite=False,saveEEG=False):\n",
    "    for animal in animalList:\n",
    "        animalFolder=os.path.join(path,animal)\n",
    "        rawfile_list = find_file(animalFolder,['.raw.kwd'])\n",
    "        for rawfile in rawfile_list:\n",
    "            if os.path.exists(rawfile[:-7]+\"dat\") and os.path.exists(rawfile[:-7]+\"prm\") and not overwrite:\n",
    "                print(\".dat file already exists, skipping this session.\")\n",
    "                continue\n",
    "            safe_nas=safe_copy_from_nas(rawfile)\n",
    "            filePath=safe_nas.start()\n",
    "            try:\n",
    "                conversion_result= convert_kwd_to_dat(filePath,overwrite=overwrite)\n",
    "                if isinstance(conversion_result,bool):\n",
    "                    print(\".raw.kwd to .dat conversion failed:%s\"%(filePath))\n",
    "                else:\n",
    "                    nchannels, sampling_rate=conversion_result\n",
    "                    experiment=os.path.splitext(os.path.splitext(os.path.split(filePath)[1])[0])[0]\n",
    "                    save_prm_file(os.path.split(filePath)[0],experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                    if saveEEG:\n",
    "                        create_eeg(filePath,downsample=int(sampling_rate/1250),\n",
    "                                   inputSamplingRate=sampling_rate)\n",
    "            except Exception as e:\n",
    "                print('Converting .dat/ saving .prm/.eeg file failed at:')\n",
    "                print(filePath)\n",
    "                print(repr(e))\n",
    "            finally:\n",
    "                safe_nas.stop()\n",
    "        \n",
    "        #check existence of .continuous files\n",
    "        rawfile_list = find_file(animalFolder,['.continuous'])\n",
    "        folder_list = list(set([os.path.dirname(f) for f in rawfile_list]))\n",
    "        if len(folder_list) >0:\n",
    "            for continuousFolder in folder_list:\n",
    "                experiment=os.path.basename(continuousFolder)\n",
    "                filename= experiment + \".dat\"\n",
    "                settingsPath=os.path.join(continuousFolder,\"Continuous_Data.openephys\")\n",
    "                \n",
    "                if not os.path.exists(settingsPath):\n",
    "                    print (settingsPath)\n",
    "                    raise FileNotFoundError(\n",
    "                        \"There must be a \\\"Continuous_Data.openephys.xml\\\"\",\n",
    "                        \" file in the same folder as .continuous files to load the settings.\")\n",
    "                \n",
    "                with open(settingsPath,'r') as f:\n",
    "                    openEphysSettings=xmltodict.parse(f.read())\n",
    "                    openEphysSettings=openEphysSettings['EXPERIMENT']['RECORDING']\n",
    "                    if isinstance(openEphysSettings, list):\n",
    "                        openEphysSettings=openEphysSettings[0]\n",
    "                    sampling_rate =openEphysSettings['@samplerate']\n",
    "                    source        =openEphysSettings['PROCESSOR']['@id']\n",
    "                    nchannels     =len(openEphysSettings['PROCESSOR']['CHANNEL'])\n",
    "                    \n",
    "                    \n",
    "                #convert to and save as .DAT file\n",
    "                pack_continuous_files(continuousFolder, filename = filename, source=source, channels = 'all', dref = None)\n",
    "                #save PRM file\n",
    "                save_prm_file(continuousFolder,experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                if saveEEG:\n",
    "                    create_eeg(rawfile,downsample=int(sampling_rate/1250),\n",
    "                               inputSamplingRate=sampling_rate)        \n",
    "    print(\"\\nConverting/saving done\")\n",
    "    \n",
    "        \n",
    "#------------------------------------------------------------------------------------------\n",
    "# if \"__file__\" not in dir():\n",
    "#     Path=\"/data/SWI002/22/\"\n",
    "#     animalList=[]\n",
    "    \n",
    "#     dat_prm_eeg_file_save_batch(Path,animalList)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# Script to Rename and Convert as a Batch\n",
    "##### 'root' must be a directory containing seperate folders for each animal(ex: /data/ containing /data/Rat001, /data/Rat002, ...)\n",
    "##### 'animalList' determines on which folders within the 'root' this notebook will operate(ex: Rat001, Rat002, ...).\n",
    "##### renaming MUST precede converting/saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from NAS...\n",
      "\n",
      "\n",
      "openning /home/david/.NASqwrytdvn2u1r/Rat175_2018_04_16_16_03.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 20000Hz\n",
      "Converting...\n",
      "wrote /home/david/.NASqwrytdvn2u1r/Rat175_2018_04_16_16_03.dat\n",
      "done\n",
      "PRM file created!\n",
      "Uploaded to NAS!\n",
      "Uploaded to NAS!\n",
      "Copying from NAS...\n",
      "\n",
      "\n",
      "openning /home/david/.NASqwrytdvn2u1r/Rat175_2018_04_30_18_28.raw.kwd\n",
      "number of channels: 37\n",
      "sampling rate: 20000Hz\n",
      "Converting...\n",
      "wrote /home/david/.NASqwrytdvn2u1r/Rat175_2018_04_30_18_28.dat\n",
      "done\n",
      "PRM file created!\n",
      "Uploaded to NAS!\n",
      "Uploaded to NAS!\n",
      "\n",
      "Converting/saving done\n"
     ]
    }
   ],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    #The directory containing separate folders for each animal\n",
    "    root=\"/NAS02/\"#\"/NAS02/\"\n",
    "    #in Windows paths must be like this: root=\"C:\\\\Data\\\\Recordings\\\\\" (double backslash instead of single)\n",
    "    \n",
    "    #Animal where to run the script\n",
    "    animalList=[\"Rat174\"]\n",
    "    \n",
    "    #Whether to save .eeg files for each .raw.kwd file\n",
    "    saveEEG=False\n",
    "    \n",
    "    #Whether to overwrite the existing .dat files\n",
    "    overwrite=False\n",
    "\n",
    "    #Whether to print \"X renamed in Y\"\n",
    "    verbose=True  \n",
    "\n",
    "    #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "    # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "    minuteDelay=0\n",
    "\n",
    "    #Files to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "    #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "    extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwe\",\".eeg\"]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "#     rename_batch(root,animalList,verbose,minuteDelay,extensionList)\n",
    "    dat_prm_eeg_file_save_batch(root,animalList,overwrite,saveEEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "## Script to run KLUSTA as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_klusta_batch(path,animalList,fileTypes=['.dat','.prm','.prb'],overwrite=False):\n",
    "    \"\"\"\n",
    "    this function is almost general, klusta-specific lines are marked with #%klusta\n",
    "    \"\"\"\n",
    "    #%klusta\n",
    "    flag= '--overwrite' if overwrite else ''\n",
    "\n",
    "    for animal in animalList:\n",
    "        animalFolder=os.path.join(path,animal)\n",
    "        rawfile_list = find_file(animalFolder,[fileTypes[0]])\n",
    "        for rawfile in rawfile_list:\n",
    "            validFile=[os.path.exists(rawfile[:-len(fileTypes[0])] +f) for f in fileTypes]\n",
    "            #%klusta\n",
    "            validFile.append(not os.path.exists(rawfile[:-len(fileTypes[0])] +'.kwik'))\n",
    "            \n",
    "            if all(validFile) or overwrite:\n",
    "                safeFiles=[safe_copy_from_nas(rawfile[:-len(fileTypes[0])] +f) for f in fileTypes]\n",
    "                newFilePaths=[safeFile.start('.NAS2KLUSTAqwrytdvnz2u1r') for safeFile in safeFiles]\n",
    "                #%klusta\n",
    "                klustaCommand=\"\"\"\n",
    "                source activate klusta\n",
    "                cd {path}\n",
    "                klusta {prmFile} {ovr}\n",
    "                source deactivate klusta\n",
    "                \"\"\".format(path=os.path.dirname(newFilePaths[0]),\n",
    "                           prmFile=newFilePaths[0][:-len(fileTypes[0])]+'.prm',\n",
    "                           ovr=flag)\n",
    "                \n",
    "                klustaCommand.replace('\"', '')\n",
    "                klustaCommand.replace('\\'', '')\n",
    "                print(\"running spike sorting...\")\n",
    "                try:\n",
    "                    subprocess.run(klustaCommand,shell=True,check=True)\n",
    "                except Exception as e:\n",
    "                    \"failed in {}\".format(rawfile)\n",
    "                    print(repr(e))\n",
    "                finally:\n",
    "                    for f in safeFiles:\n",
    "                        #%klusta\n",
    "                        f.stop(fileTypes=['.kwik','.kwe','.kwx'])\n",
    "        \n",
    "    print(\"\\nConverting/saving done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying from NAS...\n",
      "Copying from NAS...\n",
      "Copying from NAS...\n",
      "running spike sorting...\n",
      "\n",
      "Converting/saving done\n"
     ]
    }
   ],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    #The directory containing separate folders for each animal\n",
    "    root=\"/NAS02/\"\n",
    "    #in Windows paths must be like this: root=\"C:\\\\Data\\\\Recordings\\\\\" (double backslash instead of single)\n",
    "    \n",
    "    #Animal where to run the script\n",
    "    animalList=[\"RatT03\"]\n",
    "    \n",
    "    #Whether to overwrite the existing .kwik file\n",
    "    overwrite=True\n",
    "    #--------------------------------------------------------------------------------\n",
    "    run_klusta_batch(path=root,animalList=animalList,fileTypes=['.dat','.prm','.prb'],overwrite=overwrite)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
