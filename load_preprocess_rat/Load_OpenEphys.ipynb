{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This notebook aims to convert Open Ephys output files (.raw.kwd) and folders to proper names and formats.\n",
    "# This notebook contains the following sections:\n",
    "\n",
    "## Rename Directories and files altogether\n",
    "## Convert .raw.kwd files and save .dat files\n",
    "## Save .prm files\n",
    "## Save .eeg files\n",
    "\n",
    "### Run all the cells one time then put the preferred values in the last cell and run it. It will call all the functions in the previous cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import  subprocess\n",
    "import tables as tb\n",
    "import glob\n",
    "from copy import deepcopy\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "from shutil import copy,rmtree\n",
    "\n",
    "if \"__file__\" not in dir():\n",
    "    %run UtilityTools.ipynb\n",
    "    %run Extract_Events.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming\n",
    "\n",
    "-Functions and classes required for renaming all the OpenEphys files and folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_folder(folderToRemove,newFolder):\n",
    "    '''\n",
    "    folderToRemove (=old), newFolder: full paths to folders\n",
    "    merge the folders if \"newFolder\" already exists\n",
    "    files are renamed (example: folderToRemove_1.txt becomes newFolder_1.txt)\n",
    "    '''\n",
    "    #if the new folder doesn't exist, create it\n",
    "    if not os.path.exists(newFolder):\n",
    "        os.mkdir(newFolder) \n",
    "    #move and rename the files \n",
    "    for f in os.listdir(folderToRemove):\n",
    "        oldpath=os.path.join(folderToRemove,f)\n",
    "        oldFolderName=os.path.basename(folderToRemove.rstrip(os.sep))\n",
    "        newFolderName=os.path.basename(newFolder.rstrip(os.sep))\n",
    "        f=f.replace(oldFolderName,newFolderName)\n",
    "        newpath=os.path.join(newFolder,f)\n",
    "        if os.path.exists(newpath):\n",
    "            os.remove(oldpath)\n",
    "        else:\n",
    "            os.rename(oldpath,newpath)\n",
    "    os.rmdir(folderToRemove)\n",
    "    \n",
    "def get_folders_matching_format(rootFolder,globFormat):\n",
    "    '''\n",
    "    rootFolder: full path where to look for session folders (ex: \"/data/Rat034/Experiments\")\n",
    "    '''\n",
    "    fullFormat=os.path.join(rootFolder,globFormat)\n",
    "    return [os.path.basename(f) for f in glob.glob(fullFormat)]\n",
    "\n",
    "def get_regular_name(wrongName,wrongTimeFormat,regularTimeFormat):\n",
    "    '''\n",
    "    from a session name in wrong format, return the regular format\n",
    "    '''\n",
    "    date=datetime.strptime(wrongName,wrongTimeFormat)\n",
    "    return date.strftime(regularTimeFormat)\n",
    "\n",
    "def get_date(name,timeFormat):\n",
    "    '''\n",
    "    return a datetime object from a session name and a format\n",
    "    '''\n",
    "    date=datetime.strptime(name,timeFormat)\n",
    "    return date\n",
    "\n",
    "def get_regular_name_from_date(date,regularTimeFormat):\n",
    "    '''\n",
    "    return a name given a date and a format\n",
    "    '''\n",
    "    return date.strftime(regularTimeFormat)\n",
    "\n",
    "def rename_all_in_rootFolder(rootFolder,wrongGlobFormat,wrongTimeFormat,regularGlobFormat,\n",
    "                             regularTimeFormat,minuteDelay=2,verbose=False):\n",
    "    '''\n",
    "    rootFolder: full path where to look for session folders (ex: \"/data/Rat034/Experiments\")\n",
    "    wrongGlobFormat: glob pattern to search for the wrong session names\n",
    "    wrongTimeFormat: datetime pattern to read date from wrong session names\n",
    "    regularGlobFormat, regularTimeFormat: glob and datetime patterns for the right session names\n",
    "    minuteDelay: if two folders have the same date (give or take minuteDelay), merge them\n",
    "    '''\n",
    "    delay=timedelta(minutes=minuteDelay)\n",
    "\n",
    "    #get names and date of the regular folders\n",
    "    regularFolders=get_folders_matching_format(rootFolder,regularGlobFormat)\n",
    "    allRegularDates=[get_date(name,regularTimeFormat) for name in regularFolders]\n",
    "    \n",
    "    #get names of wrong folders\n",
    "    fList=get_folders_matching_format(rootFolder,wrongGlobFormat)\n",
    "    for f in fList:\n",
    "        merge=False\n",
    "        #check if there is a regular folder around the same date\n",
    "        date=get_date(f,wrongTimeFormat)\n",
    "        for otherDate in allRegularDates:\n",
    "            if abs(date-otherDate)<delay:\n",
    "                date=otherDate\n",
    "                merge=True\n",
    "                break\n",
    "        #new name\n",
    "        newFolder=get_regular_name_from_date(date,regularTimeFormat)\n",
    "        #rename/merge\n",
    "        newPath=os.path.join(rootFolder,newFolder)\n",
    "        oldPath=os.path.join(rootFolder,f)\n",
    "        rename_folder(oldPath,newPath)\n",
    "        if verbose:\n",
    "            if merge:\n",
    "                print(\"Merged %s into %s\"%(f,newFolder))\n",
    "            else:\n",
    "                print(\"Renamed %s in %s\"%(f,newFolder))\n",
    "                \n",
    "def rename_files_to_match_folder(folderPath,extensionList):\n",
    "    '''\n",
    "    folderPath= \"/data/Rat034/Experiments/Rat034_2015_etc\"\n",
    "    extensionList= [\".dat\", \".prm\", \".kwik\"]\n",
    "    --> Renames \"someFile.dat\" into \"Rat024_2015_etc.dat\"\n",
    "    '''\n",
    "    folderName=os.path.basename(folderPath.rstrip(os.sep))\n",
    "    extensionList=[ext if ext.startswith(\".\") else \".\"+ext for ext in extensionList]\n",
    "    for f in os.listdir(folderPath):\n",
    "        for ext in extensionList:\n",
    "            if f.endswith(ext):\n",
    "                oldPath=os.path.join(folderPath,f)\n",
    "                newName=folderName+ext\n",
    "                newPath=os.path.join(folderPath,newName)\n",
    "                os.rename(oldPath,newPath)\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to rename the files and folders of OpenEphys as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_batch(root,animalList,verbose,minuteDelay,extensionList):\n",
    "    \n",
    "    WRONG_FORMATS={\n",
    "    \"openEphys\":(\n",
    "        \"animal_20{0}-{0}-{0}_{0}-{0}-{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2022-02-21_11-28-17'\n",
    "        \"animal_%Y-%m-%d_%H-%M-%S\"\n",
    "    ),\n",
    "    \"seconds\":(\n",
    "        \"animal_20{0}_{0}_{0}_{0}_{0}_{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2013_12_26_20_47_54'\n",
    "        \"animal_%Y_%m_%d_%H_%M_%S\"\n",
    "    ),\n",
    "    \"tiret\":(\n",
    "        \"animal_20{0}_{0}_{0}-{0}_{0}\".format(\"[00-99]?\"),        #ex: 'Rat001_2039_10_15-06_27'\n",
    "        \"animal_%Y_%m_%d-%H_%M\"\n",
    "    ),\n",
    "    \"tiretSeconds\":(\n",
    "        \"animal_20{0}_{0}_{0}-{0}_{0}_{0}\".format(\"[00-99]?\"),    #ex: 'Rat001_2033_06_23-21_24_46'\n",
    "        \"animal_%Y_%m_%d-%H_%M_%S\"\n",
    "    ),\n",
    "    \"openEphys2\":(\n",
    "        \"20{0}-{0}-{0}_{0}-{0}-{0}\".format(\"[00-99]?\"),           #ex: '2015-02-15_11-59-48'\n",
    "        \"%Y-%m-%d_%H-%M-%S\"\n",
    "    ),\n",
    "    }\n",
    "    # The right format \n",
    "    REGULAR_FORMAT=(\"animal_20{0}_{0}_{0}_{0}_{0}\".format(\"?[00-99]\"),  #ex: 'Rat001_2039_10_15_06_27'\n",
    "                \"animal_%Y_%m_%d_%H_%M\")\n",
    "      \n",
    "    for animal in animalList:\n",
    "        rawFileList=find_file(os.path.join(root,animal),extensionList)\n",
    "        rootFolderList=[os.path.split(os.path.split(rawFile)[0])[0] for rawFile in rawFileList]\n",
    "        #rootFolderList keeps paths to the directories within 'root' which contain session folders\n",
    "        rootFolderList=list(set(rootFolderList))#to keep unique members\n",
    "        \n",
    "        for rootFolder in rootFolderList:\n",
    "            print(\"animal %s: %s\"%(animal,rootFolder))\n",
    "    \n",
    "            #RENAME\n",
    "            regularGlob=REGULAR_FORMAT[0].replace(\"animal\",animal)\n",
    "            regularTime=REGULAR_FORMAT[1].replace(\"animal\",animal)\n",
    "            for name in WRONG_FORMATS:\n",
    "                globFormat=WRONG_FORMATS[name][0].replace(\"animal\",animal)\n",
    "                timeFormat=WRONG_FORMATS[name][1].replace(\"animal\",animal)       \n",
    "                #print(get_folders_matching_format(rootFolder,globFormat))\n",
    "                rename_all_in_rootFolder(rootFolder,globFormat,timeFormat,regularGlob,regularTime,\n",
    "                                         minuteDelay=minuteDelay,verbose=verbose)\n",
    "    \n",
    "            #CONVERT\n",
    "            #for every session folder of the animal\n",
    "            for folder in os.listdir(rootFolder):\n",
    "                if not folder.startswith(animal):\n",
    "                    continue\n",
    "                print(\"*\"+folder)\n",
    "                path=os.path.join(rootFolder,folder)\n",
    "                #rename files\n",
    "                rename_files_to_match_folder(path,extensionList)\n",
    "\n",
    "                for f in os.listdir(path):\n",
    "                    fpath=os.path.join(path,f)\n",
    "                    #convert raw.kwd\n",
    "                    if f==\"settings.xml\":\n",
    "                        newPath=os.path.join(path,\"settingsOpenEphys.xml\")\n",
    "                        os.rename(fpath,newPath)\n",
    "    \n",
    "            print(\"\\nRenaming done\")\n",
    "            print(\"--------\")\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# if \"__file__\" not in dir():\n",
    "#     root=\"/data/SWI002/22/\"\n",
    "#     #Animal where to run the script\n",
    "#     animalList=[\"SWI002\"]\n",
    "\n",
    "#     #Whether to print \"X renamed in Y\"\n",
    "#     verbose=True  \n",
    "\n",
    "#     #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "#     # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "#     minuteDelay=0\n",
    "\n",
    "#     #File to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "#     #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "#     extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwx\",\".kwe\",\".eeg\"]\n",
    "\n",
    "#     #--------------------------------------------------------------------------------\n",
    "#     rename_batch(root,animalList,verbose,minuteDelay,extensionList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "c5df7411-916f-4b42-85e5-c42b806ff1dd"
    }
   },
   "source": [
    "## Converting\n",
    "-Required functions and classes to open .raw.kwd file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbpresent": {
     "id": "b23eca88-6789-483b-9d4b-538e8bed2682"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------- chunck class from Klusta\n",
    "# reads data chunk by chunk (instead of all at once, as the files are big)\n",
    "class Chunk(object):\n",
    "    def __init__(self, data=None, nsamples=None, nchannels=None,\n",
    "                 bounds=None, dtype=None, recording=0, nrecordings=1):\n",
    "        self._data = data\n",
    "        if nsamples is None and nchannels is None:\n",
    "            nsamples, nchannels = data.shape\n",
    "        self.nsamples = nsamples\n",
    "        self.nchannels = nchannels\n",
    "        self.dtype = dtype\n",
    "        self.recording = recording\n",
    "        self.nrecordings = nrecordings\n",
    "        self.s_start, self.s_end, self.keep_start, self.keep_end = bounds\n",
    "        self.window_full = self.s_start, self.s_end\n",
    "        self.window_keep = self.keep_start, self.keep_end      \n",
    "    @property\n",
    "    def data_chunk_full(self):\n",
    "        chunk = self._data[self.s_start:self.s_end,:]\n",
    "        return convert_dtype(chunk, self.dtype)   \n",
    "    @property\n",
    "    def data_chunk_keep(self):\n",
    "        chunk =  self._data[self.keep_start:self.keep_end,:]\n",
    "        return convert_dtype(chunk, self.dtype)\n",
    "\n",
    "def chunk_bounds(nsamples, chunk_size, overlap=0):\n",
    "    s_start = 0\n",
    "    s_end = chunk_size\n",
    "    keep_start = s_start\n",
    "    keep_end = s_end - overlap // 2\n",
    "    yield s_start, s_end, keep_start, keep_end    \n",
    "    while s_end - overlap + chunk_size < nsamples:\n",
    "        s_start = s_end - overlap\n",
    "        s_end = s_start + chunk_size\n",
    "        keep_start = keep_end\n",
    "        keep_end = s_end - overlap // 2\n",
    "        if s_start < s_end:\n",
    "            yield s_start, s_end, keep_start, keep_end        \n",
    "    s_start = s_end - overlap\n",
    "    s_end = nsamples\n",
    "    keep_start = keep_end\n",
    "    keep_end = s_end\n",
    "    if s_start < s_end:\n",
    "        yield s_start, s_end, keep_start, keep_end\n",
    "\n",
    "def convert_dtype(data, dtype=None, factor=None):\n",
    "    if not dtype:\n",
    "        return data\n",
    "    if data.shape[0] == 0:\n",
    "        return data.astype(dtype)\n",
    "    dtype_old = data.dtype\n",
    "    if dtype_old == dtype:\n",
    "        return data\n",
    "    key = (_get_dtype(dtype_old), _get_dtype(dtype))\n",
    "    factor = factor or _dtype_factors.get(key, 1)\n",
    "    if dtype_old in (np.float32, np.float64):\n",
    "        factor = factor/np.abs(data).max()\n",
    "    if factor != 1:\n",
    "        return (data * factor).astype(dtype)\n",
    "    else:\n",
    "        return data.astype(dtype) \n",
    "#------------------------------------------------------------------------------ open a file, from kwiklib \n",
    "def open_file(path,mode=\"r\"):\n",
    "    try:\n",
    "        f = tb.open_file(path, mode)\n",
    "        return f\n",
    "    except IOError as e:\n",
    "        warn(\"IOError: \" + str(e.message))\n",
    "        return\n",
    "# ------------------------------------------------------------------- KWD and .dat data reader from Klusta\n",
    "class BaseRawDataReader(object):\n",
    "    def __init__(self, dtype_to=np.int16):\n",
    "        self.dtype_to = dtype_to\n",
    "    def next_recording(self):\n",
    "        for self.recording in range(self.nrecordings):\n",
    "            yield self.recording, self.get_recording_data(self.recording)    \n",
    "    def get_recording_data(self, recording):\n",
    "        raise NotImplementedError()    \n",
    "    def chunks(self, chunk_size=None, chunk_overlap=0):\n",
    "        for recording, data in self.next_recording():\n",
    "            assert chunk_size is not None, \"You need to specify a chunk size.\"\"\"\n",
    "            for bounds in chunk_bounds(data.shape[0], chunk_size=chunk_size, overlap=chunk_overlap):\n",
    "                yield Chunk(data, bounds=bounds, dtype=self.dtype_to, recording=recording, nrecordings=self.nrecordings)\n",
    "class KwdRawDataReader(BaseRawDataReader):\n",
    "    def __init__(self, filename, dtype_to=np.int16):\n",
    "        self.kwd = open_file(filename, 'r')\n",
    "        self.nrecordings = self.kwd.root.recordings._v_nchildren\n",
    "        super(KwdRawDataReader, self).__init__(dtype_to=dtype_to)        \n",
    "    def get_recording_data(self, recording):\n",
    "        data = self.kwd.root.recordings._f_get_child(str(recording)).data\n",
    "        return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to open the .raw.kwd file and save the .dat file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_kwd_to_dat(filename,outputname=None,overwrite=False):\n",
    "    samplingRateList=[10000,20000,25000,30000,15000]\n",
    "    #output file name\n",
    "    if outputname is None:\n",
    "        outputname=filename[:-7]+\"dat\"\n",
    "    out=os.path.basename(outputname)\n",
    "    \n",
    "    if os.path.exists(outputname) and not overwrite:\n",
    "        print(\".dat file already exists, skipping this session.\")\n",
    "        return False\n",
    "    \n",
    "    if filename.endswith('.raw.kwd'):\n",
    "        #read number of channels in kwd file\n",
    "        with tb.open_file(filename,\"r\") as kwd:\n",
    "            print(\"\\n\\nopenning %s\"%(filename))\n",
    "            dataLen,nchannels=kwd.get_node(\"/recordings/0/data\").read().shape\n",
    "            sampling_rate=kwd.get_node(\"/recordings/0/\")._v_attrs.__getitem__('sample_rate')\n",
    "        \n",
    "        if int(sampling_rate) not in samplingRateList:  #This condition avoids OpenEphys bug in writing the sampling rate value!\n",
    "            sampling_rate=20000\n",
    "        \n",
    "        if dataLen<nchannels:\n",
    "            dataLen,nchannels=nchannels,dataLen\n",
    "        if dataLen<sampling_rate:    #not an important condition! Could be ignored!\n",
    "            raise ValueError('Data Length too short')\n",
    "\n",
    "        print(\"number of channels: %s\"%(nchannels))\n",
    "        print(\"sampling rate: %sHz\"%(sampling_rate))\n",
    "        print(\"Converting...\")\n",
    "\n",
    "        if os.path.exists(outputname):\n",
    "            if overwrite:\n",
    "                print(\"\\n%s already exist, it will be overwritten\"%(out))\n",
    "            else:\n",
    "                print(\"\\n%s already exists, no conversion to do\"%(out))\n",
    "                return [nchannels, sampling_rate]\n",
    "        #instantiate reader\n",
    "        kwd_data=KwdRawDataReader(filename)\n",
    "    else:\n",
    "        print(\"Error: the raw data file doesn't end with '.raw.kwd'\")\n",
    "        return False\n",
    "\n",
    "    #To correct for single channel recording: add a second zero-value channel\n",
    "    patch=[]\n",
    "    if nchannels==1:\n",
    "        nchannels+=1\n",
    "        patch=[0]\n",
    "\n",
    "    #create dat file, open write only in binary    \n",
    "    try:\n",
    "        with open(outputname,'wb') as output:\n",
    "            chunk_size=sampling_rate\n",
    "            for chunk in kwd_data.chunks(chunk_size):\n",
    "                data=chunk.data_chunk_full\n",
    "                for i in range(len(data)):\n",
    "                    newFileByteArray=bytearray(np.append(data[i],patch).astype(data[i].dtype))\n",
    "                    output.write(newFileByteArray)\n",
    "        kwd_data.kwd.close()\n",
    "        print(\"wrote %s\"%(outputname))\n",
    "        print(\"done\")\n",
    "        return [nchannels, sampling_rate]\n",
    "    except Exception as e:\n",
    "        print(\"Dat file writing failed:\")\n",
    "        print(repr(e))\n",
    "        os.remove(outputname)\n",
    "        return False\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------\n",
    "# if \"__file__\" not in dir():\n",
    "#     inputFile=\"\"\n",
    "#     deleteKWD=False\n",
    "#     overwrite=False\n",
    "\n",
    "#     convert_kwd_to_dat(inputFile,overwrite=overwrite)\n",
    "\n",
    "#     if deleteKWD and os.path.exists(inputFile) and False:\n",
    "#         print(\"Delete %s\"%os.path.basename(inputFile))\n",
    "#         os.remove(inputFile)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Save the .prm file with correct experiment name, sampling rate and channel number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_prm_file(filepath,experiment,sampling_rate,n_channels,overwrite=True):\n",
    "    output_name=os.path.join(filepath,experiment+\".prm\")\n",
    "    if os.path.exists(output_name):\n",
    "        if overwrite:\n",
    "            print(\"%s already exists, it will be overwritten\"%(output_name))\n",
    "        else:\n",
    "            print(\"%s already exists, no conversion to do\"%(output_name))\n",
    "            return False\n",
    "    \n",
    "    PlaceHolders={\n",
    "        'exp_name':experiment,\n",
    "        'Fs':str(sampling_rate),\n",
    "        'Nch':str(n_channels),\n",
    "    }\n",
    "    prm_content=\"\"\"experiment_name = '%(exp_name)s'\n",
    "prb_file = experiment_name + '.prb'\n",
    "\n",
    "traces=dict(\n",
    "    raw_data_files  = [experiment_name + '.dat'],\n",
    "    voltage_gain    = 10,\n",
    "    nbits           = 16,\n",
    "    dtype           = 'int16',\n",
    "    sample_rate     = %(Fs)s,\n",
    "    n_channels      = %(Nch)s,\n",
    "    )\n",
    "\n",
    "nbits          = 16\n",
    "voltage_gain   = traces['voltage_gain']\n",
    "sample_rate    = traces['sample_rate']\n",
    "nchannels      = traces['n_channels']\n",
    "\n",
    "spikedetekt=dict(\n",
    "    #######################################################################\n",
    "    # SpikeDetekt parameters\n",
    "    #######################################################################\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Raw data filtering and saving\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Whether to save the .raw.kwd file if a non-HDF5 raw data format is used.\n",
    "    # This is needed to visualise the data in TraceView etc, and speeds up\n",
    "    # future runs of SpikeDetekt. If a .raw.kwd file is used as the input,\n",
    "    # it will never be overwritten.\n",
    "    save_raw = True,\n",
    "    # Whether to save the .high.kwd file with HPF data used for spike\n",
    "    # detection. This is processed using a Butterworth band-pass filter.\n",
    "    save_high = False,\n",
    "    # Bandpass filter low corner frequency\n",
    "    filter_low = 500.,\n",
    "    # Bandpass filter high corner frequency\n",
    "    filter_high = 0.95 * .5 * sample_rate,\n",
    "    # Order of Butterworth filter.\n",
    "    filter_butter_order = 3,\n",
    "    # Whether to save a .low.kwd file; this is processed using a Hamming\n",
    "    # window FIR filter, then subsampled 16x to save space when storing.\n",
    "    save_low = False,\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # Chunks\n",
    "    # ---------------------------------------------------------------------\n",
    "    # SpikeDetekt processes the raw data in chunks with small overlaps to\n",
    "    # catch spikes which would otherwise span two chunks. These options\n",
    "    # will change the default chunk size and overlap.\n",
    "    chunk_size = int(1. * sample_rate), # 1 second\n",
    "    chunk_overlap = int(.015 * sample_rate), # 15 ms\n",
    "\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Threshold setting for spike detection\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Change this to 'positive' to detect positive spikes.\n",
    "    detect_spikes = 'negative',\n",
    "    # SpikeDetekt takes a set of uniformly distributed chunks throughout\n",
    "    # the high-pass filtered data to estimate its standard deviation. These\n",
    "    # parameters select how many excerpts are used and how long each of them are.\n",
    "    nexcerpts = 50,\n",
    "    excerpt_size = int(1. * sample_rate), # 1 second\n",
    "    # This is then used to calculate a base threshold which is multiplied\n",
    "    # by the two parameters below for the two-threshold detection process.\n",
    "    threshold_strong_std_factor = 4.5,\n",
    "    threshold_weak_std_factor = 2.,\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Spike extraction\n",
    "    # ---------------------------------------------------------------------\n",
    "    # The number of samples to extract before and after the centre of the\n",
    "    # spike for waveforms. Then, waveforms_nsamples is calculated using the\n",
    "    # formula: waveforms_nsamples = extract_s_before + extract_s_after\n",
    "    extract_s_before = int(0.0008* sample_rate),\n",
    "    extract_s_after  = int(0.0008* sample_rate),\n",
    "\n",
    "    #---------------------------------------------------------------------\n",
    "    # Features\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Number of features (PCs) per channel.\n",
    "    nfeatures_per_channel = 3,\n",
    "    # The number of spikes used to determine the PCs\n",
    "    pca_nwaveforms_max = 10000,\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Advanced\n",
    "    # ---------------------------------------------------------------------\n",
    "    # Number of samples to use in floodfill algorithm for spike detection\n",
    "    #connected_component_join_size = 1, # 1 sample\n",
    "    connected_component_join_size = int(.00005 * sample_rate), # 0.05ms\n",
    "    # Waveform alignment\n",
    "    weight_power = 2,\n",
    "    # Whether to make the features array contiguous\n",
    "    features_contiguous = True,\n",
    "    )\n",
    "\n",
    "#Mostafa: Don't know if the following section is working and what does it do?!\n",
    "##############################################################\n",
    "# KlustaKwik parameters (must be prefixed by KK_). Uncomment to override\n",
    "# the defaults, which can be shown by running 'klustakwik' with no options\n",
    "###############################################################\n",
    "\n",
    "# This causes KlustaKwik to perform clustering on a subset of spikes and\n",
    "# estimate the assignment of the other spikes. This causes a speedup in\n",
    "# computational time (by a rough factor of KK_Subset), though will not\n",
    "# significantly decrease RAM usage. For long runs where you are unsure of\n",
    "# the data quality, you can first use KK_Subset = 50 to check the\n",
    "# clustering quality before performing a Subset 1 (all spikes) run.\n",
    "KK_Subset = 1\n",
    "\n",
    "# The largest permitted number of clusters, so cluster splitting can produce\n",
    "# no more than n clusters. Note: This must be set higher than MaskStarts.\n",
    "KK_MaxPossibleClusters = 1000\n",
    "\n",
    "# Maximum number of iterations. ie. it won't try more than n iterations\n",
    "# from any starting point.\n",
    "KK_MaxIter = 10000\n",
    "\n",
    "# You can start with a chosen fixed number of clusters derived from the\n",
    "# mask vectors, set by KK_MaskStarts.\n",
    "KK_MaskStarts = 500\n",
    "\n",
    "# The number of iterations after which KlustaKwik first attempts to split\n",
    "# existing clusters. KlustaKwik then splits every SplitEvery iterations.\n",
    "KK_SplitFirst = 20\n",
    "\n",
    "# The number of iterations after which KlustaKwik attempts to split existing\n",
    "# clusters. When using masked initializations, to save time due to excessive\n",
    "# splitting, set SplitEvery to a large number, close to the number of distinct\n",
    "# masks or the number of chosen starting masks.\n",
    "KK_SplitEvery = 40\n",
    "\n",
    "# KlustaKwik uses penalties to reduce the number of clusters fit. The parameters PenaltyK and PenaltyKLogN are \n",
    "# given positive values. The higher the values, the fewer clusters you obtain. Higher penalties\n",
    "# discourage cluster splitting. PenaltyKLogN also increases penalty when there are more points. \n",
    "#-PenaltyK 0 -PenaltyKLogN 1 is the default, corresponding to the \"Bayesian Information Criterion\".\n",
    "# -PenaltyK 1 -PenaltyKLogN 0 corresponds to \"Akaike's Information Criterion\". This produces a larger number \n",
    "# of clusters, and is recommended if you are find that clusters corresponding to different neurons are incorrectly merged.\n",
    "KK_PenaltyK = 0.\n",
    "KK_PenaltyKLogN = 1.\n",
    "\n",
    "# Specifies a seed for the random number generator.\n",
    "KK_RandomSeed = 1\n",
    "\n",
    "# The number of unmasked spikes on a certain channel needed to unmask that\n",
    "# channel in the cluster. This prevents a single noisy spike, or coincident\n",
    "# noise on adjacent channels from slowing down computation time.\n",
    "KK_PointsForClusterMask = 10\n",
    "\n",
    "# Setting this saves a .temp.clu file every iteration. This slows the runtime\n",
    "# down reasonably significantly for small runs with many iterations, but allows\n",
    "# to recover where KlustaKwik left off; useful in case of large runs where you\n",
    "# are not confident that the run will be uninterrupted.\n",
    "KK_SaveTempCluEveryIter = 0\n",
    "\n",
    "# This is an integer N when, used in combination with the empty string\n",
    "# for UseFeatures above, omits the last N features. This should always\n",
    "# be used with KK_UseFeatures = \"\"\n",
    "KK_DropLastNFeatures = 0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Classic 'all channels unmasked always' mode | DO NOT uncomment\n",
    "# ---------------------------------------------------------------------\n",
    "# To use KlustaKwik in \"unmasked\" mode, set this to 0.\n",
    "# This disables the use of the new `masked Expectation-Maximization'\n",
    "# algorithm, and sets all the channels to be unmasked on all spikes.\n",
    "#KK_UseDistributional = 1\n",
    "\n",
    "# In classic mode, KlustaKwik starts from random cluster assignments,\n",
    "# running a new random start for every integer between MinClusters and\n",
    "# MaxClusters. For these values to take effect, MaskStarts must be set to 0.\n",
    "#KK_MinClusters = 100\n",
    "#KK_MaxClusters = 110\n",
    "\n",
    "# By default, this is an empty string, which means 'use all features'.\n",
    "# Or, you can you can specify a string with 1's for features you want to\n",
    "# use, and 0's for features you don't want to use. In classic mode,\n",
    "# you use this option to take out bad channels. In masked mode,\n",
    "# you should instead take bad channels out from the .PRB file.\n",
    "#KK_UseFeatures = \"\"\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Advanced\n",
    "# ---------------------------------------------------------------------\n",
    "# The algorithm will be started n times for each initial cluster count\n",
    "# between MinClusters and MaxClusters.\n",
    "KK_nStarts = 1\n",
    "\n",
    "# Saves means and covariance matrices. Stops computation at each iteration.\n",
    "# Manual input required for continuation.\n",
    "KK_SaveCovarianceMeans = 0\n",
    "\n",
    "# Saves a .clu file with masks sorted lexicographically.\n",
    "KK_SaveSorted = 0\n",
    "\n",
    "# Initialises using distinct derived binary masks. Use together with\n",
    "# AssignToFirstClosestMask below.\n",
    "KK_UseMaskedInitialConditions = 0\n",
    "\n",
    "# If starting with a number of clusters fewer than the number of distinct\n",
    "# derived binary masks, it will assign the rest of the points to the cluster\n",
    "# with the nearest mask.\n",
    "KK_AssignToFirstClosestMask = 0\n",
    "\n",
    "# All log-likelihoods are recalculated every KK_FullStepEvery steps\n",
    "# (see DistThresh).\n",
    "KK_FullStepEvery = 20\n",
    "KK_MinMaskOverlap = 0.\n",
    "KK_AlwaysSplitBimodal = 0\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# Debugging\n",
    "# ---------------------------------------------------------------------\n",
    "# Turns miscellaneous debugging information on.\n",
    "KK_Debug = 0\n",
    "# Increasing this to 2 increases the amount of information logged to\n",
    "# the console and the log.\n",
    "KK_Verbose = 1\n",
    "# Outputs more debugging information.\n",
    "KK_DistDump = 0\n",
    "# Time-saving parameter. If a point has log likelihood more than\n",
    "# DistThresh worse for a given class than for the best class, the log\n",
    "# likelihood for that class is not recalculated. This saves an awful lot\n",
    "# of time.\n",
    "KK_DistThresh = 6.907755\n",
    "# All log-likelihoods are recalculated if the fraction of instances\n",
    "# changing class exceeds ChangedThresh (see DistThresh).\n",
    "KK_ChangedThresh = 0.05\n",
    "# Produces .klg log file (default is yes, to switch off do -Log 0).\n",
    "KK_Log = 1\n",
    "# Produces parameters and progress information on the console. Set to\n",
    "# 0 to suppress output in batches.\n",
    "KK_Screen = 1\n",
    "# Helps normalize covariance matrices.\n",
    "KK_PriorPoint = 1\n",
    "# Outputs number of initial clusters.\n",
    "KK_SplitInfo = 1\n",
    "\n",
    "#No Ram Limit\n",
    "KK_RamLimitGB = -1\n",
    "    \"\"\"%PlaceHolders\n",
    "    try:\n",
    "        with open(output_name,'w') as f:\n",
    "            f.write(prm_content)\n",
    "            print(\"PRM file created!\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"PRM file failed!\")\n",
    "        os.remove(output_name)\n",
    "        return False\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "# if \"__file__\" not in dir():\n",
    "#     FilePath=\"/data/SWI0022/3/2016-09-27_15-47-38/\"\n",
    "#     Exp='experiment1_100'\n",
    "#     Fs=30000\n",
    "#     nCh=27\n",
    "    \n",
    "#     save_prm_file(FilePath,Exp,Fs,nCh,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- saves the *.params file for SpyKING Circus use (future use!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_params_file(filepath,experiment,sampling_rate,n_channels,overwrite=True):\n",
    "    output_name=os.path.join(filepath,experiment+\".params\")\n",
    "    if os.path.exists(output_name):\n",
    "        if overwrite:\n",
    "            print(\"%s already exists, it will be overwritten\"%(output_name))\n",
    "        else:\n",
    "            print(\"%s already exists, no conversion to do\"%(output_name))\n",
    "            return False\n",
    "    \n",
    "    prm_content=f\"\"\"[data]\n",
    "file_format    = raw_binary                   # Can be raw_binary, openephys, hdf5, ... See >> spyking-circus help -i for more info\n",
    "stream_mode    = None                         # None by default. Can be multi-files, or anything depending to the file format\n",
    "mapping        = {experiment}.prb  # Mapping of the electrode (see http://spyking-circus.rtfd.ord)\n",
    "suffix         =                              # Suffix to add to generated files\n",
    "global_tmp     = False                        # should be False if local /tmp/ has enough space (better for clusters)\n",
    "overwrite      = False                        # Filter or remove artefacts on site (if write access is possible). Data are duplicated otherwise\n",
    "parallel_hdf5  = True                         # Use the parallel HDF5 feature (if available)\n",
    "data_dtype      = uint16\n",
    "sampling_rate  = {sampling_rate}\n",
    "nb_channles    = {n_channels}\n",
    "\n",
    "\n",
    "[detection]\n",
    "radius         = 100       # Radius [in um] (if auto, read from the prb file)\n",
    "N_t            = 5          # Width of the templates [in ms]\n",
    "spike_thresh   = 6          # Threshold for spike detection\n",
    "peaks          = negative   # Can be negative (default), positive or both\n",
    "matched-filter = False      # If True, we perform spike detection with matched filters\n",
    "matched_thresh = 5          # Threshold for detection if matched filter is True\n",
    "alignment      = True       # Realign the waveforms by oversampling\n",
    "\n",
    "[filtering]\n",
    "cut_off        = 300, auto  # Min and Max (auto=nyquist) cut off frequencies for the band pass butterworth filter [Hz]\n",
    "filter         = True       # If True, then a low-pass filtering is performed\n",
    "remove_median  = True      # If True, median over all channels is substracted to each channels (movement artifacts)\n",
    "\n",
    "[triggers]\n",
    "trig_file      =            # External stimuli to be considered as putative artefacts [in trig units] (see documentation)\n",
    "trig_windows   =            # The time windows of those external stimuli [in trig units]\n",
    "trig_unit      = ms         # The unit in which times are expressed: can be ms or timestep\n",
    "clean_artefact = False      # If True, external artefacts induced by triggers will be suppressed from data\n",
    "dead_file      =            # Portion of the signals that should be excluded from the analysis [in dead units]\n",
    "dead_unit      = ms         # The unit in which times for dead regions are expressed: can be ms or timestep\n",
    "ignore_times   = False      # If True, any spike in the dead regions will be ignored by the analysis\n",
    "make_plots     =            # Generate sanity plots of the averaged artefacts [Nothing or None if no plots]\n",
    "\n",
    "[whitening]\n",
    "chunk_size     = 30         # Size of the data chunks [in s]\n",
    "safety_time    = 1          # Temporal zone around which templates are isolated [in ms, or auto]\n",
    "temporal       = False      # Perform temporal whitening\n",
    "spatial        = True       # Perform spatial whitening\n",
    "max_elts       = 1000       # Max number of events per electrode (should be compatible with nb_elts)\n",
    "nb_elts        = 0.8        # Fraction of max_elts that should be obtained per electrode [0-1]\n",
    "output_dim     = 5          # Can be in percent of variance explain, or num of dimensions for PCA on waveforms\n",
    "\n",
    "[clustering]\n",
    "extraction     = median-raw # Can be either median-raw (default), median-pca, mean-pca, mean-raw\n",
    "safety_space   = True       # If True, we exclude spikes in the vicinity of a selected spikes\n",
    "safety_time    = auto       # Temporal zone around which templates are isolated [in ms, or auto]\n",
    "max_elts       = 10000      # Max number of events per electrode (should be compatible with nb_elts)\n",
    "nb_elts        = 0.8        # Fraction of max_elts that should be obtained per electrode [0-1]\n",
    "nclus_min      = 0.002      # Min number of elements in a cluster (given in percentage) [0-1]\n",
    "max_clusters   = 5          # Maximal number of clusters for every electrodes\n",
    "nb_repeats     = 3          # Number of passes used for the clustering\n",
    "smart_search   = True       # Activate the smart search mode\n",
    "smart_select   = False      # Experimental: activate the smart selection of centroids (max_clusters is ignored)\n",
    "sim_same_elec  = 3          # Distance within clusters under which they are re-merged\n",
    "cc_merge       = 0.975      # If CC between two templates is higher, they are merged\n",
    "dispersion     = (5, 5)     # Min and Max dispersion allowed for amplitudes [in MAD]\n",
    "noise_thr      = 0.8        # Minimal amplitudes are such than amp*min(templates) < noise_thr*threshold in [0-1]\n",
    "remove_mixture = True       # At the end of the clustering, we remove mixtures of templates\n",
    "make_plots     =            # Generate sanity plots of the clustering [Nothing or None if no plots]\n",
    "\n",
    "[fitting]\n",
    "chunk_size     = 1          # Size of chunks used during fitting [in second]\n",
    "gpu_only       = False      # Use GPU for computation of b's AND fitting [not optimized yet]\n",
    "amp_limits     = (0.3, 5)   # Amplitudes for the templates during spike detection [if not auto]\n",
    "amp_auto       = True       # True if amplitudes are adjusted automatically for every templates\n",
    "max_chunk      = inf        # Fit only up to max_chunk\n",
    "collect_all    = False      # If True, one garbage template per electrode is created, to store unfitted spikes\n",
    "\n",
    "[merging]\n",
    "cc_overlap     = 0.7        # Only templates with CC higher than cc_overlap may be merged\n",
    "cc_bin         = 2          # Bin size for computing CC [in ms]\n",
    "correct_lag    = True       # If spikes are aligned when merging. May be better for phy usage\n",
    "auto_mode      = 0          # If >0, merging will be automatic (see doc, 0.1 is a good value) [0-1]\n",
    "\n",
    "[converting]\n",
    "erase_all      = True       # If False, a prompt will ask you to export if export has already been done\n",
    "export_pcs     = all        # Can be prompt [default] or in none, all, some\n",
    "export_all     = False      # If True, unfitted spikes will be exported as the last Ne templates\n",
    "sparse_export  = True       # For recent versions of phy, and large number of templates/channels\n",
    "\n",
    "[validating]\n",
    "nearest_elec   = auto       # Validation channel (e.g. electrode closest to the ground truth cell)\n",
    "max_iter       = 200        # Maximum number of iterations of the stochastic gradient descent (SGD)\n",
    "learning_rate  = 1.0e-3     # Initial learning rate which controls the step-size of the SGD\n",
    "roc_sampling   = 10         # Number of points to estimate the ROC curve of the BEER estimate\n",
    "test_size      = 0.3        # Portion of the dataset to include in the test split\n",
    "radius_factor  = 0.5        # Radius factor to modulate physical radius during validation\n",
    "juxta_dtype    = uint16     # Type of the juxtacellular data\n",
    "juxta_thresh   = 6          # Threshold for juxtacellular detection\n",
    "juxta_valley   = False      # True if juxta-cellular spikes are negative peaks\n",
    "juxta_spikes   =            # If none, spikes are automatically detected based on juxta_thresh\n",
    "filter         = True       # If the juxta channel need to be filtered or not\n",
    "make_plots     = png        # Generate sanity plots of the validation [Nothing or None if no plots]\n",
    "\n",
    "[extracting]\n",
    "safety_time    = 1          # Temporal zone around which spikes are isolated [in ms]\n",
    "max_elts       = 1000       # Max number of collected events per templates\n",
    "output_dim     = 5          # Percentage of variance explained while performing PCA\n",
    "cc_merge       = 0.975      # If CC between two templates is higher, they are merged\n",
    "noise_thr      = 0.8        # Minimal amplitudes are such than amp*min(templates) < noise_thr*threshold\n",
    "\n",
    "[noedits]\n",
    "filter_done    = False      #!! AUTOMATICALLY EDITED: DO NOT MODIFY !!\n",
    "artefacts_done = False      # Will become True automatically after removing artefacts\n",
    "median_done    = False      #!! AUTOMATICALLY EDITED: DO NOT MODIFY !!\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(output_name,'w') as f:\n",
    "            f.write(prm_content)\n",
    "            print(\"PRM file created!\")\n",
    "            return True\n",
    "    except:\n",
    "        print(\"PRM file failed!\")\n",
    "        os.remove(output_name)\n",
    "        return False\n",
    "\n",
    "#---------------------------------------------------------------------------------------------\n",
    "\n",
    "# if \"__file__\" not in dir():\n",
    "#     FilePath=\"/data/RatTST/Experiments/RatTST_2018_02_25_14_36/\"\n",
    "#     Exp='RatTST_2018_02_25_14_36'\n",
    "#     Fs=20000\n",
    "#     nCh=37\n",
    "    \n",
    "#     save_params_file(FilePath,Exp,Fs,nCh,overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to read and convert .continuous files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadFolderToArray(folderpath, channels = 'all', dtype = float, source = '100'):\n",
    "    '''Load CH continuous files in specified folder to a single numpy array. By default all \n",
    "    CH continous files are loaded in numerical order, ordering can be specified with\n",
    "    optional channels argument which should be a list of channel numbers.'''\n",
    "\n",
    "    if channels == 'all': \n",
    "        channels = _get_sorted_channels(folderpath,'_CH')\n",
    "        aux      = _get_sorted_channels(folderpath,'_AUX')\n",
    "\n",
    "    filelist = [source + '_CH' + x + '.continuous' for x in map(str,channels)]\n",
    "    filelist.extend([source + '_AUX' + x + '.continuous' for x in map(str,aux)])\n",
    "    numFiles = 1\n",
    "\n",
    "    print(\"Loading continuous files...\")\n",
    "    channel_1_data = loadContinuous(os.path.join(folderpath, filelist[0]), dtype)['data']\n",
    "\n",
    "    n_samples  = len(channel_1_data)\n",
    "    n_channels = len(filelist)\n",
    "\n",
    "    data_array = np.zeros([n_samples, n_channels], dtype)\n",
    "    data_array[:,0] = channel_1_data\n",
    "\n",
    "    for i, f in enumerate(filelist[1:]):\n",
    "            data_array[:, i + 1] = loadContinuous(os.path.join(folderpath, f), dtype)['data']\n",
    "            numFiles += 1\n",
    "           \n",
    "    return data_array\n",
    "\n",
    "def loadContinuous(filepath, dtype = float):\n",
    "\n",
    "    assert dtype in (float, np.int16), \\\n",
    "      'Invalid data type specified for loadContinous, valid types are float and np.int16'\n",
    "\n",
    "\n",
    "    ch = { }\n",
    "    recordNumber = np.intp(-1)\n",
    "    \n",
    "    samples = np.zeros(MAX_NUMBER_OF_CONTINUOUS_SAMPLES, dtype)\n",
    "    timestamps = np.zeros(MAX_NUMBER_OF_RECORDS)\n",
    "    recordingNumbers = np.zeros(MAX_NUMBER_OF_RECORDS)\n",
    "    indices = np.arange(0,MAX_NUMBER_OF_RECORDS*SAMPLES_PER_RECORD, SAMPLES_PER_RECORD, np.dtype(np.int64))\n",
    "    \n",
    "    #read in the data\n",
    "    f = open(filepath,'rb')\n",
    "    \n",
    "    header = readHeader(f)\n",
    "    \n",
    "    fileLength = os.fstat(f.fileno()).st_size\n",
    "   \n",
    "    while f.tell() < fileLength:\n",
    "        \n",
    "        recordNumber += 1        \n",
    "        \n",
    "        timestamps[recordNumber] = np.fromfile(f,np.dtype('<i8'),1) # little-endian 64-bit signed integer \n",
    "        N = np.fromfile(f,np.dtype('<u2'),1)[0] # little-endian 16-bit unsigned integer\n",
    "        \n",
    "        #print index\n",
    "\n",
    "        if N != SAMPLES_PER_RECORD:\n",
    "            raise Exception('Found corrupted record in block ' + str(recordNumber))\n",
    "        \n",
    "        recordingNumbers[recordNumber] = (np.fromfile(f,np.dtype('>u2'),1)) # big-endian 16-bit unsigned integer\n",
    "        \n",
    "        if dtype == float: # Convert data to float array and convert bits to voltage.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N) * float(header['bitVolts']) # big-endian 16-bit signed integer, multiplied by bitVolts   \n",
    "        else:  # Keep data in signed 16 bit integer format.\n",
    "            data = np.fromfile(f,np.dtype('>i2'),N)  # big-endian 16-bit signed integer\n",
    "        try:\n",
    "            samples[indices[recordNumber]:indices[recordNumber+1]] = data            \n",
    "        except Exception as e:\n",
    "            print(\"error reading \",filepath)\n",
    "            print(repr(e))\n",
    "            print(\"replacing missing values with zeros.\")\n",
    "            #raise\n",
    "        \n",
    "        marker = f.read(10) # dump\n",
    "        \n",
    "        \n",
    "    ch['header'] = header \n",
    "    ch['timestamps'] = timestamps[0:recordNumber]\n",
    "    ch['data'] = samples[0:indices[recordNumber]]  # OR use downsample(samples,1), to save space\n",
    "    ch['recordingNumber'] = recordingNumbers[0:recordNumber]\n",
    "    f.close()\n",
    "    return ch\n",
    "    \n",
    "def readHeader(f):\n",
    "    header = { }\n",
    "    h = f.read(1024).decode().replace('\\n','').replace('header.','')\n",
    "    for i,item in enumerate(h.split(';')):\n",
    "        if '=' in item:\n",
    "            header[item.split(' = ')[0]] = item.split(' = ')[1]\n",
    "    return header\n",
    "    \n",
    "def _get_sorted_channels(folderpath,sep='_CH'):\n",
    "    return sorted([int(f.split(sep)[1].split('.')[0]) for f in os.listdir(folderpath) \n",
    "                    if '.continuous' in f and sep in f]) \n",
    "\n",
    "\n",
    "\n",
    "# constants\n",
    "NUM_HEADER_BYTES = 1024\n",
    "SAMPLES_PER_RECORD = 1024\n",
    "RECORD_SIZE = 8 + 16 + SAMPLES_PER_RECORD*2 + 10 # size of each continuous record in bytes\n",
    "RECORD_MARKER = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 255])\n",
    "\n",
    "# constants for pre-allocating matrices:\n",
    "MAX_NUMBER_OF_SPIKES = int(1e6)\n",
    "MAX_NUMBER_OF_RECORDS = int(1e6)\n",
    "MAX_NUMBER_OF_CONTINUOUS_SAMPLES = int(1e8)\n",
    "MAX_NUMBER_OF_EVENTS = int(1e6)\n",
    "\n",
    "def pack_continuous_files(folderpath, filename = 'openephys.dat', source='100', channels = 'all', dref = None):\n",
    "\n",
    "    '''Alternative version of pack which uses numpy's tofile function to write data.\n",
    "    pack_2 is much faster than pack and avoids quantization noise incurred in pack due\n",
    "    to conversion of data to float voltages during loadContinous followed by rounding\n",
    "    back to integers for packing.  \n",
    "    source: string name of the source that openephys uses as the prefix. It is usually 100, \n",
    "            if the headstage is the first source added, but can specify something different.\n",
    "    channels:  List of channel numbers specifying order in which channels are packed. By default\n",
    "               all CH continous files are packed in numerical order.\n",
    "    dref:  Digital referencing - either supply a channel number or 'ave' to reference to the \n",
    "           average of packed channels.\n",
    "    '''\n",
    "\n",
    "    data_array = loadFolderToArray(folderpath, channels, np.int16, source)\n",
    "\n",
    "    if dref: \n",
    "        if dref == 'ave':\n",
    "            print('Digital referencing to average of all channels.')\n",
    "            reference = np.mean(data_array,1)\n",
    "        else:\n",
    "            print('Digital referencing to channel ' + str(dref))\n",
    "            if channels == 'all': \n",
    "                channels = _get_sorted_channels(folderpath)\n",
    "            reference = deepcopy(data_array[:,channels.index(dref)])\n",
    "        for i in range(data_array.shape[1]):\n",
    "            data_array[:,i] = data_array[:,i] - reference\n",
    "\n",
    "    print('Packing data to file: ' + filename)\n",
    "    data_array.tofile(os.path.join(folderpath,filename))\n",
    "    print(\".dat file created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to create .eeg files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eeg(inputName,nChannels=None,downsample=24,inputSamplingRate=30000):\n",
    "    '''\n",
    "    downsample the input file (raw.kwd or dat) and save it in .eeg\n",
    "    eegData= rawData[::16,:] (save every 16 point for each channel)\n",
    "    '''\n",
    "    downsample=int(downsample)\n",
    "    if inputName.endswith(\".raw.kwd\"):\n",
    "        raw_data=KwdRawDataReader(inputName)\n",
    "        outputName=inputName[:-7]+\"eeg\"\n",
    "    elif inputName.endswith(\".dat\"):\n",
    "        raw_data=DatRawDataReader(inputName,nChannels=nChannels)\n",
    "        outputName=inputName[:-3]+\"eeg\"\n",
    "        if nChannels is None:\n",
    "            print(\"Error in create_eeg: nChannel snot defined, can't read .dat\")\n",
    "            return False\n",
    "    else:\n",
    "        print(\"Error in create_eeg: input file doesn't end with '.raw.kwd' or '.dat'\")\n",
    "        return False\n",
    "    \n",
    "    chunk_size=inputSamplingRate # 1 second\n",
    "    with open(outputName,'wb') as output:\n",
    "        for chunk in raw_data.chunks(chunk_size=chunk_size):\n",
    "            chunk_raw = chunk.data_chunk_full # shape: (nsamples, nchannels)\n",
    "            chunk_test=chunk_raw[::downsample,:]\n",
    "            for k in range(len(chunk_test)):\n",
    "                newFileByteArray=bytearray(chunk_test[k])\n",
    "                output.write(newFileByteArray)\n",
    "    print('\\n.eeg file created at: %s'%(outputName))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Function to save .dat, .prm, and .eeg files as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_prm_eeg_file_save_batch(path,animalList,overwrite=False,saveEEG=False):\n",
    "    for animal in animalList:\n",
    "        animalFolder=os.path.join(path,animal)\n",
    "        rawfile_list = find_file(animalFolder,['.raw.kwd'])\n",
    "        for rawfile in rawfile_list:\n",
    "            if os.path.exists(rawfile[:-7]+\"dat\") and os.path.exists(rawfile[:-7]+\"prm\") and not overwrite:\n",
    "                print(\".dat file already exists, skipping this session.\")\n",
    "                continue\n",
    "            safe_nas=safe_copy_from_nas(rawfile)\n",
    "            filePath=safe_nas.start()\n",
    "            try:\n",
    "                conversion_result= convert_kwd_to_dat(filePath,overwrite=overwrite)\n",
    "                if isinstance(conversion_result,bool):\n",
    "                    print(\".raw.kwd to .dat conversion failed:%s\"%(filePath))\n",
    "                else:\n",
    "                    nchannels, sampling_rate=conversion_result\n",
    "                    experiment=os.path.splitext(os.path.splitext(os.path.split(filePath)[1])[0])[0]\n",
    "                    save_prm_file(os.path.split(filePath)[0],experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                    if saveEEG:\n",
    "                        create_eeg(filePath,downsample=int(sampling_rate/1250),\n",
    "                                   inputSamplingRate=sampling_rate)\n",
    "            except Exception as e:\n",
    "                print('Converting .dat/ saving .prm/.eeg file failed at:')\n",
    "                print(filePath)\n",
    "                print(repr(e))\n",
    "            finally:\n",
    "                safe_nas.stop()\n",
    "        \n",
    "        #check existence of .continuous files\n",
    "        rawfile_list = find_file(animalFolder,['.continuous'])\n",
    "        folder_list = list(set([os.path.dirname(f) for f in rawfile_list]))\n",
    "        if len(folder_list) >0:\n",
    "            for continuousFolder in folder_list:\n",
    "                experiment=os.path.basename(continuousFolder)\n",
    "                filename= experiment + \".dat\"\n",
    "                settingsPath=os.path.join(continuousFolder,\"Continuous_Data.openephys\")\n",
    "                \n",
    "                if not os.path.exists(settingsPath):\n",
    "                    print (settingsPath)\n",
    "                    raise FileNotFoundError(\n",
    "                        \"There must be a \\\"Continuous_Data.openephys.xml\\\"\",\n",
    "                        \" file in the same folder as .continuous files to load the settings.\")\n",
    "                \n",
    "                with open(settingsPath,'r') as f:\n",
    "                    openEphysSettings=xmltodict.parse(f.read())\n",
    "                    openEphysSettings=openEphysSettings['EXPERIMENT']['RECORDING']\n",
    "                    if isinstance(openEphysSettings, list):\n",
    "                        openEphysSettings=openEphysSettings[0]\n",
    "                    sampling_rate =openEphysSettings['@samplerate']\n",
    "                    source        =openEphysSettings['PROCESSOR']['@id']\n",
    "                    nchannels     =len(openEphysSettings['PROCESSOR']['CHANNEL'])\n",
    "                    \n",
    "                    \n",
    "                #convert to and save as .DAT file\n",
    "                pack_continuous_files(continuousFolder, filename = filename, source=source, channels = 'all', dref = None)\n",
    "                #save PRM file\n",
    "                save_prm_file(continuousFolder,experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                if saveEEG:\n",
    "                    create_eeg(rawfile,downsample=int(sampling_rate/1250),\n",
    "                               inputSamplingRate=sampling_rate)        \n",
    "    print(\"\\nConverting/saving done\")\n",
    "    \n",
    "        \n",
    "#------------------------------------------------------------------------------------------\n",
    "# if \"__file__\" not in dir():\n",
    "#     Path=\"/data/SWI002/22/\"\n",
    "#     animalList=[]\n",
    "    \n",
    "#     dat_prm_eeg_file_save_batch(Path,animalList)\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==========================================================\n",
    "### Script to Rename and Convert as a Batch\n",
    "##### 'root' must be a directory containing seperate folders for each animal(ex: /data/ containing /data/Rat001, /data/Rat002, ...)\n",
    "##### 'animalList' determines on which folders within the 'root' this notebook will operate(ex: Rat001, Rat002, ...).\n",
    "##### renaming MUST precede converting/saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    #The directory containing separate folders for each animal\n",
    "    root=\"/NAS02/\"#\"/NAS02/\"\n",
    "    #in Windows paths must be like this: root=\"C:\\\\Data\\\\Recordings\\\\\" (double backslash instead of single)\n",
    "    \n",
    "    #Animal where to run the script\n",
    "    animalList=[\"Rat174\"]\n",
    "    \n",
    "    #Whether to save .eeg files for each .raw.kwd file\n",
    "    saveEEG=False\n",
    "    \n",
    "    #Whether to overwrite the existing .dat files\n",
    "    overwrite=False\n",
    "\n",
    "    #Whether to print \"X renamed in Y\"\n",
    "    verbose=True  \n",
    "\n",
    "    #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "    # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "    minuteDelay=0\n",
    "\n",
    "    #Files to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "    #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "    extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwe\",\".eeg\"]\n",
    "    \n",
    "    #--------------------------------------------------------------------------------\n",
    "#     rename_batch(root,animalList,verbose,minuteDelay,extensionList)\n",
    "    dat_prm_eeg_file_save_batch(root,animalList,overwrite,saveEEG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ==========================================================\n",
    "# Script to create DAT+PRM+EVT.CAM+EVT.TRE files as a batch (UNTESTED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dat_prm_evt_file_batch(path,animalList,overwrite=False,saveEEG=False,\n",
    "                           nbTre=-2,nbCam=-1,fileTypes=['.dat','.prm','.behav_param','.entrancetimes']):\n",
    "    for animal in animalList:\n",
    "        animalFolder=os.path.join(path,animal)\n",
    "        rawfile_list = find_file(animalFolder,['.raw.kwd'])\n",
    "        for rawfile in rawfile_list:\n",
    "            sessionFolder=os.path.dirname(rawfile)\n",
    "            behavFiles=[]\n",
    "            datDone=False\n",
    "            if os.path.exists(rawfile[:-7]+\"dat\") and os.path.exists(rawfile[:-7]+\"prm\") and \\\n",
    "            os.path.exists(rawfile[:-7]+\"evt.cam\") and os.path.exists(rawfile[:-7]+\"evt.tre\") and not overwrite:\n",
    "                print(\"files already exists, skipping this session.\")\n",
    "                continue\n",
    "            if os.path.exists(rawfile[:-7]+\"behav_param\") and os.path.exists(rawfile[:-7]+\"entrancetimes\"):\n",
    "                behavFiles.extend([rawfile[:-7]+\"behav_param\",rawfile[:-7]+\"entrancetimes\"])\n",
    "            \n",
    "            behavFiles.append(rawfile)\n",
    "            safeFiles=[safe_copy_from_nas(f) for f in behavFiles]\n",
    "            newFilePaths=[safeFile.start('.NAS2LOCALqwrytdvnz2u1r') for safeFile in safeFiles]\n",
    "            try:\n",
    "                conversion_result= convert_kwd_to_dat(newFilePaths[-1],overwrite=overwrite)\n",
    "                if isinstance(conversion_result,bool):\n",
    "                    print(\".raw.kwd to .dat conversion failed:%s\"%(newFilePaths[-1]))\n",
    "                else:\n",
    "                    nchannels, sampling_rate=conversion_result\n",
    "                    experiment=os.path.splitext(os.path.splitext(os.path.split(newFilePaths[-1])[1])[0])[0]\n",
    "                    save_prm_file(os.path.split(newFilePaths[-1])[0],experiment,sampling_rate,nchannels,overwrite=True)\n",
    "                    if saveEEG:\n",
    "                        create_eeg(newFilePaths[-1],downsample=int(sampling_rate/1250),\n",
    "                                   inputSamplingRate=sampling_rate)\n",
    "                datDone=True\n",
    "                \n",
    "                #Now, making EVT files\n",
    "                obj=ephy_events(os.path.dirname(newFilePaths[0]),nbTre,nbCam,overwrite=overwrite)\n",
    "                obj.run()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(sessionFolder)\n",
    "                print(repr(e))\n",
    "                if not datDone:\n",
    "                    print('Converting .dat/ saving .prm/.eeg file failed')\n",
    "                else:\n",
    "                    print('EVT file failed')\n",
    "                    \n",
    "            finally:\n",
    "                safe_nas.stop(fileTypes=['.prm','.dat','.eeg','.evt.cam','.evt.tre'])\n",
    "        \n",
    "\n",
    "    print(\"\\nConverting/saving done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    #The directory containing separate folders for each animal\n",
    "    root=\"/NAS02/\"\n",
    "    #in Windows paths must be like this: root=\"C:\\\\Data\\\\Recordings\\\\\" (double backslash instead of single)\n",
    "    \n",
    "    #Animal where to run the script\n",
    "    animalList=[\"Rat174\"]\n",
    "        \n",
    "    #Whether to overwrite the existing .dat files\n",
    "    overwrite=False\n",
    "    \n",
    "    #relative channel number containing TREADMILL clock\n",
    "    nbTre=-2\n",
    "    \n",
    "    #relative channel number containing CAMERA clock\n",
    "    nbCam=-1    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Whether to save .eeg files for each .raw.kwd file\n",
    "    saveEEG=False\n",
    "    \n",
    "    #Whether to print \"X renamed in Y\"\n",
    "    verbose=True  \n",
    "\n",
    "    #If a wrong folder and a regular folder are less than \"minuteDelay\" apart, they are merged\n",
    "    # example: if minuteDelay=2, 'Rat001_2034_04_22-04_26_12' would be merged with 'Rat001_2034_04_22_04_27'\n",
    "    minuteDelay=0\n",
    "\n",
    "    #Files to rename (ex: \"someFile.dat\" -> \"Rat024_2015_etc.dat\")\n",
    "    #extensionList=[\".dat\",\"+6.raw.kwd\",\".nrs\",\".kwx\",\".kwik\",\".prm\",\".prb\"]\n",
    "    extensionList=[\".dat\",\".raw.kwd\",\".nrs\",\".kwe\",\".eeg\"]\n",
    "    \n",
    "\n",
    "    #--------------------------------------------------------------------------------\n",
    "#     rename_batch(root,animalList,verbose,minuteDelay,extensionList)\n",
    "    dat_prm_eeg_file_save_batch(root,animalList,overwrite,saveEEG)\n",
    "    run_event_batch(root,animalList,nbTre,nbCam,overwrite=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ==========================================================\n",
    "### Script to run KLUSTA as a Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_klusta_batch(path,animalList,fileTypes=['.dat','.prm','.prb'],overwrite=False):\n",
    "    \"\"\"\n",
    "    this function is almost general, klusta-specific lines are marked with #%klusta\n",
    "    \"\"\"\n",
    "    #%klusta\n",
    "    flag= '--overwrite' if overwrite else ''\n",
    "\n",
    "    for animal in animalList:\n",
    "        animalFolder=os.path.join(path,animal)\n",
    "        rawfile_list = find_file(animalFolder,[fileTypes[0]])\n",
    "        for rawfile in rawfile_list:\n",
    "            validFile=[os.path.exists(rawfile[:-len(fileTypes[0])] +f) for f in fileTypes]\n",
    "            #%klusta\n",
    "            validFile.append(not os.path.exists(rawfile[:-len(fileTypes[0])] +'.kwik'))\n",
    "            \n",
    "            if all(validFile) or overwrite:\n",
    "                safeFiles=[safe_copy_from_nas(rawfile[:-len(fileTypes[0])] +f) for f in fileTypes]\n",
    "                newFilePaths=[safeFile.start('.NAS2KLUSTAqwrytdvnz2u1r') for safeFile in safeFiles]\n",
    "                #%klusta\n",
    "                klustaCommand=\"\"\"\n",
    "                source activate klusta\n",
    "                cd {path}\n",
    "                klusta {prmFile} {ovr}\n",
    "                source deactivate klusta\n",
    "                \"\"\".format(path=os.path.dirname(newFilePaths[0]),\n",
    "                           prmFile=newFilePaths[0][:-len(fileTypes[0])]+'.prm',\n",
    "                           ovr=flag)\n",
    "                \n",
    "                klustaCommand.replace('\"', '')\n",
    "                klustaCommand.replace('\\'', '')\n",
    "                print(\"running spike sorting...\")\n",
    "                try:\n",
    "                    subprocess.run(klustaCommand,shell=True,check=True)\n",
    "                except Exception as e:\n",
    "                    \"failed in {}\".format(rawfile)\n",
    "                    print(repr(e))\n",
    "                finally:\n",
    "                    #%klusta\n",
    "                    safeFiles[0].stop(fileTypes=['.kwik','.kwe','.kwx'])\n",
    "        \n",
    "    print(\"\\nConverting/saving done\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    #The directory containing separate folders for each animal\n",
    "    root=\"/NAS02/\"\n",
    "    #in Windows paths must be like this: root=\"C:\\\\Data\\\\Recordings\\\\\" (double backslash instead of single)\n",
    "    \n",
    "    #Animal where to run the script\n",
    "    animalList=[\"RatT03\"]\n",
    "    \n",
    "    #Whether to overwrite the existing .kwik file\n",
    "    overwrite=True\n",
    "    #--------------------------------------------------------------------------------\n",
    "    run_klusta_batch(path=root,animalList=animalList,fileTypes=['.dat','.prm','.prb'],overwrite=overwrite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Phy on shanks of a single session on NAS\n",
    "\n",
    "#### phy will be automatiically opened, curate the clusters, save, close the phy. Confirm proceeding to next shank (press Enter). Phy will be reopened..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "\n",
    "    root='/NAS02'\n",
    "    session=\"Rat174_2018_07_03_15_10\"\n",
    "\n",
    "    #DO NOT TOUCH THE BELOW CODE\n",
    "    animal=session[:6]\n",
    "    fullPath=os.path.join(root,animal,'Experiments',session,session)\n",
    "    files=['.dat','.kwe','.kwx','.kwik','.prm','.prb']\n",
    "    safeFiles=[safe_copy_from_nas(fullPath+f) for f in files]\n",
    "    newFilePaths=[safeFile.start('.NAS2KLUSTAqwrytdvnz2u1r') for safeFile in safeFiles]\n",
    "    newFullPath=newFilePaths[0][:newFilePaths[0].rfind('.')]\n",
    "    \n",
    "    prb=prm_reader(newFullPath+'.prb')['channel_groups']\n",
    "    for shank in prb:\n",
    "        if not any(prb[shank]['channels']):\n",
    "            continue   #dead shanks\n",
    "\n",
    "        phyCommand=\"\"\"\n",
    "        source activate phy\n",
    "        cd {path}\n",
    "        phy kwik-gui {kwikFile} --channel-group={shank}\n",
    "        source deactivate phy\n",
    "        \"\"\".format(path=os.path.dirname(newFullPath),\n",
    "                   kwikFile=newFullPath+'.kwik',\n",
    "                   shank=shank\n",
    "                  )\n",
    "        phyCommand.replace('\"', '')\n",
    "        phyCommand.replace('\\'', '')\n",
    "        print(\"running phy... shank:\",shank)\n",
    "        try:\n",
    "            subprocess.run(phyCommand,shell=True,check=True)\n",
    "        except Exception as e:\n",
    "            \"failed in shank {}\".format(shank)\n",
    "            print(repr(e))\n",
    "        finally:\n",
    "            print(\"finished!\")\n",
    "            if input(\"Continue?\").lower() not in ['','y','yes']:\n",
    "                safeFiles[0].stop(fileTypes=['.kwik','.kwe','.kwx'])\n",
    "                break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
